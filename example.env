# AutoLlama v2.3.1 Configuration Template (Legacy)
# NOTE: For new deployments, use .env.public.example instead
# Copy this file to .env and update the values

# N8N Webhook Configuration
# Test webhook for development and testing
WEBHOOK_TEST_URL=your_webhook_test_url_here

# Production webhook for live usage
WEBHOOK_PROD_URL=your_webhook_prod_url_here

# Use test webhook by default (set to false for production)
USE_TEST_WEBHOOK=true

# Service Configuration
SERVICE_NAME=autollama
DOMAIN=autollama.io

# Debug Configuration
# Enable debug mode for verbose logging
DEBUG_MODE=true

# Enable file logging (logs will be sent with webhook requests)
LOG_TO_FILE=true

# Request Configuration (optional)
REQUEST_TIMEOUT=30000  # 30 seconds
RETRY_ATTEMPTS=3
RETRY_DELAY=1000      # 1 second base delay (exponential backoff)

# Logging Configuration
LOG_LEVEL=debug       # debug, info, warn, error
MAX_LOG_SIZE=1048576  # 1MB

# Contextual Embeddings Configuration
# Enable contextual embeddings for improved RAG performance
ENABLE_CONTEXTUAL_EMBEDDINGS=true

# AI model to use for context generation
CONTEXTUAL_EMBEDDING_MODEL=gpt-4o-mini

# Batch size for context generation (future use)
CONTEXT_GENERATION_BATCH_SIZE=5

# API Keys for AI Services
OPENAI_API_KEY=your_openai_api_key_here

# Vector Database Configuration
QDRANT_URL=your_qdrant_url_here
QDRANT_API_KEY=your_qdrant_api_key_here