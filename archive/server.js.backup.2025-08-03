const express = require('express');
const axios = require('axios');
const https = require('https');
const TurndownService = require('turndown');
const OpenAI = require('openai');
const { v4: uuidv4 } = require('uuid');
const cheerio = require('cheerio');
const pdfParse = require('pdf-parse');
const Busboy = require('busboy');
const connectBusboy = require('connect-busboy');
const mammoth = require('mammoth');
const epub = require('epub');
const { parse: csvParse } = require('csv-parse');
const tmp = require('tmp');
const WebSocket = require('ws');
// const { createSession, createChannel } = require('better-sse'); // Requires Node 20+
// const { Queue, Worker } = require('bullmq'); // Requires Redis


// Import the new PostgreSQL database layer
const db = require('./database');

// Import new AI and storage services for gradual migration
const { initializeAIServices } = require('./src/services/ai');
const { initializeStorageServices } = require('./src/services/storage');

// Initialize AI services (gradually replacing individual functions)
let aiServices = null;
let storageServices = null;

try {
  aiServices = initializeAIServices({
    openaiApiKey: process.env.OPENAI_API_KEY,
    defaultModel: 'gpt-4o-mini',
    embeddingModel: 'text-embedding-3-small',
    maxTokens: 4000,
    temperature: 0.3
  });
  console.log('‚úÖ New AI services initialized successfully');
} catch (error) {
  console.warn('‚ö†Ô∏è New AI services initialization failed:', error.message);
}

try {
  storageServices = initializeStorageServices({
    vector: {
      url: process.env.QDRANT_URL,
      apiKey: process.env.QDRANT_API_KEY,
      collection: 'autollama-content'
    },
    bm25: {
      url: 'http://localhost:3002'
    }
  });
  console.log('‚úÖ New storage services initialized successfully');
} catch (error) {
  console.warn('‚ö†Ô∏è New storage services initialization failed:', error.message);
}

const app = express();
const PORT = process.env.PORT || 3001;
const WS_PORT = parseInt(process.env.WS_PORT || '3003');

console.log('üöÄ AutoLlama API server starting up with new CORS middleware...');

// Import new route system for gradual migration - PARTIAL ENABLE FOR SSE
const { setupRoutes } = require('./src/routes');
let routesEnabled = true;

// Import new CORS middleware system
const { createCorsMiddleware, routeSpecificCors } = require('./src/middleware/cors.middleware');

// Apply new CORS middleware with EventSource/SSE support
app.use(createCorsMiddleware({
  // Use streaming-specific CORS config for EventSource compatibility
  ...routeSpecificCors.streaming,
  origin: '*', // Allow all origins for development
  credentials: false,
  allowedHeaders: '*' // Allow all headers for EventSource
}));

app.use(express.json());
// Removed global connect-busboy middleware to prevent conflicts with manual busboy setup in upload endpoints

// Use existing WebSocket/SSE infrastructure instead of better-sse
console.log('Using existing SSE infrastructure (Node 18 compatible)');

// BullMQ requires Redis - using direct processing for now
console.log('Using direct processing (no Redis dependency)');

// WebSocket server variables
let wss;
const connectedClients = new Set();

// RAG activity tracking with Qdrant telemetry
let lastRagActivity = null;
let lastQdrantSearchCount = null;

// Function to get actual Qdrant search activity from telemetry
async function getQdrantSearchActivity() {
    try {
        const response = await axios.get(`${QDRANT_URL}/telemetry`, {
            headers: {
                'api-key': QDRANT_API_KEY
            },
            timeout: 5000
        });
        
        const telemetry = response.data.result;
        const restResponses = telemetry.requests?.rest?.responses || {};
        
        // Track search and query endpoints that indicate RAG activity
        const searchEndpoints = [
            'POST /collections/{name}/points/search',
            'POST /collections/{name}/points/query'
        ];
        
        let totalSearches = 0;
        let lastSearchActivity = null;
        
        for (const endpoint of searchEndpoints) {
            const endpointData = restResponses[endpoint];
            if (endpointData && endpointData['200']) {
                totalSearches += endpointData['200'].count || 0;
            }
        }
        
        // If search count increased, update activity timestamp
        if (lastQdrantSearchCount !== null && totalSearches > lastQdrantSearchCount) {
            lastSearchActivity = new Date();
        }
        
        lastQdrantSearchCount = totalSearches;
        
        return {
            totalSearches,
            lastSearchActivity,
            telemetryTimestamp: new Date(),
            qdrantStatus: 'active'
        };
        
    } catch (error) {
        console.error('‚ùå Error fetching Qdrant telemetry:', error.message);
        return {
            totalSearches: 0,
            lastSearchActivity: null,
            telemetryTimestamp: new Date(),
            qdrantStatus: 'error',
            error: error.message
        };
    }
}

// Function to initialize WebSocket server
function initializeWebSocket() {
    console.log(`üîå Attempting to start WebSocket server on port ${WS_PORT}`);
    
    try {
        wss = new WebSocket.Server({ port: WS_PORT });
        console.log(`‚úÖ WebSocket server started successfully on port ${WS_PORT}`);
        
        wss.on('connection', (ws) => {
            console.log('New WebSocket client connected');
            connectedClients.add(ws);
            
            // Send initial connection confirmation
            ws.send(JSON.stringify({
                type: 'connection',
                status: 'connected',
                timestamp: new Date().toISOString()
            }));
            
            ws.on('close', () => {
                console.log('WebSocket client disconnected');
                connectedClients.delete(ws);
            });
            
            ws.on('error', (error) => {
                console.error('WebSocket error:', error);
                connectedClients.delete(ws);
            });
        });
        
        return true;
    } catch (error) {
        console.error(`‚ùå Failed to start WebSocket server on port ${WS_PORT}:`, error);
        return false;
    }
}

// Function to broadcast pipeline updates to all connected WebSocket clients
function broadcastPipelineUpdate(updateData) {
    if (!wss) return;
    
    const message = JSON.stringify({
        type: 'pipeline_update',
        data: updateData,
        timestamp: new Date().toISOString()
    });
    
    connectedClients.forEach((client) => {
        if (client.readyState === WebSocket.OPEN) {
            try {
                client.send(message);
            } catch (error) {
                console.error('Failed to send WebSocket message:', error);
                connectedClients.delete(client);
            }
        }
    });
}

// BM25 Service Integration
const BM25_SERVICE_URL = 'http://localhost:3002';

async function storeBM25Index(chunks, filename) {
    // Use new storage services if available, fallback to original implementation
    if (storageServices && storageServices.bm25Service) {
        try {
            return await storageServices.bm25Service.storeBM25Index(chunks, filename);
        } catch (error) {
            console.warn('New BM25 service failed, falling back to original:', error.message);
        }
    }

    // Original implementation as fallback
    try {
        
        // Prepare chunks for BM25 service
        const bm25Chunks = chunks.map(chunk => ({
            id: chunk.chunk_id,
            text: chunk.chunk_text,
            metadata: {
                chunk_index: chunk.chunk_index,
                url: chunk.original_url,
                title: chunk.title || ''
            }
        }));
        
        const response = await axios.post(`${BM25_SERVICE_URL}/index/${encodeURIComponent(filename)}`, {
            chunks: bm25Chunks,
            filename: filename,
            replace_existing: true
        });
        
        console.log(`‚úÖ BM25 index created: ${response.data.chunks} chunks indexed in ${response.data.processing_time_seconds}s`);
        return response.data;
        
    } catch (error) {
        console.error('‚ùå BM25 indexing failed:', error.message);
        throw new Error(`BM25 indexing failed: ${error.message}`);
    }
}

// Initialize services
const turndownService = new TurndownService();
// Initialize OpenAI client with hardcoded API key
const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY || 'your-openai-api-key-here'
});

// This will be reassigned when configuration is loaded from database
let openaiClient = openai;

// Configuration
// Airtable configuration removed - system now uses PostgreSQL database exclusively
const QDRANT_URL = process.env.QDRANT_URL || 'https://c4c8ee46-d9dd-4c0f-a00e-9215675351da.us-west-1-0.aws.cloud.qdrant.io';
const QDRANT_API_KEY = process.env.QDRANT_API_KEY || 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.ghMgF9xxBObWVQnMQab9wCk7JP4jkHI7k4Z1TYo8zqg';

// Contextual Embeddings Configuration
const ENABLE_CONTEXTUAL_EMBEDDINGS = process.env.ENABLE_CONTEXTUAL_EMBEDDINGS === 'true';
const CONTEXTUAL_EMBEDDING_MODEL = process.env.CONTEXTUAL_EMBEDDING_MODEL || 'gpt-4o-mini';
const CONTEXT_GENERATION_BATCH_SIZE = parseInt(process.env.CONTEXT_GENERATION_BATCH_SIZE || '5');

// Dynamic configuration loading from database
async function initializeApiConfiguration() {
    try {
        console.log('üîß Initializing API configuration from database...');
        const dbSettings = await db.getApiSettings();
        
        // Check if we have a valid OpenAI API key in database
        const dbOpenAIKey = dbSettings.openai_api_key;
        if (dbOpenAIKey && dbOpenAIKey.trim() !== '' && dbOpenAIKey !== 'your_openai_api_key_here') {
            console.log('‚úÖ Found OpenAI API key in database, reinitializing client...');
            
            // Reinitialize OpenAI client with hardcoded key (override database)
            const { OpenAI } = require('openai');
            openaiClient = new OpenAI({
                apiKey: process.env.OPENAI_API_KEY || 'your-openai-api-key-here'
            });
            
            console.log('‚úÖ OpenAI client successfully initialized with database settings');
            return true;
        } else {
            console.log('‚ö†Ô∏è No valid OpenAI API key found in database');
            if (!process.env.OPENAI_API_KEY || process.env.OPENAI_API_KEY === 'your_openai_api_key_here') {
                console.log('‚ùå No valid OpenAI API key in environment either - service will have limited functionality');
                return false;
            } else {
                console.log('‚úÖ Using OpenAI API key from environment variables');
                return true;
            }
        }
    } catch (error) {
        console.error('‚ùå Error initializing API configuration:', error.message);
        return false;
    }
}

// In-memory tracking for active processing sessions
const activeProcessingSessions = new Map();
// Make activeProcessingSessions available globally for new route system
global.activeProcessingSessions = activeProcessingSessions;

// Simple cache for Airtable records
const recordsCache = {
    data: null,
    timestamp: null,
    ttl: 3600000 // 1 hour cache
};

// User agents for rotation
const USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15'
];

// Processing functions
// Helper function to extract title from content
function extractTitle(content, url) {
    // Try to extract title from content
    const lines = content.split('\n');
    for (const line of lines) {
        const trimmed = line.trim();
        if (trimmed.length > 10 && trimmed.length < 200) {
            // Check if it looks like a title (starts with capital, no ending punctuation)
            if (/^[A-Z]/.test(trimmed) && !/[.!?]$/.test(trimmed)) {
                return trimmed;
            }
        }
    }
    
    // Fallback to URL-based title
    const urlParts = url.split('/');
    const lastPart = urlParts[urlParts.length - 1] || urlParts[urlParts.length - 2];
    return lastPart.replace(/[-_]/g, ' ').replace(/\.\w+$/, '');
}

// Helper function to generate document summary
async function generateDocumentSummary(content, url) {
    // Use new AI services if available, fallback to original implementation
    if (aiServices && aiServices.analysisService) {
        try {
            return await aiServices.analysisService.generateDocumentSummary(content, {
                maxTokens: 150,
                temperature: 0.5,
                maxContentLength: 2000
            });
        } catch (error) {
            console.warn('New summary service failed, falling back to original:', error.message);
        }
    }

    // Original implementation as fallback
    try {
        // Use first 2000 characters for summary generation
        const preview = content.substring(0, 2000);
        
        const response = await openaiClient.chat.completions.create({
            model: "gpt-4o-mini",
            messages: [
                {
                    role: "system",
                    content: "Generate a concise 2-3 sentence summary of this document."
                },
                {
                    role: "user",
                    content: preview
                }
            ],
            temperature: 0.5,
            max_tokens: 150
        });
        
        return response.choices[0].message.content.trim();
    } catch (error) {
        console.error('Error generating document summary:', error);
        return 'Document summary unavailable';
    }
}

async function fetchWebContent(url, retryCount = 0) {
    console.log(`[1/3] Fetching content from: ${url}`);
    const maxRetries = 2;
    const userAgent = USER_AGENTS[retryCount % USER_AGENTS.length];
    
    try {
        // Add random delay to appear more human-like
        if (retryCount > 0) {
            await new Promise(resolve => setTimeout(resolve, Math.random() * 2000 + 1000));
        }
        
        const response = await axios.get(url, {
            headers: {
                'User-Agent': userAgent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'Connection': 'keep-alive'
            },
            timeout: 30000,
            responseType: 'arraybuffer',  // Get response as buffer to handle both HTML and PDF
            httpsAgent: new https.Agent({
                rejectUnauthorized: false  // Allow self-signed certificates for academic/research sites
            })
        });
        
        console.log('‚úÖ Fetch successful. Status:', response.status);
        const contentType = response.headers['content-type'] || '';
        
        // Check if it's a PDF
        if (contentType.includes('application/pdf') || url.toLowerCase().endsWith('.pdf')) {
            console.log('Detected PDF content, extracting text...');
            const pdfData = await pdfParse(response.data);
            return { 
                content: pdfData.text, 
                type: 'pdf',
                metadata: {
                    pages: pdfData.numpages,
                    info: pdfData.info
                }
            };
        } else {
            // Convert buffer back to string for HTML content
            const htmlContent = response.data.toString('utf-8');
            console.log('-- HTML Content --');
            console.log(htmlContent.substring(0, 1000));
            console.log('-- End HTML Content --');
            return { 
                content: htmlContent, 
                type: 'html' 
            };
        }
    } catch (error) {
        // Check if it's a 403 and we haven't exhausted retries
        if (error.response?.status === 403 && retryCount < maxRetries) {
            console.log(`Attempt ${retryCount + 1} failed with 403, retrying with different user agent...`);
            return fetchWebContent(url, retryCount + 1);
        }
        
        // Provide more helpful error messages
        if (error.response?.status === 403) {
            throw new Error(`Access denied (403): This website blocks automated access. The site may require JavaScript, have CAPTCHA protection, or block all automated requests. Try using a different URL or accessing the content manually.`);
        } else if (error.response?.status === 404) {
            throw new Error(`Page not found (404): The URL doesn't exist or has been moved.`);
        } else if (error.response?.status >= 500) {
            throw new Error(`Server error (${error.response.status}): The website is experiencing issues.`);
        } else {
            throw new Error(`Failed to fetch URL: ${error.message}`);
        }
    }
}

function htmlToMarkdown(html) {
    console.log('\n[2/3] Converting HTML to Markdown...');
    const $ = cheerio.load(html);
    console.log('  - Loaded HTML into cheerio');
    // Remove script and style elements
    $('script, style, nav, footer, aside').remove();
    console.log('  - Removed script and style elements');
    // Get main content (try common content selectors)
    const contentSelectors = ['main', 'article', '.content', '.post', '#content', '.entry-content'];
    let content = '';
    
    for (const selector of contentSelectors) {
        console.log(`  - Trying selector: ${selector}`);
        const element = $(selector);
        if (element.length > 0 && element.text().trim().length > 100) {
            console.log(`    - Found element with selector: ${selector}`);
            content = element.html();
            break;
        }
    }
    
    // If no main content found, use body but clean it up
    if (!content) {
        console.log('  - No main content found, using body');
        $('header, nav, footer, aside, .sidebar, .menu, .navigation').remove();
        content = $('body').html() || html;
    }
    
    const markdown = turndownService.turndown(content);
    console.log(`‚úÖ Conversion successful. Markdown length: ${markdown.length}`);
    return markdown;
}

function chunkText(content, url, chunkSize = 1200, overlap = 200) {
    if (!content || content.length === 0) {
        throw new Error('No content to chunk');
    }
    
    const cleanContent = content.replace(/\s+/g, ' ').trim();
    
    // Adaptive chunking for large files
    let adaptiveChunkSize = chunkSize;
    let adaptiveOverlap = overlap;
    
    // For large files (>1MB text), use larger chunks to reduce processing load
    if (cleanContent.length > 1000000) {
        adaptiveChunkSize = Math.min(2400, chunkSize * 2); // Max 2400 chars
        adaptiveOverlap = Math.min(400, overlap * 2);      // Max 400 chars overlap
    }
    
    const chunks = [];
    const totalChunks = Math.ceil(cleanContent.length / (adaptiveChunkSize - adaptiveOverlap));
    
    for (let i = 0; i < cleanContent.length; i += adaptiveChunkSize - adaptiveOverlap) {
        const chunk = cleanContent.slice(i, i + adaptiveChunkSize);
        const chunkId = uuidv4();
        
        chunks.push({
            chunk_text: chunk.trim(),
            chunk_id: chunkId,
            chunk_index: Math.floor(i / (adaptiveChunkSize - adaptiveOverlap)),
            original_url: url,
            total_chunks: totalChunks,
            chunk_size_used: adaptiveChunkSize,
            overlap_used: adaptiveOverlap
        });
    }
    
    console.log(`‚úÇÔ∏è Created ${chunks.length} chunks from ${Math.round(cleanContent.length/1000)}K characters`);
    return chunks;
}

async function analyzeChunk(chunkText) {
    // Use new AI services if available, fallback to original implementation
    if (aiServices && aiServices.analysisService) {
        try {
            return await aiServices.analysisService.analyzeChunk(chunkText);
        } catch (error) {
            console.warn('New analysis service failed, falling back to original:', error.message);
        }
    }

    // Original implementation as fallback
    const systemPrompt = `You are a RAG content analyzer. Extract structured metadata from text chunks.

Return JSON with these exact fields:
{
  "title": "Main title/topic (string)",
  "summary": "2-3 sentence summary (string)", 
  "category": "Primary category (string)",
  "tags": ["tag1", "tag2", "tag3"] (array of strings),
  "key_concepts": ["concept1", "concept2"] (array of strings),
  "content_type": "article|blog|academic|news|reference|other (string)",
  "technical_level": "beginner|intermediate|advanced (string)",
  "sentiment": "positive|negative|neutral|mixed (string)",
  "emotions": ["emotion1", "emotion2"] (array from: joy, sadness, anger, fear, surprise, disgust, trust, anticipation),
  "key_entities": {
    "people": ["name1", "name2"] (array of person names mentioned),
    "organizations": ["org1", "org2"] (array of organization names),
    "locations": ["location1", "location2"] (array of place names)
  },
  "main_topics": ["topic1", "topic2", "topic3"] (array of broader topics beyond the title)
}

Analyze thoroughly but keep responses concise and focused.`;

    try {
        const response = await openaiClient.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                { role: 'system', content: systemPrompt },
                { role: 'user', content: `Process this text chunk: ${chunkText}` }
            ],
            response_format: { type: 'json_object' }
        });
        
        return JSON.parse(response.choices[0].message.content);
    } catch (error) {
        throw new Error(`Failed to analyze chunk: ${error.message}`);
    }
}

async function generateChunkContext(fullDocument, chunkText) {
    // Use new AI services if available, fallback to original implementation
    if (aiServices && aiServices.analysisService) {
        try {
            return await aiServices.analysisService.generateChunkContext(fullDocument, chunkText);
        } catch (error) {
            console.warn('New context generation service failed, falling back to original:', error.message);
        }
    }

    // Original implementation as fallback
    const prompt = `Here is the full document content:

<document>
${fullDocument.substring(0, 8000)}${fullDocument.length > 8000 ? '...[truncated]' : ''}
</document>

Here is the chunk we want to situate within the whole document:
<chunk>
${chunkText}
</chunk>

Please give a short succinct context (1-2 sentences) to situate this chunk within the overall document for better retrieval.`;

    try {
        const response = await openaiClient.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                { role: 'system', content: 'You are a helpful assistant that provides contextual summaries for document chunks to improve retrieval.' },
                { role: 'user', content: prompt }
            ],
            max_tokens: 100,
            temperature: 0.3
        });
        
        return response.choices[0].message.content.trim();
    } catch (error) {
        console.warn(`Context generation failed: ${error.message}`);
        return null; // Fall back to non-contextual embedding
    }
}

async function generateEmbedding(text, context = null) {
    // Use new AI services if available, fallback to original implementation
    if (aiServices && aiServices.embeddingService) {
        try {
            return await aiServices.embeddingService.generateEmbedding(text, context);
        } catch (error) {
            console.warn('New embedding service failed, falling back to original:', error.message);
        }
    }

    // Original implementation as fallback
    try {
        // Combine context with chunk text if context is provided
        const enhancedText = context ? `${context}\n\n${text}` : text;
        
        const response = await openaiClient.embeddings.create({
            model: 'text-embedding-3-small',
            input: enhancedText
        });
        
        return response.data[0].embedding;
    } catch (error) {
        throw new Error(`Failed to generate embedding: ${error.message}`);
    }
}

async function storeInQdrant(chunkData, embedding, analysis, contextualSummary = null) {
    // Use new storage services if available, fallback to original implementation
    if (storageServices && storageServices.vectorService) {
        try {
            return await storageServices.vectorService.storeInQdrant(chunkData, embedding, analysis, contextualSummary);
        } catch (error) {
            console.warn('New vector service failed, falling back to original:', error.message);
        }
    }

    // Original implementation as fallback
    try {
        // üîç Debug logging for contextual embeddings
        console.log('üß† Qdrant Storage Debug:');
        console.log('  Chunk ID:', chunkData.chunk_id);
        console.log('  Has contextual summary:', contextualSummary ? 'YES' : 'NO');
        console.log('  Uses contextual embedding:', contextualSummary !== null);
        if (contextualSummary) {
            console.log('  Contextual summary preview:', contextualSummary.substring(0, 100) + '...');
        }

        const payload = {
            url: chunkData.original_url,
            title: analysis.title,
            chunk_text: chunkData.chunk_text,
            chunk_id: chunkData.chunk_id, // üêõ FIX: Add missing chunk_id to payload
            chunk_index: chunkData.chunk_index,
            summary: analysis.summary,
            category: analysis.category,
            tags: analysis.tags,
            key_concepts: analysis.key_concepts,
            content_type: analysis.content_type,
            technical_level: analysis.technical_level,
            contextual_summary: contextualSummary,
            uses_contextual_embedding: contextualSummary !== null,
            processed_date: new Date().toISOString()
        };
        
        // üîç Log the full payload being sent to Qdrant
        console.log('  Qdrant payload keys:', Object.keys(payload));
        console.log('  Payload contextual fields:', {
            contextual_summary: payload.contextual_summary ? 'present' : 'null',
            uses_contextual_embedding: payload.uses_contextual_embedding,
            chunk_id: payload.chunk_id
        });

        const response = await axios.put(
            `${QDRANT_URL}/collections/autollama-content/points`,
            {
                points: [{
                    id: chunkData.chunk_id,
                    vector: embedding,
                    payload: payload
                }]
            },
            {
                headers: {
                    'api-key': QDRANT_API_KEY,
                    'Content-Type': 'application/json'
                }
            }
        );
        
        // üîç Log Qdrant response
        console.log('  Qdrant response status:', response.status);
        console.log('  Qdrant operation result:', response.data.result?.operation_id || 'success');
        
        return response.data;
    } catch (error) {
        console.error('‚ùå Qdrant storage failed:', error.message);
        console.error('   Response data:', error.response?.data);
        throw new Error(`Failed to store in Qdrant: ${error.message}`);
    }
}

async function storeInPostgreSQL(chunkData, analysis, embeddingStatus = 'unknown', sessionId = null, contextualSummary = null, uploadSource = 'user', parentDocumentId = null) {
    try {
        console.log('Storing in PostgreSQL with analysis:', analysis?.title || 'No title');
        
        // Ensure analysis has required fields
        if (!analysis || !analysis.title || !analysis.summary) {
            throw new Error('Analysis object missing required fields');
        }
        
        // Helper function to map embedding status
        const mapEmbeddingStatus = (status) => {
            const statusMap = {
                'success': 'complete',
                'failed': 'error',
                'pending': 'pending'
            };
            return statusMap[status] || 'pending';
        };

        // Build PostgreSQL data structure
        const contentData = {
            url: chunkData.original_url,
            title: analysis.title || 'Untitled',
            summary: analysis.summary || 'No summary available',
            chunk_text: chunkData.chunk_text,
            chunk_id: chunkData.chunk_id,
            chunk_index: chunkData.chunk_index || 0,
            sentiment: analysis.sentiment || null,
            emotions: analysis.emotions || [],
            category: analysis.category || null,
            content_type: analysis.content_type || 'article',
            technical_level: analysis.technical_level || 'intermediate',
            main_topics: analysis.main_topics || [],
            key_concepts: analysis.key_concepts || [],
            tags: Array.isArray(analysis.tags) ? analysis.tags.join(', ') : (analysis.tags || ''),
            key_entities: analysis.key_entities || {},
            embedding_status: mapEmbeddingStatus(embeddingStatus),
            processing_status: 'completed',
            contextual_summary: contextualSummary,
            uses_contextual_embedding: contextualSummary !== null,
            upload_source: uploadSource,
            record_type: 'chunk', // This is a chunk, not a document
            parent_document_id: parentDocumentId // Link to parent document
        };
        
        console.log('PostgreSQL payload:', contentData);
        
        const result = await db.addContentRecord(contentData);
        console.log('PostgreSQL storage successful!', result.id);
        return { id: result.id, status: 'success' };
    } catch (error) {
        console.error('POSTGRESQL STORAGE ERROR:');
        console.error('Error message:', error.message);
        console.error('Error details:', error);
        
        throw new Error(`Failed to store in PostgreSQL: ${error.message}`);
    }
}

// Upload session management functions
async function createUploadSession(filename, totalChunks, filePath, uploadSource = 'user') {
    try {
        const sessionId = uuidv4();
        const sessionData = {
            sessionId: sessionId,
            filename: filename,
            totalChunks: totalChunks,
            filePath: filePath,
            upload_source: uploadSource
        };
        
        const session = await db.createUploadSession(sessionData);
        console.log(`Upload session created: ${session.sessionId} (source: ${uploadSource})`);
        return { id: session.id, sessionId: session.sessionId };
    } catch (error) {
        console.error('Failed to create upload session:', error.message);
        throw new Error(`Failed to create upload session: ${error.message}`);
    }
}

async function updateUploadSession(sessionId, completedChunks, status = null) {
    try {
        const updateData = {
            completed_chunks: completedChunks
        };
        
        if (status) {
            updateData.status = status.toLowerCase();
        }
        
        const result = await db.updateUploadSession(sessionId, updateData);
        return result;
    } catch (error) {
        console.error('Failed to update upload session:', error.message);
        throw new Error(`Failed to update upload session: ${error.message}`);
    }
}

async function getUploadSession(sessionId) {
    try {
        // Query PostgreSQL directly for upload session
        const result = await db.pool.query(
            'SELECT * FROM upload_sessions WHERE session_id = $1',
            [sessionId]
        );
        
        if (result.rows.length > 0) {
            const session = result.rows[0];
            return {
                id: session.id,
                fields: {
                    'Session ID': session.session_id,
                    'Filename': session.filename,
                    'Total Chunks': session.total_chunks,
                    'Completed Chunks': session.completed_chunks,
                    'Status': session.status,
                    'File Path': session.file_path,
                    'Created At': session.created_at,
                    'Updated At': session.updated_at
                }
            };
        }
        return null;
    } catch (error) {
        console.error('Failed to get upload session:', error.message);
        throw new Error(`Failed to get upload session: ${error.message}`);
    }
}

async function getProcessedChunksForSession(sessionId) {
    try {
        // Get processed chunks from PostgreSQL by session ID
        const result = await db.pool.query(
            'SELECT chunk_id, chunk_index, processing_status FROM processed_content WHERE url LIKE $1',
            [`%session=${sessionId}%`]
        );
        
        return result.rows.map(row => ({
            chunkIndex: row.chunk_index,
            chunkId: row.chunk_id,
            status: row.processing_status || 'completed'
        }));
    } catch (error) {
        console.error('Failed to get processed chunks:', error.message);
        return [];
    }
}

// Enhanced SSE function for pipeline visualization
function sendSSEUpdate(res, step, message, progress = null, chunkData = null) {
    const data = {
        step,
        message,
        progress,
        timestamp: new Date().toISOString(),
        // Pipeline visualization data
        chunkId: chunkData?.chunkId,
        stage: step, // queue, fetch, convert, chunk, analyze, context, embed, bm25, store, complete
        filename: chunkData?.filename,
        position: chunkData?.position, // For animation positioning
        metadata: chunkData?.preview, // First 200 chars for preview
        sessionId: chunkData?.sessionId,
        totalChunks: chunkData?.totalChunks,
        currentChunk: chunkData?.currentChunk,
        chunkData: chunkData // Include full chunk data for Flow View
    };
    
    // Send to the specific processing stream
    res.write(`data: ${JSON.stringify(data)}\n\n`);
    
    // Broadcast to all global SSE clients (for Flow View)
    broadcastToSSEClients(data);
}

/* LEGACY UPLOAD ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION
// Server-Sent Events endpoint for real-time processing updates
app.post('/api/process-url-stream', async (req, res) => {
    // Set up SSE headers
    res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Cache-Control'
    });

    const startTime = Date.now();
    let processedChunks = 0;
    let qdrantStored = 0;
    let airtableStored = 0;

    try {
        const { url } = req.body;
        
        if (!url) {
            sendSSEUpdate(res, 'error', 'URL is required');
            res.end();
            return;
        }

        sendSSEUpdate(res, 'start', `üöÄ Starting processing for: ${url}`);

        // Step 1: Fetch web content
        sendSSEUpdate(res, 'fetch', 'üì• Fetching content from URL...');
        const contentData = await fetchWebContent(url);
        
        // Step 2: Content detection and conversion
        let markdown;
        if (contentData.type === 'pdf') {
            sendSSEUpdate(res, 'extract', `üìÑ PDF detected - extracting text from ${contentData.metadata.pages} pages...`);
            markdown = contentData.content;
        } else {
            sendSSEUpdate(res, 'convert', 'üîÑ HTML detected - converting to markdown...');
            markdown = htmlToMarkdown(contentData.content);
        }
        
        if (!markdown || markdown.trim().length < 100) {
            sendSSEUpdate(res, 'error', '‚ùå No substantial content found at URL');
            res.end();
            return;
        }

        // Step 3: Chunk the text
        sendSSEUpdate(res, 'chunk', '‚úÇÔ∏è Splitting content into chunks...');
        const chunks = chunkText(markdown, url);
        sendSSEUpdate(res, 'chunk', `‚úÖ Content split into ${chunks.length} chunks`);

        // Step 4: Process each chunk
        await processContentChunks(markdown, url, (type, data) => {
            sendSSEUpdate(res, type, data.message, data.progress, data);
        });
        
        const processingTime = Math.round((Date.now() - startTime) / 1000);
        
        sendSSEUpdate(res, 'complete', 
            `‚úÖ Processing complete! ${chunks.length} chunks processed in ${processingTime}s`
        );
        
        sendSSEUpdate(res, 'summary', JSON.stringify({
            status: 'success',
            url: url,
            chunks_total: chunks.length,
            chunks_processed: chunks.length,
            qdrant_stored: chunks.length,
            airtable_stored: chunks.length,
            processing_time: `${processingTime}s`
        }));
        
    } catch (error) {
        const processingTime = Math.round((Date.now() - startTime) / 1000);
        sendSSEUpdate(res, 'error', `‚ùå Processing failed: ${error.message}`);
        sendSSEUpdate(res, 'summary', JSON.stringify({
            status: 'error',
            error: error.message,
            chunks_processed: processedChunks,
            qdrant_stored: qdrantStored,
            airtable_stored: airtableStored,
            processing_time: `${processingTime}s`
        }));
    }
    
    res.end();
});
LEGACY UPLOAD ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION */

/* LEGACY UPLOAD ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION
// Main processing endpoint - replaces n8n webhook
app.post('/api/process-url', async (req, res) => {
    const startTime = Date.now();
    let processedChunks = 0;
    let qdrantStored = 0;
    let airtableStored = 0;
    
    try {
        const { url } = req.body;
        
        if (!url) {
            return res.status(400).json({ error: 'URL is required' });
        }
        
        console.log(`Starting processing for URL: ${url}`);
        
        // Step 1: Fetch web content
        console.log('Fetching web content...');
        const contentData = await fetchWebContent(url);
        
        // Step 2: Convert to markdown (or use PDF text directly)
        let markdown;
        if (contentData.type === 'pdf') {
            console.log(`Processing PDF with ${contentData.metadata.pages} pages...`);
            markdown = contentData.content; // PDF text is already plain text
        } else {
            console.log('Converting HTML to markdown...');
            markdown = htmlToMarkdown(contentData.content);
        }
        
        if (!markdown || markdown.trim().length < 100) {
            return res.status(400).json({ error: 'No substantial content found at URL' });
        }
        
        // Step 3: Chunk the text
        console.log('Chunking text...');
        const chunks = chunkText(markdown, url);
        console.log(`Created ${chunks.length} chunks`);
        
        // Step 4: Process each chunk
        const result = await processContentChunks(markdown, url);
        
        const processingTime = Math.round((Date.now() - startTime) / 1000);
        
        console.log(`Processing complete: ${result.processedChunks}/${result.chunks} chunks processed in ${processingTime}s`);
        
        res.json({
            status: 'success',
            url: url,
            chunks_total: result.chunks,
            chunks_processed: result.processedChunks,
            qdrant_stored: result.qdrantStored,
            airtable_stored: result.airtableStored,
            processing_time: `${processingTime}s`,
            timestamp: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('Processing failed:', error);
        
        const processingTime = Math.round((Date.now() - startTime) / 1000);
        
        res.status(500).json({
            status: 'error',
            error: error.message,
            chunks_processed: processedChunks,
            qdrant_stored: qdrantStored,
            airtable_stored: airtableStored,
            processing_time: `${processingTime}s`,
            timestamp: new Date().toISOString()
        });
    }
});
LEGACY UPLOAD ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION */

/* LEGACY ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION
// Route to get individual record from Airtable
app.get('/api/record', async (req, res) => {
    try {
        const recordId = req.query.id;
        
        if (!recordId) {
            return res.status(400).json({ error: 'Record ID is required' });
        }

        console.log(`üìÑ Fetching record: ${recordId}`);
        
        // Get record from PostgreSQL using the database function
        const record = await db.getRecordById(recordId);
        
        if (!record) {
            console.log(`‚ùå Record not found: ${recordId}`);
            return res.status(404).json({ error: 'Record not found' });
        }
        
        // Transform PostgreSQL record to frontend format
        const transformedRecord = {
            id: record.id || record.airtable_id,
            url: record.url || '',
            title: record.title || 'Untitled',
            summary: record.summary || '',
            content: record.chunk_text || '',
            processedDate: record.processed_date || record.created_time,
            category: record.category || 'General',
            status: record.processing_status === 'completed' ? 'Complete' : (record.processing_status || 'Unknown'),
            createdTime: record.created_time,
            
            // Rich metadata fields
            sentiment: record.sentiment || 'Neutral',
            emotions: record.emotions || [],
            tags: record.tags || '',
            keyEntities: record.key_entities || {},
            mainTopics: record.main_topics || [],
            keyConcepts: Array.isArray(record.key_concepts) ? record.key_concepts.join(', ') : (record.key_concepts || ''),
            contentType: record.content_type || 'other',
            technicalLevel: record.technical_level || 'intermediate',
            chunkId: record.chunk_id || '',
            chunkIndex: record.chunk_index || 0,
            chunkText: record.chunk_text || '',
            embeddingStatus: record.embedding_status || 'pending',
            source: record.source || 'autollama.io',
            links: '',
            
            // Additional PostgreSQL specific fields
            airtableId: record.airtable_id,
            vectorId: record.vector_id,
            sentToLi: record.sent_to_li,
            
            // üß† v2.0 Contextual Embeddings fields
            uses_contextual_embedding: record.uses_contextual_embedding || false,
            usesContextualEmbedding: record.uses_contextual_embedding || false, // Camel case alias
            contextual_summary: record.contextual_summary || null,
            contextualSummary: record.contextual_summary || null // Camel case alias
        };

        console.log(`‚úÖ Record found: ${transformedRecord.title}`);
        res.json(transformedRecord);
    } catch (error) {
        console.error('‚ùå Error fetching record from PostgreSQL:', error);
        res.status(500).json({ error: 'Failed to fetch record' });
    }
});
LEGACY ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION */

/* LEGACY ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION
// NEW: Smart recent records endpoint with real-time + cached performance
app.get('/api/recent-records', async (req, res) => {
    try {
        const startTime = Date.now();
        
        // Get smart content mix (real-time recent + cached historical)
        const result = await db.getSmartContentMix();
        
        const responseTime = Date.now() - startTime;
        
        // Add performance headers
        res.set({
            'X-Response-Time': `${responseTime}ms`,
            'X-Data-Source': 'postgresql-hybrid',
            'X-Recent-Count': result.metadata.recent_count,
            'X-Historical-Count': result.metadata.historical_count,
            'X-Cache-Status': result.metadata.cache_status
        });
        
        res.json(result.records);
        
    } catch (error) {
        console.error('‚ùå Error in smart recent records:', error.message);
        console.error(error.stack);
        
        // Fallback: return sample data to keep UI functional
        const fallbackData = [
            {
                id: 'fallback-1',
                url: 'https://fallback.example.com',
                title: 'Service Temporarily Unavailable',
                summary: 'The content service is temporarily unavailable. Please try again in a moment.',
                sentiment: 'Neutral',
                category: 'System',
                content_type: 'notice',
                embedding_status: 'complete',
                created_time: new Date().toISOString(),
                data_source: 'fallback'
            }
        ];
        
        res.status(200).json(fallbackData); // Return 200 to keep UI working
    }
});
LEGACY ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION */

// NEW: Dedicated search endpoint for full database search
// Legacy search routes - moved to new route system  
/*
app.get('/api/search', async (req, res) => {
    try {
        const query = req.query.q;
        const limit = parseInt(req.query.limit) || 50;
        
        if (!query || query.trim().length === 0) {
            return res.json([]);
        }
        
        const startTime = Date.now();
        
        // Track RAG activity
        lastRagActivity = new Date();
        
        // Use the database search function
        const results = await db.searchContent(query.trim(), limit);
        
        const responseTime = Date.now() - startTime;
        
        // Add performance headers
        res.set({
            'X-Response-Time': `${responseTime}ms`,
            'X-Search-Query': query,
            'X-Results-Count': results.length,
            'X-Data-Source': 'postgresql-search'
        });
        
        res.json(results);
        
    } catch (error) {
        console.error('‚ùå Search error:', error.message);
        res.status(500).json({ 
            error: 'Search failed', 
            message: error.message 
        });
    }
});
*/

// NEW: Get all documents with chunk counts and processing status - moved to new route system
/*
app.get('/api/documents', async (req, res) => {
    try {
        const limit = parseInt(req.query.limit) || 50;
        const offset = parseInt(req.query.offset) || 0;
        
        const documents = await db.getDocumentsOnly(limit, offset);
        res.json({ documents });
    } catch (error) {
        console.error('Error fetching documents:', error);
        res.status(500).json({ error: 'Failed to fetch documents' });
    }
});

/* LEGACY ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION
// NEW: Get all chunks for a specific document by URL with pagination (moved before documentId to avoid conflicts)
// NEW: Document chunks API that properly handles URLs with query parameters (fixes encoding issues)
app.get('/api/document-chunks', async (req, res) => {
    try {
        const url = req.query.url;
        const chunkIndex = req.query.index ? parseInt(req.query.index) : null;
        const page = parseInt(req.query.page) || 1;
        const limit = parseInt(req.query.limit) || 100;
        
        if (!url) {
            return res.status(400).json({ 
                error: 'Missing required parameter',
                message: 'URL parameter is required'
            });
        }
        
        // If index is specified, get that specific chunk
        if (chunkIndex !== null && !isNaN(chunkIndex)) {
            console.log(`   URL: ${url}`);
            console.log(`   Index: ${chunkIndex}`);
            const startTime = Date.now();
            
            // Get the specific chunk by index, handling duplicates by taking the first one
            const query = `
                SELECT 
                    id, chunk_id, chunk_index, title, summary, chunk_text,
                    sentiment, emotions, category, content_type, technical_level,
                    main_topics, key_concepts, tags, key_entities,
                    embedding_status, processing_status, created_time, processed_date,
                    contextual_summary, uses_contextual_embedding
                FROM processed_content
                WHERE url = $1 AND chunk_index = $2
                ORDER BY created_time ASC
                LIMIT 1
            `;
            
            const result = await db.pool.query(query, [url, chunkIndex]);
            const responseTime = Date.now() - startTime;
            
            if (result.rows.length === 0) {
                console.log(`‚ùå Chunk ${chunkIndex} not found for document`);
                return res.status(404).json({ 
                    error: 'Chunk not found',
                    message: `Chunk with index ${chunkIndex} not found for this document`
                });
            }
            
            const chunk = result.rows[0];
            const formattedChunk = {
                id: chunk.id,
                chunkId: chunk.chunk_id,
                chunkIndex: chunk.chunk_index,
                title: chunk.title,
                summary: chunk.summary,
                chunkText: chunk.chunk_text,
                sentiment: chunk.sentiment,
                emotions: chunk.emotions,
                category: chunk.category,
                contentType: chunk.content_type,
                technicalLevel: chunk.technical_level,
                mainTopics: chunk.main_topics,
                keyConcepts: chunk.key_concepts,
                tags: chunk.tags,
                keyEntities: chunk.key_entities,
                embeddingStatus: chunk.embedding_status,
                processingStatus: chunk.processing_status,
                createdTime: chunk.created_time,
                processedDate: chunk.processed_date,
                contextualSummary: chunk.contextual_summary,
                usesContextualEmbedding: chunk.uses_contextual_embedding
            };
            
            console.log(`‚úÖ Chunk by index API completed in ${responseTime}ms`);
            console.log(`   Found chunk: ${chunk.chunk_id} with ${chunk.chunk_text ? chunk.chunk_text.length : 0} chars`);
            
            return res.json({ chunk: formattedChunk });
        }
        
        // Otherwise, use normal pagination
        console.log(`üß© Document chunks API called (query-based):`);
        console.log(`   URL: ${url}`);
        console.log(`   Page: ${page}, Limit: ${limit}`);
        const startTime = Date.now();
        
        const result = await db.getDocumentChunks(url, page, limit);
        
        const responseTime = Date.now() - startTime;
        
        console.log(`‚úÖ Document chunks API completed in ${responseTime}ms`);
        console.log(`   Found ${result.chunks.length} chunks (page ${result.pagination.currentPage}/${result.pagination.totalPages})`);
        
        res.json(result);
    } catch (error) {
        console.error('Error fetching document chunks:', error);
        res.status(500).json({ 
            error: 'Failed to fetch chunks',
            message: error.message 
        });
    }
});
LEGACY ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION */

// FIXED: Use catch-all route pattern to handle URLs with slashes properly
app.get('/api/document/*/chunks', async (req, res) => {
    try {
        // Extract everything after /api/document/ and before /chunks
        const fullPath = req.originalUrl;
        const match = fullPath.match(/\/api\/document\/(.+)\/chunks/);
        
        if (!match) {
            return res.status(400).json({ 
                error: 'Invalid URL format',
                message: 'Expected format: /api/document/{encodedUrl}/chunks'
            });
        }
        
        const encodedUrl = match[1];
        
        // Check if this looks like a UUID (to route to documentId endpoint)
        const uuidRegex = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i;
        if (uuidRegex.test(encodedUrl)) {
            // This is actually a document ID, route to the ID-based handler
            const chunks = await db.getChunksByDocumentId(encodedUrl);
            return res.json({ chunks });
        }
        
        // This is a URL, handle it as such
        const url = decodeURIComponent(encodedUrl);
        const page = parseInt(req.query.page) || 1;
        const limit = parseInt(req.query.limit) || 100;
        
        console.log(`üß© Document chunks API called (wildcard-based):`);
        console.log(`   Raw param: ${encodedUrl}`);
        console.log(`   Decoded URL: ${url}`);
        console.log(`   Page: ${page}, Limit: ${limit}`);
        const startTime = Date.now();
        
        const result = await db.getDocumentChunks(url, page, limit);
        
        const responseTime = Date.now() - startTime;
        
        console.log(`‚úÖ Document chunks API completed in ${responseTime}ms`);
        console.log(`   Found ${result.chunks.length} chunks (page ${result.pagination.currentPage}/${result.pagination.totalPages})`);
        
        res.json(result);
    } catch (error) {
        console.error('Error fetching document chunks:', error);
        res.status(500).json({ 
            error: 'Failed to fetch chunks',
            message: error.message 
        });
    }
});

// DEBUG: Test endpoint to verify API changes are working
app.get('/api/debug-test', (req, res) => {
    console.log('üß™ Debug test endpoint called');
    res.json({ message: 'Debug endpoint working', timestamp: new Date().toISOString() });
});

// DEPRECATED: This endpoint is now handled by the unified endpoint above
// app.get('/api/document/:documentId/chunks', async (req, res) => {

// NEW: Search content grouped by document
app.get('/api/search/grouped', async (req, res) => {
    try {
        const query = req.query.q;
        const limit = parseInt(req.query.limit) || 50;
        
        if (!query || query.trim().length === 0) {
            return res.json([]);
        }
        
        const startTime = Date.now();
        
        const results = await db.searchContentGrouped(query.trim(), limit);
        
        const responseTime = Date.now() - startTime;
        console.log(`‚úÖ Grouped search API completed in ${responseTime}ms, found ${results.length} document groups`);
        
        res.json(results);
        
    } catch (error) {
        console.error('‚ùå Grouped search API error:', error.message);
        res.status(500).json({ 
            error: 'Grouped search failed', 
            message: error.message 
        });
    }
});

// NEW: Get document summary by URL
app.get('/api/document/:encodedUrl/summary', async (req, res) => {
    try {
        const encodedUrl = req.params.encodedUrl;
        const url = decodeURIComponent(encodedUrl);
        
        console.log(`üìã Document summary API called:`);
        console.log(`   Raw param: ${encodedUrl}`);
        console.log(`   Decoded URL: ${url}`);
        const startTime = Date.now();
        
        const summary = await db.getDocumentSummary(url);
        
        const responseTime = Date.now() - startTime;
        
        if (summary) {
            console.log(`‚úÖ Document summary API completed in ${responseTime}ms`);
            console.log(`   Found document: ${summary.title}`);
            res.json(summary);
        } else {
            console.log(`‚ùå Document not found: ${url}`);
            // Try to find similar URLs in the database
            const allDocs = await db.getAllDocuments(10, 0);
            console.log(`   Available URLs in DB:`);
            allDocs.forEach(doc => console.log(`     - ${doc.url}`));
            
            res.status(404).json({ 
                error: 'Document not found', 
                url: url,
                availableUrls: allDocs.map(d => d.url)
            });
        }
        
    } catch (error) {
        console.error('‚ùå Document summary API error:', error.message);
        res.status(500).json({ 
            error: 'Failed to fetch document summary', 
            message: error.message 
        });
    }
});

/* LEGACY ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION
// NEW: Get all chunks
app.get('/api/chunks', async (req, res) => {
    try {
        const limit = parseInt(req.query.limit) || 100;
        const offset = parseInt(req.query.offset) || 0;
        
        console.log(`üß© Chunks API called (limit: ${limit}, offset: ${offset})`);
        const startTime = Date.now();
        
        const chunks = await db.getAllChunks(limit, offset);
        
        // Ensure proper JSON serialization
        const safeChunks = chunks.map(chunk => ({
            ...chunk,
            emotions: Array.isArray(chunk.emotions) ? chunk.emotions : [],
            mainTopics: Array.isArray(chunk.mainTopics) ? chunk.mainTopics : [],
            keyConcepts: Array.isArray(chunk.keyConcepts) ? chunk.keyConcepts : [],
            keyEntities: chunk.keyEntities || {},
            createdTime: chunk.createdTime ? new Date(chunk.createdTime).toISOString() : null,
            processedDate: chunk.processedDate ? new Date(chunk.processedDate).toISOString() : null
        }));
        
        const responseTime = Date.now() - startTime;
        console.log(`‚úÖ Chunks API completed in ${responseTime}ms, returned ${safeChunks.length} chunks`);
        
        res.json(safeChunks);
        
    } catch (error) {
        console.error('‚ùå Chunks API error:', error.message, error.stack);
        res.status(500).json({ 
            error: 'Failed to fetch chunks', 
            message: error.message 
        });
    }
});
LEGACY ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION */

app.get('/api/pipeline/download', async (req, res) => {
    try {
        const fs = require('fs');
        const path = require('path');
        
        // Read the pipeline template
        const pipelinePath = path.join(__dirname, 'autollama_rag_pipeline.py');
        let pipelineContent = fs.readFileSync(pipelinePath, 'utf8');
        
        // Replace placeholders with actual configuration
        pipelineContent = pipelineContent.replace(
            'QDRANT_URL: str = "https://c4c8ee46-d9dd-4c0f-a00e-9215675351da.us-west-1-0.aws.cloud.qdrant.io"',
            `QDRANT_URL: str = "${QDRANT_URL}"`
        );
        
        pipelineContent = pipelineContent.replace(
            'QDRANT_API_KEY: str = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiO2JtIn0.ghMgF9xxBObWVQnMQab9wCk7JP4jkHI7k4Z1TYo8zqg"',
            `QDRANT_API_KEY: str = "${QDRANT_API_KEY}"`
        );
        
        // Add current date
        const currentDate = new Date().toISOString().split('T')[0];
        pipelineContent = pipelineContent.replace(
            'date: 2025-07-25',
            `date: ${currentDate}`
        );
        
        // Set response headers for file download
        res.setHeader('Content-Type', 'text/x-python');
        res.setHeader('Content-Disposition', 'attachment; filename="autollama_rag_pipeline.py"');
        res.setHeader('Access-Control-Allow-Origin', '*');
        
        res.send(pipelineContent);
        
    } catch (error) {
        console.error('Error generating pipeline:', error);
        res.status(500).json({ error: 'Failed to generate pipeline file' });
    }
});

// Knowledge base stats endpoint
app.get('/api/knowledge-base/stats', async (req, res) => {
    try {
        
        // Get stats directly from database using existing functions
        const dbStats = await db.getDatabaseStats();
        
        
        res.json({
            success: true,
            total_documents: dbStats.total_urls || 0,
            total_chunks: dbStats.total_chunks || 0,
            contextual_documents: dbStats.contextual_count || 0,
            qdrant_status: (dbStats.total_chunks || 0) > 0 ? 'active' : 'inactive',
            processing_stats: {
                total_uploads: dbStats.total_chunks || 0,
                completed_uploads: dbStats.embedded_count || 0,
                processing_uploads: dbStats.active_sessions || 0,
                failed_uploads: 0
            },
            lastUpdated: new Date().toISOString(),
            timestamp: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('‚ùå Error getting knowledge base stats:', error);
        res.status(500).json({
            success: false,
            error: 'Failed to retrieve knowledge base statistics',
            total_documents: 0,
            total_chunks: 0,
            contextual_documents: 0,
            qdrant_status: 'error',
            processing_stats: {
                total_uploads: 0,
                completed_uploads: 0,
                processing_uploads: 0,
                failed_uploads: 0
            },
            lastUpdated: new Date().toISOString(),
            timestamp: new Date().toISOString()
        });
    }
});

// Health check endpoint
app.get('/health', async (req, res) => {
    console.log('Health endpoint called');
    
    try {
        // Test database connectivity
        const dbConnected = await db.testConnection();
        
        // Get API keys from database settings
        const dbOpenAIKey = await db.getApiSetting('openai_api_key');
        const dbQdrantKey = await db.getApiSetting('qdrant_api_key');
        const dbQdrantUrl = await db.getApiSetting('qdrant_url');
        
        res.json({ 
            status: 'OK',
            version: '2.0.0',
            contextual_embeddings: {
                enabled: process.env.ENABLE_CONTEXTUAL_EMBEDDINGS !== 'false',
                model: process.env.CONTEXTUAL_EMBEDDING_MODEL || 'gpt-4o-mini',
                batch_size: parseInt(process.env.CONTEXT_GENERATION_BATCH_SIZE || '5')
            },
            database: {
                postgresql: dbConnected ? 'connected' : 'disconnected',
                qdrant: (dbQdrantUrl || process.env.QDRANT_URL) ? 'configured' : 'not_configured'
            },
            api_keys: {
                openai: (dbOpenAIKey || process.env.OPENAI_API_KEY) ? 'configured' : 'missing',
                qdrant: (dbQdrantKey || process.env.QDRANT_API_KEY) ? 'configured' : 'missing',
                // airtable removed - system uses PostgreSQL exclusively
            },
            timestamp: new Date().toISOString()
        });
    } catch (error) {
        res.status(500).json({
            status: 'ERROR',
            version: '2.0.0',
            error: error.message,
            timestamp: new Date().toISOString()
        });
    }
});

// Comprehensive system status endpoint
app.get('/api/system/status', async (req, res) => {
    console.log('System status endpoint called');
    
    try {
        const startTime = Date.now();
        
        // Test all system components in parallel
        const [
            dbConnected,
            qdrantStatus,
            bm25Status,
            dbStats
        ] = await Promise.allSettled([
            // Test PostgreSQL
            db.testConnection(),
            // Test Qdrant
            testQdrantConnection(),
            // Test BM25 service
            testBM25Service(),
            // Get database stats
            db.getDatabaseStats()
        ]);

        const responseTime = Date.now() - startTime;
        
        // Calculate overall health
        const services = {
            postgresql: {
                status: dbConnected.status === 'fulfilled' && dbConnected.value ? 'healthy' : 'unhealthy',
                response_time: dbConnected.status === 'fulfilled' ? '< 100ms' : 'timeout',
                details: dbConnected.status === 'rejected' ? dbConnected.reason?.message : 'Connected'
            },
            qdrant: {
                status: qdrantStatus.status === 'fulfilled' && qdrantStatus.value ? 'healthy' : 'unhealthy',
                response_time: qdrantStatus.status === 'fulfilled' ? '< 500ms' : 'timeout',
                details: qdrantStatus.status === 'rejected' ? qdrantStatus.reason?.message : 'Vector database operational'
            },
            bm25: {
                status: bm25Status.status === 'fulfilled' && bm25Status.value ? 'healthy' : 'unhealthy',
                response_time: bm25Status.status === 'fulfilled' ? '< 200ms' : 'timeout',
                details: bm25Status.status === 'rejected' ? 'Service unavailable' : 'Search service operational'
            }
        };

        const healthyServices = Object.values(services).filter(s => s.status === 'healthy').length;
        const totalServices = Object.keys(services).length;
        const overallHealth = healthyServices === totalServices ? 'healthy' : 
                             healthyServices >= totalServices * 0.75 ? 'degraded' : 'unhealthy';

        res.json({
            status: overallHealth.toUpperCase(),
            version: '2.1.0',
            health: {
                overall: overallHealth,
                services_healthy: `${healthyServices}/${totalServices}`,
                percentage: Math.round((healthyServices / totalServices) * 100)
            },
            services,
            data: dbStats.status === 'fulfilled' ? dbStats.value : null,
            response_time_ms: responseTime,
            timestamp: new Date().toISOString()
        });
    } catch (error) {
        res.status(500).json({
            status: 'ERROR',
            version: '2.1.0',
            error: error.message,
            timestamp: new Date().toISOString()
        });
    }
});

// Helper function to test Qdrant connection
async function testQdrantConnection() {
    try {
        const qdrantUrl = process.env.QDRANT_URL || await db.getApiSetting('qdrant_url');
        if (!qdrantUrl) return false;
        
        const response = await axios.get(`${qdrantUrl}/health`, {
            timeout: 3000,
            headers: process.env.QDRANT_API_KEY ? { 'api-key': process.env.QDRANT_API_KEY } : {}
        });
        return response.status === 200;
    } catch (error) {
        console.log('Qdrant health check failed:', error.message);
        return false;
    }
}

// Helper function to test BM25 service
async function testBM25Service() {
    // Use new storage services if available, fallback to original implementation
    if (storageServices && storageServices.bm25Service) {
        try {
            const testResult = await storageServices.bm25Service.testService();
            return testResult.success;
        } catch (error) {
            console.warn('New BM25 service test failed, falling back to original:', error.message);
        }
    }

    // Original implementation as fallback
    try {
        const response = await axios.get('http://autollama-on-hstgr:3002/health', {
            timeout: 2000
        });
        return response.status === 200;
    } catch (error) {
        console.log('BM25 health check failed:', error.message);
        return false;
    }
}

// Settings management endpoints
// Legacy settings routes - moved to new route system
/*
app.get('/api/settings', async (req, res) => {
    try {
        console.log('üîß API Settings GET endpoint called');
        const settings = await db.getApiSettings();
        
        res.json({
            success: true,
            settings: settings,
            timestamp: new Date().toISOString()
        });
    } catch (error) {
        console.error('‚ùå Error fetching API settings:', error.message);
        res.status(500).json({
            success: false,
            error: error.message,
            timestamp: new Date().toISOString()
        });
    }
});

app.post('/api/settings', async (req, res) => {
    try {
        console.log('üîß API Settings POST endpoint called');
        const { settings } = req.body;
        
        if (!settings || typeof settings !== 'object') {
            return res.status(400).json({
                success: false,
                error: 'Settings object is required',
                timestamp: new Date().toISOString()
            });
        }
        
        // Update settings in database
        const results = await db.updateApiSettings(settings);
        
        console.log(`‚úÖ Updated ${results.length} API settings`);
        
        res.json({
            success: true,
            message: `Updated ${results.length} settings`,
            updated: results.map(r => r.setting_key),
            timestamp: new Date().toISOString()
        });
    } catch (error) {
        console.error('‚ùå Error updating API settings:', error.message);
        res.status(500).json({
            success: false,
            error: error.message,
            timestamp: new Date().toISOString()
        });
    }
});

app.get('/api/settings/:key', async (req, res) => {
    try {
        const { key } = req.params;
        console.log(`üîß API Settings GET single key: ${key}`);
        
        const value = await db.getApiSetting(key);
        
        if (value === null) {
            return res.status(404).json({
                success: false,
                error: `Setting '${key}' not found`,
                timestamp: new Date().toISOString()
            });
        }
        
        res.json({
            success: true,
            key: key,
            value: value,
            timestamp: new Date().toISOString()
        });
    } catch (error) {
        console.error(`‚ùå Error fetching API setting ${req.params.key}:`, error.message);
        res.status(500).json({
            success: false,
            error: error.message,
            timestamp: new Date().toISOString()
        });
    }
});
*/

// NEW: Real-time in-progress endpoint with in-memory tracking - moved to new route system
/*
app.get('/api/in-progress', async (req, res) => {
    try {
        const startTime = Date.now();
        
        // Get active sessions from in-memory tracking
        const sessions = Array.from(activeProcessingSessions.values());
        
        const responseTime = Date.now() - startTime;
        
        // Transform to match frontend expectations
        const transformedSessions = sessions.map(session => ({
            id: session.id,
            url: session.url,
            filename: session.filename || 'Unknown File',
            title: session.title || session.filename || 'Processing...',
            totalChunks: session.totalChunks || 0,
            completedChunks: session.processedChunks || 0,
            processedChunks: session.processedChunks || 0,
            status: session.status,
            lastActivity: session.lastUpdate ? session.lastUpdate.toISOString() : new Date().toISOString(),
            createdAt: session.startTime ? session.startTime.toISOString() : new Date().toISOString(),
            progress: session.totalChunks > 0 ? 
                Math.round((session.processedChunks / session.totalChunks) * 100) : 0
        }));
        
        // Add performance headers
        res.set({
            'X-Response-Time': `${responseTime}ms`,
            'X-Data-Source': 'memory-realtime',
            'X-Active-Sessions': sessions.length
        });
        
        res.json(transformedSessions);
        
    } catch (error) {
        console.error('‚ùå Error in real-time in-progress:', error.message);
        
        // Return empty array to keep UI functional
        res.status(200).json([]);
    }
});
*/

// Upload session status endpoint for reconnection - moved to new route system
/*
app.get('/api/upload-session/:sessionId', async (req, res) => {
    try {
        const { sessionId } = req.params;
        
        // Check if session exists in active processing sessions
        const activeSession = activeProcessingSessions.get(sessionId);
        
        if (activeSession) {
            res.json({
                sessionId: sessionId,
                status: 'processing',
                progress: activeSession.totalChunks > 0 ? 
                    Math.round((activeSession.processedChunks / activeSession.totalChunks) * 100) : 0,
                filename: activeSession.filename,
                totalChunks: activeSession.totalChunks,
                processedChunks: activeSession.processedChunks
            });
        } else {
            // Check database for completed sessions
            try {
                const dbResult = await db.pool.query(
                    'SELECT * FROM upload_sessions WHERE session_id = $1',
                    [sessionId]
                );
                
                if (dbResult.rows.length > 0) {
                    const session = dbResult.rows[0];
                    res.json({
                        sessionId: sessionId,
                        status: session.status,
                        progress: 100,
                        filename: session.filename,
                        completedAt: session.updated_at
                    });
                } else {
                    res.status(404).json({ error: 'Session not found' });
                }
            } catch (dbError) {
                console.error('Database error checking session:', dbError);
                res.status(404).json({ error: 'Session not found' });
            }
        }
    } catch (error) {
        console.error('Error checking upload session:', error);
        res.status(500).json({ error: 'Failed to check session status' });
    }
});
*/

// Session-specific SSE stream for reconnection
app.get('/api/session-stream/:sessionId', (req, res) => {
    const { sessionId } = req.params;
    
    // Set up SSE headers
    res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*'
    });
    
    // Check if session is still active
    const activeSession = activeProcessingSessions.get(sessionId);
    if (!activeSession) {
        res.write(`data: ${JSON.stringify({ 
            type: 'session_not_found', 
            message: 'Session not found or completed' 
        })}\n\n`);
        res.end();
        return;
    }
    
    // Add client to SSE broadcast list
    const clientId = `reconnect-${Date.now()}`;
    sseClients.set(clientId, res);
    
    // Send current session status
    res.write(`data: ${JSON.stringify({ 
        type: 'session_reconnected', 
        data: {
            sessionId: sessionId,
            progress: activeSession.totalChunks > 0 ? 
                Math.round((activeSession.processedChunks / activeSession.totalChunks) * 100) : 0,
            status: 'processing',
            filename: activeSession.filename
        }
    })}\n\n`);
    
    // Handle client disconnect
    res.on('close', () => {
        sseClients.delete(clientId);
    });
});

// File processing utilities
async function streamToBuffer(stream) {
    return new Promise((resolve, reject) => {
        const chunks = [];
        stream.on('data', chunk => chunks.push(chunk));
        stream.on('end', () => resolve(Buffer.concat(chunks)));
        stream.on('error', reject);
    });
}

class FileProcessor {
    constructor() {
        this.parsers = {
            'application/pdf': this.parsePDF.bind(this),
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document': this.parseDOCX.bind(this),
            'application/epub+zip': this.parseEPUB.bind(this),
            'text/plain': this.parseText.bind(this),
            'text/csv': this.parseCSV.bind(this),
            'text/html': this.parseHTML.bind(this),
            'application/msword': this.parseDOCX.bind(this) // fallback for older Word docs
        };
    }

    async selectParser(mimeType, filename) {
        // Try mime type first
        if (this.parsers[mimeType]) {
            return this.parsers[mimeType];
        }
        
        // Fallback to file extension
        const ext = filename.toLowerCase().split('.').pop();
        const extToMime = {
            'pdf': 'application/pdf',
            'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'doc': 'application/msword',
            'epub': 'application/epub+zip',
            'txt': 'text/plain',
            'csv': 'text/csv',
            'html': 'text/html',
            'htm': 'text/html'
        };
        
        const detectedMime = extToMime[ext];
        if (detectedMime && this.parsers[detectedMime]) {
            return this.parsers[detectedMime];
        }
        
        throw new Error(`Unsupported file type: ${mimeType} (${filename})`);
    }

    async parsePDF(buffer) {
        const data = await pdfParse(buffer);
        return {
            content: data.text,
            type: 'pdf',
            metadata: {
                pages: data.numpages,
                info: data.info
            }
        };
    }

    async parseDOCX(buffer) {
        const result = await mammoth.extractRawText({ buffer });
        return {
            content: result.value,
            type: 'docx',
            metadata: {
                messages: result.messages
            }
        };
    }

    async parseEPUB(buffer) {
        return new Promise((resolve, reject) => {
            // Add overall timeout for EPUB parsing
            const parseTimeout = setTimeout(() => {
                console.error('‚ùå EPUB parsing timeout after 60 seconds');
                reject(new Error('EPUB parsing timeout - file may be corrupted or too large'));
            }, 60000); // 60 second timeout
            
            // Write buffer to temporary file since epub library requires file path
            tmp.file({ postfix: '.epub' }, (err, path, fd, cleanupCallback) => {
                if (err) {
                    clearTimeout(parseTimeout);
                    reject(err);
                    return;
                }
                
                console.log('üìñ EPUB: Writing buffer to temp file for parsing');
                require('fs').writeFile(path, buffer, (writeErr) => {
                    if (writeErr) {
                        clearTimeout(parseTimeout);
                        cleanupCallback();
                        reject(writeErr);
                        return;
                    }
                    
                    console.log('üìñ EPUB: Starting EPUB parser');
                    // Parse EPUB
                    const epubParser = new epub(path);
                    
                    epubParser.on('error', (error) => {
                        console.error('‚ùå EPUB parsing error:', error.message);
                        clearTimeout(parseTimeout);
                        cleanupCallback();
                        reject(error);
                    });
                    
                    epubParser.on('end', () => {
                        console.log('üìñ EPUB: Parser finished, processing chapters');
                        let content = '';
                        const chapters = epubParser.flow;
                        
                        let processed = 0;
                        const totalChapters = chapters.length;
                        console.log(`üìñ EPUB: Found ${totalChapters} chapters to process`);
                        
                        if (totalChapters === 0) {
                            clearTimeout(parseTimeout);
                            cleanupCallback();
                            resolve({
                                content: '',
                                type: 'epub',
                                metadata: {
                                    title: epubParser.metadata.title,
                                    author: epubParser.metadata.creator,
                                    language: epubParser.metadata.language
                                }
                            });
                            return;
                        }
                        
                        // Add timeout for chapter processing
                        const chapterTimeout = setTimeout(() => {
                            console.error('‚ùå EPUB chapter processing timeout');
                            clearTimeout(parseTimeout);
                            cleanupCallback();
                            reject(new Error('EPUB chapter processing timeout'));
                        }, 120000); // 2 minute timeout for chapter processing
                        
                        chapters.forEach((chapter, index) => {
                            epubParser.getChapter(chapter.id, (error, text) => {
                                if (!error && text) {
                                    // Remove HTML tags
                                    const $ = cheerio.load(text);
                                    const chapterText = $.text();
                                    content += chapterText + '\n\n';
                                    console.log(`üìñ EPUB: Processed chapter ${index + 1}/${totalChapters} (${chapterText.length} chars)`);
                                } else if (error) {
                                    console.warn(`‚ö†Ô∏è EPUB: Failed to read chapter ${index + 1}:`, error.message);
                                }
                                
                                processed++;
                                
                                if (processed === totalChapters) {
                                    clearTimeout(parseTimeout);
                                    clearTimeout(chapterTimeout);
                                    cleanupCallback();
                                    console.log(`‚úÖ EPUB: Completed processing ${totalChapters} chapters (${content.length} total chars)`);
                                    resolve({
                                        content: content.trim(),
                                        type: 'epub',
                                        metadata: {
                                            title: epubParser.metadata.title,
                                            author: epubParser.metadata.creator,
                                            language: epubParser.metadata.language
                                        }
                                    });
                                }
                            });
                        });
                    });
                    
                    epubParser.parse();
                });
            });
        });
    }

    async parseText(buffer) {
        return {
            content: buffer.toString('utf-8'),
            type: 'text',
            metadata: {}
        };
    }

    async parseCSV(buffer) {
        return new Promise((resolve, reject) => {
            const content = buffer.toString('utf-8');
            const rows = [];
            
            csvParse(content, {
                columns: true,
                skip_empty_lines: true
            }, (err, records) => {
                if (err) {
                    reject(err);
                    return;
                }
                
                // Convert CSV to readable text
                let textContent = '';
                if (records.length > 0) {
                    const headers = Object.keys(records[0]);
                    textContent = `CSV Data with columns: ${headers.join(', ')}\n\n`;
                    
                    records.forEach((row, index) => {
                        textContent += `Row ${index + 1}:\n`;
                        headers.forEach(header => {
                            textContent += `${header}: ${row[header]}\n`;
                        });
                        textContent += '\n';
                    });
                }
                
                resolve({
                    content: textContent,
                    type: 'csv',
                    metadata: {
                        rowCount: records.length,
                        columns: records.length > 0 ? Object.keys(records[0]) : []
                    }
                });
            });
        });
    }

    async parseHTML(buffer) {
        const html = buffer.toString('utf-8');
        const $ = cheerio.load(html);
        
        // Remove script and style elements
        $('script, style').remove();
        
        // Extract text content
        const content = $.text().replace(/\s+/g, ' ').trim();
        
        return {
            content,
            type: 'html',
            metadata: {
                title: $('title').text(),
                description: $('meta[name="description"]').attr('content')
            }
        };
    }
}

// Initialize file processor
const fileProcessor = new FileProcessor();

// Simplified upload progress endpoint (Node 18 compatible)
app.get('/api/upload-progress/:uploadId', (req, res) => {
    // Set headers for Server-Sent Events
    res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Cache-Control'
    });
    
    // Send initial connection confirmation
    res.write(`data: ${JSON.stringify({ 
        event: 'connected', 
        data: { 
            uploadId: req.params.uploadId, 
            timestamp: new Date().toISOString() 
        } 
    })}\n\n`);
    
    // Keep connection alive
    const keepAlive = setInterval(() => {
        res.write(`data: ${JSON.stringify({ event: 'heartbeat', data: { timestamp: new Date().toISOString() } })}\n\n`);
    }, 30000);
    
    // Handle client disconnect
    req.on('close', () => {
        clearInterval(keepAlive);
    });
});

// Use existing processing infrastructure (Node 18 compatible)
console.log('Using existing file processing infrastructure');

// Restore original working file upload endpoint with connect-busboy optimization
// Add test endpoint to verify routing
app.get('/api/test-routing', (req, res) => {
    console.log('üî• Test routing endpoint hit');
    res.json({ message: 'Routing works!', timestamp: new Date() });
});

// TEMPORARY: Re-enable legacy file upload route until new system is properly configured
console.log('üî• Registering /api/process-file-stream endpoint');
app.post('/api/process-file-stream', (req, res) => {
    console.log('üî• File upload endpoint hit - starting process');
    
    // Set headers for SSE
    res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Cache-Control'
    });

    let processingId = uuidv4();
    
    function sendSSEUpdate(event, data) {
        res.write(`data: ${JSON.stringify({ event, data })}\n\n`);
    }

    sendSSEUpdate('start', { processingId, message: 'Starting file upload...' });
    console.log('üî• SSE headers sent, creating busboy');

    const busboy = Busboy({ 
        headers: req.headers,
        limits: {
            fileSize: 100 * 1024 * 1024 // 100MB limit
        }
    });

    busboy.on('file', async (name, file, info) => {
        let uploadSession = null;
        let tempFilePath = null;
        
        try {
            const { filename, mimeType } = info;
            console.log('üî• File upload started:', filename, mimeType);
            sendSSEUpdate('upload', { progress: 5, message: `Uploading ${filename}...` });
            
            // Stream file to buffer
            const fileBuffer = await streamToBuffer(file);
            sendSSEUpdate('upload', { progress: 25, message: 'File uploaded, starting processing...' });
            
            // Save temporary file using tmp library
            const fs = require('fs');
            const tmpFile = tmp.fileSync({ postfix: `-${filename}`, keep: true });
            tempFilePath = tmpFile.name;
            fs.writeFileSync(tempFilePath, fileBuffer);
            sendSSEUpdate('upload', { progress: 30, message: 'File saved, analyzing content...' });
            
            // Select and execute parser with timeout
            const parser = await fileProcessor.selectParser(mimeType, filename);
            sendSSEUpdate('parse', { progress: 40, message: 'Parsing file content...' });
            
            console.log(`üîç Starting parser for ${mimeType} file: ${filename}`);
            const parseResult = await Promise.race([
                parser(fileBuffer),
                new Promise((_, reject) => 
                    setTimeout(() => reject(new Error(`File parsing timeout after 3 minutes for ${filename}`)), 180000)
                )
            ]);
            console.log(`‚úÖ Parsing completed for ${filename} (${parseResult.content.length} chars)`);
            sendSSEUpdate('parse', { progress: 50, message: 'Content parsed, creating chunks...' });
            
            // Create chunks to determine total count
            const chunks = chunkText(parseResult.content, `file://${filename}`);
            sendSSEUpdate('parse', { progress: 55, message: `Content split into ${chunks.length} chunks` });
            
            // Create upload session
            try {
                uploadSession = await createUploadSession(filename, chunks.length, tempFilePath, 'user');
                sendSSEUpdate('session', { 
                    progress: 60, 
                    message: 'Upload session created',
                    sessionId: uploadSession.sessionId 
                });
            } catch (sessionError) {
                console.error('Failed to create upload session:', sessionError);
                // Continue without session tracking if Airtable is not available
                uploadSession = { id: null, sessionId: processingId };
            }
            
            // Track this processing session in memory for /api/in-progress endpoint
            activeProcessingSessions.set(uploadSession.sessionId, {
                id: uploadSession.sessionId,
                url: tempFilePath,
                filename: filename,
                totalChunks: chunks.length,
                processedChunks: 0,
                startTime: new Date(),
                lastUpdate: new Date(),
                status: 'processing'
            });
            
            // Process with enhanced heartbeat monitoring
            const heartbeatCallback = createHeartbeatMonitoredCallback(
                createSSEBroadcastCallback(res),
                uploadSession,
                filename
            );
            
            const processedData = await processContentChunks(
                parseResult.content, 
                `file://${filename}`,
                heartbeatCallback,
                uploadSession
            );
            
            sendSSEUpdate('complete', { 
                progress: 100, 
                message: 'File processing completed!',
                data: processedData,
                sessionId: uploadSession.sessionId
            });
            
            // Remove from active processing sessions on successful completion
            activeProcessingSessions.delete(uploadSession.sessionId);
            
            // Clean up temp file on successful completion
            if (tempFilePath && fs.existsSync(tempFilePath)) {
                fs.unlinkSync(tempFilePath);
            }
            
        } catch (error) {
            console.error('File processing error:', error);
            
            // Update session status to failed if session was created
            if (uploadSession && uploadSession.id) {
                try {
                    await updateUploadSession(uploadSession.id, 0, 'Failed');
                } catch (updateError) {
                    console.error('Failed to update session status:', updateError);
                }
            }
            
            // Remove from active processing sessions on error
            if (uploadSession && uploadSession.sessionId) {
                activeProcessingSessions.delete(uploadSession.sessionId);
            }
            
            // Enhanced error handling with recovery suggestions
            const errorType = classifyError(error);
            const recoverySuggestions = getRecoverySuggestions(errorType, error);
            
            sendSSEUpdate('error', { 
                message: `Error processing file: ${error.message}`,
                error: error.toString(),
                error_type: errorType,
                recovery_suggestions: recoverySuggestions,
                sessionId: uploadSession?.sessionId,
                resumable: !!tempFilePath,
                retry_recommended: ['network', 'timeout', 'temporary'].includes(errorType)
            });
        } finally {
            res.end();
        }
    });

    busboy.on('error', (error) => {
        console.error('Busboy error:', error);
        sendSSEUpdate('error', { 
            message: `Upload error: ${error.message}`,
            error: error.toString()
        });
        res.end();
    });

    // Pipe the request to busboy to start processing
    req.pipe(busboy);
});
// END TEMPORARY LEGACY ROUTE

// TEMPORARY: Re-enable legacy file upload route until new system is properly configured
// Standard file upload endpoint (non-streaming)
app.post('/api/process-file', (req, res) => {
    const busboy = Busboy({ 
        headers: req.headers,
        limits: {
            fileSize: 500 * 1024 * 1024 // 500MB limit
        }
    });

    busboy.on('file', async (name, file, info) => {
        try {
            const { filename, mimeType } = info;
            
            // Stream file to buffer
            const fileBuffer = await streamToBuffer(file);
            
            // Select and execute parser
            const parser = await fileProcessor.selectParser(mimeType, filename);
            const parseResult = await parser(fileBuffer);
            
            // Process with existing pipeline
            const processedData = await processContentChunks(parseResult.content, `file://${filename}`);
            
            res.json({
                success: true,
                message: 'File processed successfully',
                data: processedData
            });
            
        } catch (error) {
            console.error('File processing error:', error);
            res.status(500).json({
                success: false,
                error: error.message
            });
        }
    });

    busboy.on('error', (error) => {
        console.error('Busboy error:', error);
        res.status(500).json({
            success: false,
            error: error.message
        });
    });

    req.pipe(busboy);
});
// END TEMPORARY LEGACY ROUTE

// This function is deprecated and will be removed in a future version.
// Please use processContentChunks instead.
async function processFileContent(parseResult, filename, sseCallback = null) {
    return processContentChunks(parseResult.content, `file://${filename}`, sseCallback);
}

// This function is deprecated and will be removed in a future version.
// Please use processContentChunks instead.
async function processFileContentWithSession(parseResult, filename, chunks, uploadSession, sseCallback = null) {
    return processContentChunks(parseResult.content, `file://${filename}`, sseCallback, uploadSession);
}

async function processContentChunks(content, url, sseCallback = null, uploadSession = null) {
    let processedChunks = 0;
    let qdrantStored = 0;
    let airtableStored = 0;
    let documentRecord = null;
    
    // Generate session ID for tracking
    const sessionId = uploadSession ? uploadSession.sessionId : uuidv4();
    
    try {
        // Step 1: Chunk the text
        console.log('Chunking text...');
        const chunks = chunkText(content, url);
        console.log(`Created ${chunks.length} chunks`);
        
        // Track this processing session in memory or update existing session
        const existingSession = activeProcessingSessions.get(sessionId);
        if (existingSession) {
            // Update existing session with chunk count
            existingSession.totalChunks = chunks.length;
            existingSession.lastUpdate = new Date();
            activeProcessingSessions.set(sessionId, existingSession);
        } else {
            // Create new session for URL processing
            activeProcessingSessions.set(sessionId, {
                id: sessionId,
                url: url,
                filename: url.split('/').pop() || 'Unknown File',
                totalChunks: chunks.length,
                processedChunks: 0,
                startTime: new Date(),
                lastUpdate: new Date(),
                status: 'processing'
            });
        }
        
        // Create database session for URL processing (same as file uploads)
        if (!uploadSession) {
            try {
                const filename = url.split('/').pop() || 'Unknown URL';
                uploadSession = await createUploadSession(filename, chunks.length, url, 'user');
                console.log('Created upload session:', uploadSession.sessionId);
            } catch (error) {
                console.error('Error creating upload session:', error);
            }
        }
        
        // IMPORTANT: Create a document record first
        try {
            // Extract title and summary from content
            const title = extractTitle(content, url);
            let summary = 'Content processed'; // Default summary
            
            try {
                summary = await generateDocumentSummary(content, url);
            } catch (summaryError) {
                console.warn('Failed to generate AI summary, using default:', summaryError.message);
                // Continue processing without AI summary to prevent hanging
            }
            
            documentRecord = await db.createDocumentRecord({
                url: url,
                title: title,
                summary: summary,
                full_content: content.substring(0, 5000), // Store first 5000 chars as preview
                upload_source: 'user',
                metadata: {
                    total_chunks: chunks.length,
                    content_length: content.length
                }
            });
            
            console.log('üìÑ Created document record:', documentRecord.id, 'URL:', url, 'Title:', title);
            
            if (sseCallback) {
                sseCallback('document_created', {
                    documentId: documentRecord.id,
                    title: title,
                    totalChunks: chunks.length
                });
            }
            
            // Always broadcast document creation to all SSE clients for dashboard refresh
            console.log('üì° Broadcasting document_created SSE event:', documentRecord.id);
            broadcastToSSEClients({
                step: 'document_created',
                message: `üìÑ New document created: ${title}`,
                data: {
                    documentId: documentRecord.id,
                    title: title,
                    totalChunks: chunks.length
                },
                timestamp: new Date().toISOString()
            });
        } catch (error) {
            console.error('Error creating document record:', error);
            // Continue processing even if document creation fails
        }
        
        // Step 2: Process chunks with contextual embeddings
        // Adaptive concurrency: reduce parallel processing for large files to prevent resource exhaustion
        let concurrencyLimit = 3;
        if (chunks.length > 100) {
            concurrencyLimit = 2; // Reduce for files with 100+ chunks
        } else if (chunks.length > 500) {
            concurrencyLimit = 1; // Serial processing for very large files
        }
        
        for (let i = 0; i < chunks.length; i += concurrencyLimit) {
            const batch = chunks.slice(i, i + concurrencyLimit);
            
            const batchPromises = batch.map(async (chunk) => {
                try {
                    const progress = Math.round(((chunk.chunk_index + 1) / chunks.length) * 100);
                    console.log(`Processing chunk ${chunk.chunk_index + 1}/${chunks.length} (${progress}%) in session ${sessionId}`);
                    
                    // Enhanced chunk data for Flow View
                    const chunkData = {
                        chunkId: chunk.chunk_id,
                        sessionId: sessionId,
                        currentChunk: chunk.chunk_index + 1,
                        totalChunks: chunks.length,
                        title: chunk.title || `Chunk ${chunk.chunk_index + 1}`,
                        preview: chunk.chunk_text?.substring(0, 200) + '...',
                        filename: chunk.url || filename || 'unknown',
                        position: chunk.chunk_index / chunks.length
                    };
                    
                    // Send chunk processing start event for Flow View
                    if (sseCallback) {
                        sseCallback('chunk_processing_start', { 
                            message: `ü§ñ Processing chunk ${chunk.chunk_index + 1}/${chunks.length}`,
                            sessionId: sessionId,
                            chunkData: chunkData,
                            progress: progress
                        });
                    }
                    
                    // Analyze chunk with GPT-4o-mini
                    if (sseCallback) {
                        sseCallback('analyze', { 
                            message: `üß† Analyzing chunk ${chunk.chunk_index + 1}...`,
                            sessionId: sessionId,
                            chunkData: chunkData
                        });
                    }
                    
                    // Analyze chunk with timeout and error handling
                    let analysis;
                    try {
                        analysis = await Promise.race([
                            analyzeChunk(chunk.chunk_text),
                            new Promise((_, reject) => setTimeout(() => reject(new Error('Analysis timeout')), 30000))
                        ]);
                    } catch (analysisError) {
                        console.warn(`Analysis failed for chunk ${chunk.chunk_index + 1}, using defaults:`, analysisError.message);
                        analysis = {
                            title: 'Content chunk',
                            summary: chunk.chunk_text.substring(0, 100) + '...',
                            sentiment: 'neutral',
                            emotions: [],
                            category: 'other',
                            content_type: 'article',
                            technical_level: 'general',
                            main_topics: [],
                            key_concepts: [],
                            tags: '',
                            key_entities: {}
                        };
                    }
                    
                    if (sseCallback) {
                        sseCallback('analyze_complete', { 
                            message: `‚úÖ Analyzed chunk ${chunk.chunk_index + 1}: ${analysis.sentiment || 'neutral'}`,
                            sessionId: sessionId,
                            chunkData: chunkData
                        });
                    }
                    
                    // Generate contextual summary if enabled
                    let contextualSummary = null;
                    if (ENABLE_CONTEXTUAL_EMBEDDINGS) {
                        if (sseCallback) {
                            sseCallback('context_generate', { 
                                message: `üìù Generating contextual summary for chunk ${chunk.chunk_index + 1}/${chunks.length}...`,
                                sessionId: sessionId,
                                chunkData: chunkData
                            });
                        }
                        contextualSummary = await generateChunkContext(content, chunk.chunk_text);
                        
                        if (sseCallback) {
                            sseCallback('context_complete', { 
                                message: `‚úÖ Generated context for chunk ${chunk.chunk_index + 1}`,
                                sessionId: sessionId,
                                chunkData: chunkData
                            });
                        }
                    }
                    
                    // Generate embedding (with context if available)
                    if (sseCallback) {
                        sseCallback('embedding', { 
                            message: `üîó Creating ${contextualSummary ? 'contextual ' : ''}embeddings for chunk ${chunk.chunk_index + 1}...`,
                            sessionId: sessionId,
                            chunkData: chunkData
                        });
                    }
                    const embedding = await generateEmbedding(chunk.chunk_text, contextualSummary);
                    
                    // Store in Qdrant
                    let embeddingStatus = 'pending';
                    try {
                        if (sseCallback) {
                            sseCallback('embed_storing', { 
                                message: `üíæ Storing chunk ${chunk.chunk_index + 1} in vector database...`,
                                sessionId: sessionId,
                                chunkData: chunkData
                            });
                        }
                        await storeInQdrant(chunk, embedding, analysis, contextualSummary);
                        qdrantStored++;
                        embeddingStatus = 'success';
                        
                        if (sseCallback) {
                            sseCallback('embed_complete', { 
                                message: `‚úÖ Stored chunk ${chunk.chunk_index + 1} in vector database`,
                                sessionId: sessionId,
                                chunkData: chunkData
                            });
                        }
                    } catch (qdrantError) {
                        console.error('Qdrant storage failed:', qdrantError);
                        embeddingStatus = 'failed';
                        
                        if (sseCallback) {
                            sseCallback('embed_error', { 
                                message: `‚ö†Ô∏è Vector storage failed for chunk ${chunk.chunk_index + 1}: ${qdrantError.message}`,
                                sessionId: sessionId,
                                chunkData: chunkData
                            });
                        }
                    }
                    
                    // Store in PostgreSQL with session ID and contextual summary
                    try {
                        if (sseCallback) {
                            sseCallback('storing', { 
                                message: `üíæ Storing chunk ${chunk.chunk_index + 1} metadata...`,
                                sessionId: sessionId,
                                chunkData: chunkData
                            });
                        }
                        await storeInPostgreSQL(chunk, analysis, embeddingStatus, sessionId, contextualSummary, 'user', documentRecord ? documentRecord.id : null);
                        airtableStored++;
                        
                        if (sseCallback) {
                            sseCallback('store_complete', { 
                                message: `‚úÖ Stored chunk ${chunk.chunk_index + 1} metadata`,
                                sessionId: sessionId,
                                chunkData: chunkData
                            });
                        }
                    } catch (airtableError) {
                        console.error('PostgreSQL storage failed:', airtableError);
                        
                        if (sseCallback) {
                            sseCallback('store_error', { 
                                message: `‚ö†Ô∏è Metadata storage failed for chunk ${chunk.chunk_index + 1}: ${airtableError.message}`,
                                sessionId: sessionId,
                                chunkData: chunkData
                            });
                        }
                    }
                    
                    processedChunks++;
                    
                    // Update active session with processed chunk count
                    const session = activeProcessingSessions.get(sessionId);
                    if (session) {
                        session.processedChunks = processedChunks;
                        session.lastUpdate = new Date();
                        activeProcessingSessions.set(sessionId, session);
                    }
                    
                    // Send chunk processing complete event for Flow View
                    if (sseCallback) {
                        sseCallback('chunk_processing_complete', { 
                            message: `üéâ Completed chunk ${chunk.chunk_index + 1}/${chunks.length}`,
                            sessionId: sessionId,
                            chunkData: chunkData,
                            progress: Math.round(((chunk.chunk_index + 1) / chunks.length) * 100)
                        });
                    }
                    
                    // Update session progress
                    if (uploadSession.id) {
                        try {
                            await updateUploadSession(uploadSession.id, processedChunks);
                        } catch (updateError) {
                            console.error('Failed to update session progress:', updateError);
                        }
                    }
                    
                    // Update progress
                    if (sseCallback) {
                        const progress = 65 + Math.round((processedChunks / chunks.length) * 25);
                        sseCallback('analyze', { 
                            progress, 
                            message: `Processed ${processedChunks}/${chunks.length} chunks`,
                            current: processedChunks,
                            total: chunks.length,
                            sessionId: sessionId
                        });
                    }
                    
                    return { success: true, chunkIndex: chunk.chunk_index };
                    
                } catch (chunkError) {
                    console.error(`Error processing chunk ${chunk.chunk_index + 1}:`, chunkError);
                    return { success: false, chunkIndex: chunk.chunk_index, error: chunkError.message };
                }
            });
            
            // Wait for this batch to complete before starting the next
            await Promise.all(batchPromises);
            
            // Small delay between batches to prevent overwhelming APIs
            if (i + concurrencyLimit < chunks.length) {
                await new Promise(resolve => setTimeout(resolve, 500));
            }
        }
        
        // Mark session as completed
        if (uploadSession.id) {
            try {
                await updateUploadSession(uploadSession.id, processedChunks, 'Completed');
            } catch (updateError) {
                console.error('Failed to mark session as completed:', updateError);
            }
        }
        
        console.log(`Session ${sessionId} completed: ${processedChunks} chunks processed, ${qdrantStored} stored in Qdrant, ${airtableStored} stored in Airtable`);
        
        // Mark session as completed and remove from active sessions
        const session = activeProcessingSessions.get(sessionId);
        if (session) {
            session.status = 'completed';
            session.completedAt = new Date();
            activeProcessingSessions.delete(sessionId);
            console.log(`Session ${sessionId} completed and removed from active tracking`);
        }
        
        return {
            url,
            chunks: chunks.length,
            processedChunks,
            qdrantStored,
            airtableStored,
            sessionId: sessionId
        };
        
    } catch (error) {
        console.error('Error in processContentChunks:', error);
        
        // Mark session as failed and remove from active sessions
        const session = activeProcessingSessions.get(sessionId);
        if (session) {
            session.status = 'failed';
            session.error = error.message;
            session.completedAt = new Date();
            activeProcessingSessions.delete(sessionId);
            console.log(`Session ${sessionId} failed and removed from active tracking`);
        }
        
        // Mark session as failed
        if (uploadSession.id) {
            try {
                await updateUploadSession(uploadSession.id, processedChunks, 'Failed');
            } catch (updateError) {
                console.error('Failed to mark session as failed:', updateError);
            }
        }
        
        throw error;
    }
}

// Duplicate function removed - using the main processContentChunks function above that accepts uploadSession parameter

/* LEGACY UPLOAD ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION
// Resume upload endpoint
app.post('/api/resume-upload/:sessionId', async (req, res) => {
    // Set headers for SSE
    res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Cache-Control'
    });

    const { sessionId } = req.params;
    
    function sendSSEUpdate(event, data) {
        res.write(`data: ${JSON.stringify({ event, data })}\n\n`);
    }

    try {
        sendSSEUpdate('start', { sessionId, message: 'Checking session status...' });
        
        // Get upload session details
        const uploadSession = await getUploadSession(sessionId);
        if (!uploadSession) {
            sendSSEUpdate('error', { message: 'Upload session not found' });
            res.end();
            return;
        }
        
        const sessionFields = uploadSession.fields;
        const sessionRecordId = uploadSession.id;
        
        // Check if session is already completed
        if (sessionFields.Status === 'Completed') {
            sendSSEUpdate('complete', { 
                message: 'Session already completed',
                sessionId: sessionId,
                progress: 100
            });
            res.end();
            return;
        }
        
        sendSSEUpdate('session', { 
            progress: 10, 
            message: `Found session: ${sessionFields.Filename}`,
            filename: sessionFields.Filename,
            totalChunks: sessionFields['Total Chunks'],
            completedChunks: sessionFields['Completed Chunks']
        });
        
        // Check if temp file still exists
        const fs = require('fs');
        const tempFilePath = sessionFields['File Path'];
        if (!tempFilePath || !fs.existsSync(tempFilePath)) {
            await updateUploadSession(sessionRecordId, sessionFields['Completed Chunks'], 'Failed');
            sendSSEUpdate('error', { 
                message: 'Temporary file no longer exists. Please re-upload the file.',
                sessionId: sessionId
            });
            res.end();
            return;
        }
        
        sendSSEUpdate('upload', { progress: 20, message: 'Reading file from temporary storage...' });
        
        // Read and parse the file
        const fileBuffer = fs.readFileSync(tempFilePath);
        const filename = sessionFields.Filename;
        const mimeType = getMimeTypeFromFilename(filename);
        
        const parser = await fileProcessor.selectParser(mimeType, filename);
        sendSSEUpdate('parse', { progress: 30, message: 'Parsing file content...' });
        
        const parseResult = await parser(fileBuffer);
        const chunks = chunkText(parseResult.content, `file://${filename}`);
        sendSSEUpdate('parse', { progress: 40, message: `Content split into ${chunks.length} chunks` });
        
        // Get already processed chunks
        const processedChunksData = await getProcessedChunksForSession(sessionId);
        const processedChunkIndices = new Set(processedChunksData.map(c => c.chunkIndex));
        
        // Filter out already processed chunks
        const remainingChunks = chunks.filter(chunk => !processedChunkIndices.has(chunk.chunk_index));
        
        sendSSEUpdate('resume', { 
            progress: 50, 
            message: `Resuming processing: ${remainingChunks.length} chunks remaining`,
            totalChunks: chunks.length,
            completedBefore: processedChunkIndices.size,
            remaining: remainingChunks.length
        });
        
        if (remainingChunks.length === 0) {
            // All chunks already processed, mark as completed
            await updateUploadSession(sessionRecordId, chunks.length, 'Completed');
            sendSSEUpdate('complete', { 
                progress: 100, 
                message: 'All chunks already processed!',
                sessionId: sessionId
            });
            
            // Clean up temp file
            if (fs.existsSync(tempFilePath)) {
                fs.unlinkSync(tempFilePath);
            }
            
            res.end();
            return;
        }
        
        // Update session status to processing
        await updateUploadSession(sessionRecordId, processedChunkIndices.size, 'Processing');
        
        // Resume processing with remaining chunks
        const resumeSession = {
            id: sessionRecordId,
            sessionId: sessionId
        };
        
        await processRemainingChunks(remainingChunks, resumeSession, processedChunkIndices.size, sendSSEUpdate);
        
        sendSSEUpdate('complete', { 
            progress: 100, 
            message: 'Upload resumed and completed successfully!',
            sessionId: sessionId
        });
        
        // Broadcast session completion to all SSE clients for dashboard refresh
        broadcastToSSEClients({
            step: 'session_complete',
            message: `‚úÖ Upload session completed: ${sessionId}`,
            data: { sessionId: sessionId },
            timestamp: new Date().toISOString()
        });
        
        // Clean up temp file on successful completion
        if (fs.existsSync(tempFilePath)) {
            fs.unlinkSync(tempFilePath);
        }
        
    } catch (error) {
        console.error('Resume upload error:', error);
        sendSSEUpdate('error', { 
            message: `Resume failed: ${error.message}`,
            sessionId: sessionId
        });
    } finally {
        res.end();
    }
});

// Helper function to get MIME type from filename
function getMimeTypeFromFilename(filename) {
    const ext = filename.toLowerCase().split('.').pop();
    const extToMime = {
        'pdf': 'application/pdf',
        'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'doc': 'application/msword',
        'epub': 'application/epub+zip',
        'txt': 'text/plain',
        'csv': 'text/csv',
        'html': 'text/html',
        'htm': 'text/html'
    };
    return extToMime[ext] || 'application/octet-stream';
}

// Process remaining chunks for resume functionality
async function processRemainingChunks(remainingChunks, uploadSession, initialCompletedCount, sseCallback) {
    let processedChunks = initialCompletedCount;
    let qdrantStored = 0;
    let airtableStored = 0;
    
    try {
        // Process chunks with concurrency limit (3 at a time)
        const concurrencyLimit = 3;
        
        for (let i = 0; i < remainingChunks.length; i += concurrencyLimit) {
            const batch = remainingChunks.slice(i, i + concurrencyLimit);
            
            const batchPromises = batch.map(async (chunk) => {
                try {
                    console.log(`Resuming chunk ${chunk.chunk_index + 1} in session ${uploadSession.sessionId}`);
                    
                    // Analyze chunk with GPT-4o-mini
                    const analysis = await analyzeChunk(chunk.chunk_text);
                    
                    // Generate embedding
                    const embedding = await generateEmbedding(chunk.chunk_text);
                    
                    // Store in Qdrant
                    let embeddingStatus = 'pending';
                    try {
                        await storeInQdrant(chunk, embedding, analysis);
                        qdrantStored++;
                        embeddingStatus = 'success';
                    } catch (qdrantError) {
                        console.error('Qdrant storage failed:', qdrantError);
                        embeddingStatus = 'failed';
                    }
                    
                    // Store in Airtable with session ID
                    try {
                        await storeInPostgreSQL(chunk, analysis, embeddingStatus, uploadSession.sessionId, null, 'user');
                        airtableStored++;
                    } catch (airtableError) {
                        console.error('Airtable storage failed:', airtableError);
                    }
                    
                    processedChunks++;
                    
                    // Update session progress
                    if (uploadSession.id) {
                        try {
                            await updateUploadSession(uploadSession.id, processedChunks);
                        } catch (updateError) {
                            console.error('Failed to update session progress:', updateError);
                        }
                    }
                    
                    // Update active session with processed chunk count
                    const session = activeProcessingSessions.get(uploadSession.sessionId);
                    if (session) {
                        session.processedChunks = processedChunks;
                        session.lastUpdate = new Date();
                        activeProcessingSessions.set(uploadSession.sessionId, session);
                    }
                    
                    // Update progress
                    if (sseCallback) {
                        const currentChunkInBatch = processedChunks - initialCompletedCount;
                        const progress = 50 + Math.round((currentChunkInBatch / remainingChunks.length) * 45);
                        sseCallback('analyze', { 
                            progress, 
                            message: `Resumed: processed ${processedChunks} total chunks`,
                            current: processedChunks,
                            total: initialCompletedCount + remainingChunks.length,
                            sessionId: uploadSession.sessionId
                        });
                    }
                    
                    return { success: true, chunkIndex: chunk.chunk_index };
                    
                } catch (chunkError) {
                    console.error(`Error processing chunk ${chunk.chunk_index + 1}:`, chunkError);
                    return { success: false, chunkIndex: chunk.chunk_index, error: chunkError.message };
                }
            });
            
            // Wait for this batch to complete before starting the next
            await Promise.all(batchPromises);
            
            // Small delay between batches to prevent overwhelming APIs
            if (i + concurrencyLimit < remainingChunks.length) {
                await new Promise(resolve => setTimeout(resolve, 500));
            }
        }
        
        // Mark session as completed
        if (uploadSession.id) {
            try {
                await updateUploadSession(uploadSession.id, processedChunks, 'Completed');
            } catch (updateError) {
                console.error('Failed to mark session as completed:', updateError);
            }
        }
        
        console.log(`Session ${uploadSession.sessionId} resumed and completed: ${processedChunks} total chunks processed`);
        
    } catch (error) {
        console.error('Error in processRemainingChunks:', error);
        
        // Mark session as failed
        if (uploadSession.id) {
            try {
                await updateUploadSession(uploadSession.id, processedChunks, 'Failed');
            } catch (updateError) {
                console.error('Failed to mark session as failed:', updateError);
            }
        }
        
        throw error;
    }
}


// Pipeline service health check endpoint
app.get('/api/pipeline/health', async (req, res) => {
    try {
        const pipelineUrl = 'http://127.0.0.1:9099';
        const response = await fetch(pipelineUrl, {
            method: 'GET',
            timeout: 5000
        });
        
        const isHealthy = response.ok;
        
        // Get actual Qdrant search activity
        const qdrantActivity = await getQdrantSearchActivity();
        
        res.json({
            success: true,
            status: isHealthy ? 'healthy' : 'unhealthy',
            serviceUrl: pipelineUrl,
            port: '9099',
            lastChecked: new Date().toISOString(),
            responseStatus: response.status,
            lastRagActivity: qdrantActivity.lastSearchActivity ? qdrantActivity.lastSearchActivity.toISOString() : null,
            totalRagSearches: qdrantActivity.totalSearches,
            qdrantStatus: qdrantActivity.qdrantStatus
        });
        
    } catch (error) {
        console.error('Pipeline health check failed:', error);
        
        // Still try to get Qdrant activity even if pipeline is down
        let qdrantActivity;
        try {
            qdrantActivity = await getQdrantSearchActivity();
        } catch (qdrantError) {
            qdrantActivity = { lastSearchActivity: null, totalSearches: 0, qdrantStatus: 'error' };
        }
        
        res.json({
            success: false,
            status: 'error',
            serviceUrl: 'http://127.0.0.1:9099',
            port: '9099',
            lastChecked: new Date().toISOString(),
            error: error.message,
            lastRagActivity: qdrantActivity.lastSearchActivity ? qdrantActivity.lastSearchActivity.toISOString() : null,
            totalRagSearches: qdrantActivity.totalSearches,
            qdrantStatus: qdrantActivity.qdrantStatus
        });
    }
});

// ENHANCED: Comprehensive system health monitoring endpoint
app.get('/api/health/comprehensive', async (req, res) => {
    const startTime = Date.now();
    const health = {
        timestamp: new Date().toISOString(),
        status: 'healthy',
        services: {},
        performance: {},
        sessions: {},
        alerts: []
    };

    try {
        // Run comprehensive health checks in parallel
        const healthChecks = await Promise.allSettled([
            // 1. Database health with performance metrics
            (async () => {
                const dbStart = Date.now();
                const dbResult = await db.pool.query('SELECT NOW() as db_time, COUNT(*) as active_connections FROM pg_stat_activity WHERE state = \'active\'');
                return {
                    status: 'healthy',
                    responseTime: Date.now() - dbStart,
                    activeConnections: parseInt(dbResult.rows[0].active_connections),
                    poolStats: {
                        total: db.pool.totalCount,
                        idle: db.pool.idleCount,
                        waiting: db.pool.waitingCount
                    }
                };
            })(),

            // 2. Qdrant health
            (async () => {
                const qdrantStart = Date.now();
                const qdrantResponse = await fetch(`${process.env.QDRANT_URL || 'http://localhost:6333'}/health`, {
                    timeout: 5000,
                    headers: process.env.QDRANT_API_KEY ? { 'api-key': process.env.QDRANT_API_KEY } : {}
                });
                return {
                    status: qdrantResponse.ok ? 'healthy' : 'unhealthy',
                    responseTime: Date.now() - qdrantStart,
                    httpStatus: qdrantResponse.status
                };
            })(),

            // 3. BM25 service health
            (async () => {
                const bm25Start = Date.now();
                const bm25Response = await fetch('http://localhost:3002/health', { timeout: 3000 });
                return {
                    status: bm25Response.ok ? 'healthy' : 'unhealthy',
                    responseTime: Date.now() - bm25Start,
                    httpStatus: bm25Response.status
                };
            })(),

            // 4. OpenAI API health (lightweight test)
            (async () => {
                try {
                    if (!process.env.OPENAI_API_KEY) {
                        return { status: 'not_configured', message: 'API key not set' };
                    }
                    // Just check if the key format is valid without making API call
                    const keyValid = process.env.OPENAI_API_KEY.startsWith('sk-');
                    return {
                        status: keyValid ? 'configured' : 'invalid_key',
                        keyFormat: keyValid ? 'valid' : 'invalid'
                    };
                } catch (error) {
                    return { status: 'error', error: error.message };
                }
            })(),

            // 5. Session analysis
            (async () => {
                const sessionStats = await db.pool.query(`
                    SELECT 
                        status,
                        COUNT(*) as count,
                        AVG(EXTRACT(EPOCH FROM (NOW() - created_at))/60)::int as avg_duration_minutes,
                        MAX(EXTRACT(EPOCH FROM (NOW() - created_at))/60)::int as max_duration_minutes
                    FROM upload_sessions 
                    WHERE created_at > NOW() - INTERVAL '24 hours'
                    GROUP BY status
                `);
                
                const stuckSessions = await db.pool.query(`
                    SELECT COUNT(*) as stuck_count
                    FROM upload_sessions 
                    WHERE status = 'processing' 
                      AND last_activity < NOW() - INTERVAL '2 minutes'
                `);

                return {
                    byStatus: sessionStats.rows,
                    currentlyStuck: parseInt(stuckSessions.rows[0].stuck_count),
                    inMemoryActive: activeProcessingSessions.size
                };
            })(),

            // 6. Recent error analysis
            (async () => {
                const recentErrors = await db.pool.query(`
                    SELECT 
                        LEFT(error_message, 100) as error_type,
                        COUNT(*) as count,
                        MAX(updated_at) as last_occurrence
                    FROM upload_sessions 
                    WHERE status = 'failed' 
                      AND updated_at > NOW() - INTERVAL '1 hour'
                      AND error_message IS NOT NULL
                    GROUP BY LEFT(error_message, 100)
                    ORDER BY count DESC
                    LIMIT 5
                `);

                return recentErrors.rows;
            })()
        ]);

        // Process health check results
        health.services.database = healthChecks[0].status === 'fulfilled' ? healthChecks[0].value : { status: 'error', error: healthChecks[0].reason?.message };
        health.services.qdrant = healthChecks[1].status === 'fulfilled' ? healthChecks[1].value : { status: 'error', error: healthChecks[1].reason?.message };
        health.services.bm25 = healthChecks[2].status === 'fulfilled' ? healthChecks[2].value : { status: 'error', error: healthChecks[2].reason?.message };
        health.services.openai = healthChecks[3].status === 'fulfilled' ? healthChecks[3].value : { status: 'error', error: healthChecks[3].reason?.message };
        health.sessions = healthChecks[4].status === 'fulfilled' ? healthChecks[4].value : { error: healthChecks[4].reason?.message };
        health.performance.recentErrors = healthChecks[5].status === 'fulfilled' ? healthChecks[5].value : [];

        // Overall performance metrics
        health.performance.totalResponseTime = Date.now() - startTime;
        health.performance.memoryUsage = process.memoryUsage();
        health.performance.uptime = process.uptime();
        health.performance.nodeVersion = process.version;

        // Generate alerts based on health metrics
        if (health.services.database.status !== 'healthy') {
            health.alerts.push({ severity: 'critical', message: 'Database connectivity issues detected' });
        }
        
        if (health.services.database.responseTime > 1000) {
            health.alerts.push({ severity: 'warning', message: `Database response time high: ${health.services.database.responseTime}ms` });
        }

        if (health.sessions.currentlyStuck > 0) {
            health.alerts.push({ severity: 'warning', message: `${health.sessions.currentlyStuck} sessions currently stuck` });
        }

        if (health.services.qdrant.status !== 'healthy') {
            health.alerts.push({ severity: 'high', message: 'Qdrant vector database unavailable' });
        }

        if (health.performance.recentErrors.length > 0) {
            health.alerts.push({ severity: 'info', message: `${health.performance.recentErrors.length} error types in past hour` });
        }

        // Memory usage alerts
        const memUsageMB = health.performance.memoryUsage.rss / 1024 / 1024;
        if (memUsageMB > 1024) { // 1GB
            health.alerts.push({ severity: 'warning', message: `High memory usage: ${Math.round(memUsageMB)}MB` });
        }

        // Set overall status based on critical issues
        const hasCriticalIssues = health.alerts.some(alert => alert.severity === 'critical');
        const hasHighIssues = health.alerts.some(alert => alert.severity === 'high');
        
        if (hasCriticalIssues) {
            health.status = 'critical';
        } else if (hasHighIssues) {
            health.status = 'degraded';
        } else if (health.alerts.length > 0) {
            health.status = 'warning';
        }

        // Return appropriate HTTP status
        const httpStatus = health.status === 'critical' ? 503 : 
                          health.status === 'degraded' ? 503 : 
                          health.status === 'warning' ? 200 : 200;

        res.status(httpStatus).json(health);

    } catch (error) {
        console.error('‚ùå Comprehensive health check failed:', error);
        
        health.status = 'error';
        health.error = error.message;
        health.alerts.push({ severity: 'critical', message: 'Health check system failure' });
        
        res.status(503).json(health);
    }
});

// NEW: Qdrant RAG activity monitoring endpoint
app.get('/api/qdrant/activity', async (req, res) => {
    try {
        const qdrantActivity = await getQdrantSearchActivity();
        
        res.json({
            success: true,
            qdrant: {
                status: qdrantActivity.qdrantStatus,
                totalSearches: qdrantActivity.totalSearches,
                lastSearchActivity: qdrantActivity.lastSearchActivity ? qdrantActivity.lastSearchActivity.toISOString() : null,
                telemetryChecked: qdrantActivity.telemetryTimestamp.toISOString(),
                error: qdrantActivity.error || null
            },
            message: 'Real-time Qdrant vector database activity tracking'
        });
        
    } catch (error) {
        console.error('Qdrant activity check failed:', error);
        res.status(500).json({
            success: false,
            error: error.message,
            qdrant: {
                status: 'error',
                error: error.message
            }
        });
    }
});

// PostgreSQL database statistics endpoint
app.get('/api/database/stats', async (req, res) => {
    try {
        
        // Get table information
        const tablesQuery = `
            SELECT 
                schemaname,
                relname as tablename,
                COALESCE(n_tup_ins, 0) as inserts,
                COALESCE(n_tup_upd, 0) as updates,
                COALESCE(n_tup_del, 0) as deletes,
                COALESCE(n_live_tup, 0) as live_rows,
                COALESCE(n_dead_tup, 0) as dead_rows
            FROM pg_stat_user_tables 
            ORDER BY n_live_tup DESC;
        `;
        
        const tables = await db.pool.query(tablesQuery);
        
        // Get database size
        const sizeQuery = `
            SELECT 
                pg_size_pretty(pg_database_size(current_database())) as db_size,
                pg_database_size(current_database()) as db_size_bytes
        `;
        
        const sizeResult = await db.pool.query(sizeQuery);
        
        // Calculate totals
        let totalRecords = 0;
        const tableStats = tables.rows.map(table => {
            totalRecords += parseInt(table.live_rows);
            return {
                name: table.tablename,
                records: parseInt(table.live_rows),
                inserts: parseInt(table.inserts),
                updates: parseInt(table.updates),
                deletes: parseInt(table.deletes)
            };
        });
        
        res.json({
            success: true,
            database: {
                totalTables: tables.rows.length,
                totalRecords: totalRecords,
                size: sizeResult.rows[0].db_size,
                sizeBytes: parseInt(sizeResult.rows[0].db_size_bytes),
                tables: tableStats,
                lastChecked: new Date().toISOString()
            },
            message: 'PostgreSQL database statistics'
        });
        
    } catch (error) {
        console.error('Database stats error:', error);
        res.status(500).json({
            success: false,
            error: error.message,
            database: {
                totalTables: 0,
                totalRecords: 0,
                size: 'Unknown',
                error: error.message
            }
        });
    }
});

// Server-Sent Events endpoint for real-time updates
// Global SSE clients for broadcasting processing events - MOVED TO NEW ROUTE SYSTEM
const globalSSEClients = new Set();

// Legacy SSE endpoint disabled - now handled by new route system in pipeline.routes.js
/*
app.get('/api/stream', (req, res) => {
    console.log('üì° New SSE connection established');
    
    // Set SSE headers with comprehensive CORS support
    res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'GET, OPTIONS',
        'Access-Control-Allow-Headers': 'Cache-Control, Content-Type, Authorization, X-Requested-With, Accept, Accept-Encoding, Accept-Language, DNT, User-Agent, Referer',
        'Access-Control-Allow-Credentials': 'false',
        'Access-Control-Max-Age': '86400'
    });
    
    // Add this client to global broadcast list
    globalSSEClients.add(res);
    
    // Send initial connection event
    res.write(`data: ${JSON.stringify({
        type: 'connected',
        timestamp: new Date().toISOString(),
        message: 'SSE connection established'
    })}\n\n`);
    
    // Keep connection alive with periodic heartbeat
    const heartbeat = setInterval(() => {
        if (!res.destroyed) {
            res.write(`data: ${JSON.stringify({
                type: 'heartbeat',
                timestamp: new Date().toISOString()
            })}\n\n`);
        }
    }, 30000); // Every 30 seconds
    
    // Clean up on client disconnect
    req.on('close', () => {
        console.log('üì° SSE connection closed');
        clearInterval(heartbeat);
        globalSSEClients.delete(res);
    });
    
    req.on('end', () => {
        console.log('üì° SSE connection ended');
        clearInterval(heartbeat);
        globalSSEClients.delete(res);
    });
});
LEGACY UPLOAD ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION */

// Function to broadcast processing events to all SSE clients
function broadcastToSSEClients(eventData) {
    if (globalSSEClients.size > 0) {
        console.log(`üì° Broadcasting to ${globalSSEClients.size} SSE clients:`, eventData.step);
    }
    globalSSEClients.forEach(client => {
        if (!client.destroyed) {
            try {
                client.write(`data: ${JSON.stringify(eventData)}\n\n`);
            } catch (error) {
                console.error('Failed to broadcast to SSE client:', error);
                globalSSEClients.delete(client);
            }
        } else {
            globalSSEClients.delete(client);
        }
    });
}

// Special SSE callback for file processing that broadcasts in the right format
function createSSEBroadcastCallback(res) {
    return (step, data) => {
        
        // Create the event data in the expected format
        const eventData = {
            step,
            message: data.message || '',
            progress: data.progress || null,
            timestamp: new Date().toISOString(),
            sessionId: data.sessionId,
            chunkData: data.chunkData || null
        };
        
        // Send to the specific processing stream
        res.write(`data: ${JSON.stringify({event: step, data})}\n\n`);
        
        // Broadcast to all global SSE clients (for Flow View)
        broadcastToSSEClients(eventData);
    };
}

/* LEGACY UPLOAD ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION
// AI-generated content endpoint - for n8n or other AI systems
app.post('/api/process-ai-content', async (req, res) => {
    // Set up SSE headers
    res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Cache-Control'
    });

    const startTime = Date.now();
    let processedChunks = 0;
    let qdrantStored = 0;
    let airtableStored = 0;

    try {
        const { url, source = 'ai' } = req.body;
        
        if (!url) {
            sendSSEUpdate(res, 'error', 'URL is required');
            res.end();
            return;
        }
        
        console.log(`ü§ñ Processing AI-generated content from: ${url} (source: ${source})`);
        sendSSEUpdate(res, 'start', `Starting AI content processing for ${url}`);
        
        // Fetch web content
        const contentData = await fetchWebContent(url);
        sendSSEUpdate(res, 'fetch', 'Content fetched successfully');
        
        // Convert to markdown
        let markdown;
        if (contentData.type === 'pdf') {
            markdown = contentData.content;
        } else {
            markdown = htmlToMarkdown(contentData.content);
        }
        
        // Extract title
        const title = extractTitle(markdown) || url.split('/').pop() || 'AI Generated Content';
        sendSSEUpdate(res, 'parse', `Parsing: ${title}`);
        
        // Chunk the content
        const chunks = chunkText(markdown, url);
        sendSSEUpdate(res, 'chunk', `Content split into ${chunks.length} chunks`);
        
        // Create upload session with AI source
        let uploadSession = null;
        try {
            const filename = url.split('/').pop() || 'ai-content';
            uploadSession = await createUploadSession(filename, chunks.length, url, source);
            sendSSEUpdate(res, 'session', { 
                progress: 5, 
                message: 'AI processing session created',
                sessionId: uploadSession.sessionId 
            });
        } catch (sessionError) {
            console.error('Failed to create AI processing session:', sessionError);
            uploadSession = { id: null, sessionId: uuidv4() };
        }
        
        // Process chunks
        for (let i = 0; i < chunks.length; i++) {
            const chunk = chunks[i];
            const chunkNum = i + 1;
            
            // Enhanced chunk data for Flow View
            const chunkData = {
                chunkId: chunk.chunk_id,
                sessionId: uploadSession.sessionId,
                currentChunk: chunkNum,
                totalChunks: chunks.length,
                title: chunk.title || `Chunk ${chunkNum}`,
                preview: chunk.text?.substring(0, 200) + '...',
                filename: chunk.url || 'unknown',
                position: i / chunks.length // For Flow View positioning
            };
            
            // Send chunk processing start event for Flow View
            sendSSEUpdate(res, 'chunk_processing_start', `ü§ñ Processing chunk ${chunkNum}/${chunks.length}`, 
                Math.round((chunkNum / chunks.length) * 100), chunkData);
            
            // AI analysis
            sendSSEUpdate(res, 'analyze', `üß† Analyzing chunk ${chunkNum}...`, null, chunkData);
            const analysis = await analyzeChunk(chunk.text);
            processedChunks++;
            
            // Update active session with processed chunk count
            const session = activeProcessingSessions.get(uploadSession.sessionId);
            if (session) {
                session.processedChunks = processedChunks;
                session.lastUpdate = new Date();
                activeProcessingSessions.set(uploadSession.sessionId, session);
            }
            
            sendSSEUpdate(res, 'analyze_complete', `‚úÖ Analyzed chunk ${chunkNum}: ${analysis.sentiment || 'neutral'}`, null, chunkData);
            
            // Generate embeddings and store
            let embeddingStatus = 'pending';
            let contextualSummary = null;
            
            try {
                if (process.env.ENABLE_CONTEXTUAL_EMBEDDINGS === 'true') {
                    sendSSEUpdate(res, 'context_generate', `üìù Generating context for chunk ${chunkNum}...`, null, chunkData);
                    const contextGenStart = Date.now();
                    contextualSummary = await generateChunkContext(title, markdown, chunk.text, i);
                    const contextGenTime = ((Date.now() - contextGenStart) / 1000).toFixed(2);
                    sendSSEUpdate(res, 'context_complete', `‚úÖ Generated context for chunk ${chunkNum} in ${contextGenTime}s`, null, chunkData);
                }
                
                sendSSEUpdate(res, 'embedding', `üîó Creating embeddings for chunk ${chunkNum}...`, null, chunkData);
                await storeInQdrant(chunk, contextualSummary);
                qdrantStored++;
                embeddingStatus = 'complete';
                sendSSEUpdate(res, 'embed_complete', `‚úÖ Stored chunk ${chunkNum} in vector database`, null, chunkData);
            } catch (error) {
                console.error('Vector storage failed:', error);
                embeddingStatus = 'failed';
                sendSSEUpdate(res, 'embed_error', `‚ö†Ô∏è Vector storage failed for chunk ${chunkNum}: ${error.message}`, null, chunkData);
            }
            
            // Store in PostgreSQL with AI source
            try {
                sendSSEUpdate(res, 'storing', `üíæ Storing chunk ${chunkNum} metadata...`, null, chunkData);
                const result = await storeInPostgreSQL(chunk, analysis, embeddingStatus, uploadSession.sessionId, contextualSummary, source);
                if (result.status !== 'skipped') {
                    airtableStored++;
                    sendSSEUpdate(res, 'store_complete', `‚úÖ Stored chunk ${chunkNum} metadata`, null, chunkData);
                }
            } catch (error) {
                console.error('PostgreSQL storage failed:', error);
                sendSSEUpdate(res, 'store_error', `‚ö†Ô∏è Metadata storage failed for chunk ${chunkNum}: ${error.message}`, null, chunkData);
            }
            
            // Send chunk processing complete event for Flow View
            const progress = Math.round((chunkNum / chunks.length) * 100);
            sendSSEUpdate(res, 'chunk_processing_complete', `üéâ Completed chunk ${chunkNum}/${chunks.length}`, 
                progress, chunkData);
            
            // Update overall progress
            sendSSEUpdate(res, 'progress', { chunk: chunkNum, total: chunks.length, percent: progress });
        }
        
        // Update session status
        if (uploadSession.id) {
            await db.updateUploadSession(uploadSession.sessionId, {
                processed_chunks: chunks.length,
                completed_chunks: chunks.length,
                status: 'completed'
            });
        }
        
        const processingTime = Math.round((Date.now() - startTime) / 1000);
        sendSSEUpdate(res, 'complete', {
            message: `ü§ñ AI content processing completed successfully`,
            processed_chunks: processedChunks,
            qdrant_stored: qdrantStored,
            postgresql_stored: airtableStored,
            processing_time: `${processingTime}s`,
            source: source
        });
        
        // Broadcast completion event to all SSE clients for dashboard refresh
        broadcastToSSEClients({
            step: 'processing_complete',
            message: `‚úÖ Processing completed: ${processedChunks} chunks processed`,
            data: {
                processed_chunks: processedChunks,
                qdrant_stored: qdrantStored,
                postgresql_stored: airtableStored,
                processing_time: `${processingTime}s`,
                source: source
            },
            timestamp: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('AI content processing error:', error);
        sendSSEUpdate(res, 'error', `Processing failed: ${error.message}`);
    } finally {
        res.end();
    }
});
LEGACY UPLOAD ROUTE - COMMENTED OUT FOR NEW SYSTEM MIGRATION */

// Pre-upload validation and cleanup endpoint
app.post('/api/pre-upload-check', async (req, res) => {
    try {
        const startTime = Date.now();
        
        // 1. Aggressive cleanup of stuck sessions (5min threshold instead of 15min)
        const cleanedCount = await cleanupStuckSessions(5);
        
        // 2. Check system health
        const [dbStats, systemHealth] = await Promise.all([
            db.getDatabaseStats(),
            checkSystemHealth()
        ]);
        
        // 3. Check for any remaining stuck sessions
        const stuckCheck = await db.pool.query(`
            SELECT COUNT(*) as stuck_count, 
                   MIN(updated_at) as oldest_stuck
            FROM upload_sessions 
            WHERE status = 'processing' 
            AND updated_at < NOW() - INTERVAL '2 minutes'
        `);
        
        const responseTime = Date.now() - startTime;
        const isReady = stuckCheck.rows[0].stuck_count === '0' && systemHealth.database && systemHealth.qdrant;
        
        res.json({
            success: true,
            ready_for_upload: isReady,
            cleaned_sessions: cleanedCount,
            system_health: {
                database: systemHealth.database,
                qdrant: systemHealth.qdrant,
                response_time_ms: responseTime
            },
            session_status: {
                stuck_sessions: parseInt(stuckCheck.rows[0].stuck_count),
                oldest_stuck: stuckCheck.rows[0].oldest_stuck
            },
            recommendations: isReady ? [] : [
                !systemHealth.database && 'Database connection issues detected',
                !systemHealth.qdrant && 'Vector database connection issues detected', 
                stuckCheck.rows[0].stuck_count > 0 && 'Stuck processing sessions detected - try again in a moment'
            ].filter(Boolean),
            timestamp: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('‚ùå Pre-upload check failed:', error);
        res.status(500).json({
            success: false,
            ready_for_upload: false,
            error: 'System health check failed',
            message: error.message,
            recommendations: ['System temporarily unavailable - please try again in a few moments']
        });
    }
});

// Quick test endpoint for debugging
app.get('/api/quick-stats', async (req, res) => {
    try {
        const [docCount, chunkCount] = await Promise.all([
            db.pool.query('SELECT COUNT(*) FROM processed_content WHERE record_type = $1', ['document']),
            db.pool.query('SELECT COUNT(*) FROM processed_content WHERE record_type = $1', ['chunk'])
        ]);
        
        res.json({
            documents: parseInt(docCount.rows[0].count),
            chunks: parseInt(chunkCount.rows[0].count),
            status: 'ok',
            timestamp: new Date().toISOString()
        });
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});

// ENHANCED: Advanced manual cleanup endpoint  
app.post('/api/cleanup-sessions/advanced', async (req, res) => {
    try {
        console.log('üßπ Advanced manual cleanup requested');
        
        const { 
            thresholdMinutes = null, 
            enableHealthCheck = true,
            enableOrphanCleanup = true,
            enableMemoryCleanup = true 
        } = req.body;
        
        const results = await advancedSessionCleanup({
            thresholdMinutes,
            enableHealthCheck,
            enableOrphanCleanup,
            enableMemoryCleanup
        });
        
        res.json({
            success: true,
            message: `Advanced cleanup completed`,
            results: {
                sessions_cleaned: results.sessions_cleaned,
                chunks_recovered: results.chunks_recovered,
                memory_freed: results.memory_freed,
                health_issues_found: results.health_issues.length,
                total_response_time_ms: Date.now() - new Date(results.timestamp).getTime()
            },
            detailed_results: results,
            timestamp: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('‚ùå Advanced manual cleanup failed:', error);
        res.status(500).json({
            success: false,
            error: 'Failed to run advanced cleanup',
            message: error.message
        });
    }
});

// Manual session cleanup endpoint
app.post('/api/cleanup-sessions', async (req, res) => {
    try {
        console.log('üßπ Manual session cleanup requested');
        
        const { threshold } = req.body;
        const thresholdMinutes = threshold ? parseInt(threshold) : null;
        
        const cleanedCount = await cleanupStuckSessions(thresholdMinutes);
        
        res.json({
            success: true,
            message: `Cleaned up ${cleanedCount} stuck processing sessions`,
            cleaned_count: cleanedCount,
            threshold_minutes: thresholdMinutes || parseInt(process.env.SESSION_CLEANUP_THRESHOLD || '15'),
            timestamp: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('‚ùå Manual cleanup failed:', error);
        res.status(500).json({
            success: false,
            error: 'Failed to cleanup sessions',
            message: error.message
        });
    }
});

// Session cleanup status endpoint
app.get('/api/cleanup-status', async (req, res) => {
    try {
        const [stuckSessions, recentCleanups] = await Promise.all([
            // Count current stuck sessions
            db.pool.query(`
                SELECT COUNT(*) as stuck_count,
                       MIN(updated_at) as oldest_stuck
                FROM upload_sessions 
                WHERE status = 'processing' 
                AND updated_at < NOW() - INTERVAL '15 minutes'
            `),
            // Get recent cleanup activity (if we stored it)
            db.pool.query(`
                SELECT status, COUNT(*) as count
                FROM upload_sessions 
                WHERE error_message LIKE 'Auto-cleanup:%'
                AND updated_at > NOW() - INTERVAL '24 hours'
                GROUP BY status
            `)
        ]);
        
        const config = {
            cleanup_interval_minutes: parseInt(process.env.SESSION_CLEANUP_INTERVAL || '300') / 60,
            cleanup_threshold_minutes: parseInt(process.env.SESSION_CLEANUP_THRESHOLD || '15'),
            enabled: true
        };
        
        res.json({
            success: true,
            config,
            current_stuck_sessions: parseInt(stuckSessions.rows[0].stuck_count),
            oldest_stuck_session: stuckSessions.rows[0].oldest_stuck,
            recent_cleanups: recentCleanups.rows,
            timestamp: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('‚ùå Cleanup status error:', error);
        res.status(500).json({
            success: false,
            error: 'Failed to get cleanup status',
            message: error.message
        });
    }
});

// Check for stuck upload sessions (for frontend recovery)
app.get('/api/upload-sessions/check-stuck', async (req, res) => {
    try {
        const stuckSessions = await db.pool.query(`
            SELECT session_id, filename, url, status, created_at, updated_at,
                   EXTRACT(EPOCH FROM (NOW() - updated_at))/60 as minutes_stuck
            FROM upload_sessions 
            WHERE status = 'processing' 
              AND updated_at < NOW() - INTERVAL '5 minutes'
            ORDER BY updated_at ASC
        `);
        
        res.json({
            success: true,
            stuckSessions: stuckSessions.rows,
            count: stuckSessions.rows.length,
            timestamp: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('‚ùå Check stuck sessions error:', error);
        res.status(500).json({
            success: false,
            error: 'Failed to check stuck sessions',
            message: error.message
        });
    }
});

// Cleanup stuck upload sessions (for frontend recovery)
app.post('/api/upload-sessions/cleanup-stuck', async (req, res) => {
    try {
        console.log('üßπ Frontend-requested session cleanup');
        
        const cleanedCount = await cleanupStuckSessions(5); // 5-minute threshold for frontend cleanup
        
        res.json({
            success: true,
            message: `Cleaned up ${cleanedCount} stuck processing sessions`,
            cleaned_count: cleanedCount,
            timestamp: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('‚ùå Frontend cleanup failed:', error);
        res.status(500).json({
            success: false,
            error: 'Failed to cleanup stuck sessions',
            message: error.message
        });
    }
});

// Automatic session cleanup system
// Heartbeat monitoring system
function createHeartbeatMonitoredCallback(originalCallback, uploadSession, filename) {
    let lastHeartbeat = Date.now();
    let totalProgress = 0;
    
    // Start heartbeat monitor
    const heartbeatInterval = setInterval(async () => {
        const timeSinceLastHeartbeat = Date.now() - lastHeartbeat;
        
        // If no progress for 90 seconds, consider it stuck
        if (timeSinceLastHeartbeat > 90000) {
            console.warn(`‚ö†Ô∏è Heartbeat timeout detected for ${filename} (${timeSinceLastHeartbeat}ms since last update)`);
            
            // Update session as potentially stuck
            if (uploadSession?.id) {
                try {
                    await db.pool.query(
                        'UPDATE upload_sessions SET error_message = $1, updated_at = NOW() WHERE id = $2',
                        [`Heartbeat timeout - no progress for ${Math.round(timeSinceLastHeartbeat/1000)}s`, uploadSession.id]
                    );
                } catch (error) {
                    console.error('Failed to update heartbeat timeout:', error);
                }
            }
            
            // Send heartbeat timeout notification
            originalCallback('heartbeat_timeout', {
                message: 'Processing is taking longer than expected...',
                time_since_last_update: timeSinceLastHeartbeat,
                total_progress: totalProgress,
                sessionId: uploadSession?.sessionId
            });
        }
    }, 30000); // Check every 30 seconds
    
    // Return enhanced callback that tracks heartbeats
    return (step, data) => {
        lastHeartbeat = Date.now();
        
        // Track overall progress
        if (data?.progress) {
            totalProgress = data.progress;
        }
        
        // Send heartbeat update every significant progress
        if (step === 'progress' || step === 'chunk_complete') {
            originalCallback('heartbeat', {
                timestamp: lastHeartbeat,
                step,
                progress: totalProgress,
                sessionId: uploadSession?.sessionId
            });
        }
        
        // Clear interval on completion or error
        if (step === 'complete' || step === 'error') {
            clearInterval(heartbeatInterval);
        }
        
        // Call original callback
        return originalCallback(step, data);
    };
}

// Error classification and recovery system
function classifyError(error) {
    const message = error.message?.toLowerCase() || '';
    const stack = error.stack?.toLowerCase() || '';
    
    // Network/connection errors
    if (message.includes('timeout') || message.includes('econnreset') || message.includes('enotfound')) {
        return 'network';
    }
    
    // Database errors
    if (message.includes('connection') && (message.includes('database') || message.includes('postgres'))) {
        return 'database';
    }
    
    // Qdrant/vector database errors  
    if (message.includes('qdrant') || message.includes('vector') || stack.includes('qdrant')) {
        return 'vector_db';
    }
    
    // File parsing errors
    if (message.includes('parse') || message.includes('invalid file') || message.includes('corrupt')) {
        return 'file_format';
    }
    
    // Memory/resource errors
    if (message.includes('out of memory') || message.includes('heap') || message.includes('maximum call stack')) {
        return 'resource';
    }
    
    // API/OpenAI errors
    if (message.includes('openai') || message.includes('api key') || message.includes('rate limit')) {
        return 'api';
    }
    
    // Timeout errors
    if (message.includes('timeout') || message.includes('abort')) {
        return 'timeout';
    }
    
    return 'unknown';
}

function getRecoverySuggestions(errorType, error) {
    const suggestions = {
        network: [
            'Check your internet connection',
            'Try uploading again in a few moments',
            'Ensure the file is not corrupted'
        ],
        database: [
            'System temporarily unavailable - please try again',
            'Contact support if the issue persists'
        ],
        vector_db: [
            'Vector database temporarily unavailable',
            'Try again in a few minutes',
            'Your file content was preserved and can be retried'
        ],
        file_format: [
            'Check that your file is not corrupted',
            'Ensure the file format is supported (PDF, EPUB, DOCX, TXT, CSV)',
            'Try saving the file in a different format'
        ],
        resource: [
            'File may be too large or complex',
            'Try splitting large files into smaller sections',
            'Wait a moment for system resources to become available'
        ],
        api: [
            'AI processing service temporarily unavailable',
            'Try again in a few minutes',
            'Check if API keys are properly configured'
        ],
        timeout: [
            'Processing took too long - this may be due to a large file',
            'Try uploading a smaller file first',
            'Wait a moment and try again'
        ],
        temporary: [
            'Temporary system issue',
            'Try uploading again in a few moments'
        ],
        unknown: [
            'An unexpected error occurred',
            'Try uploading again',
            'Contact support if the issue continues'
        ]
    };
    
    return suggestions[errorType] || suggestions.unknown;
}

// System health check function
async function checkSystemHealth() {
    const health = {
        database: false,
        qdrant: false
    };
    
    try {
        // Check database connection
        const dbResult = await db.pool.query('SELECT 1');
        health.database = true;
    } catch (error) {
        console.error('Database health check failed:', error.message);
    }
    
    try {
        // Check Qdrant connection
        const qdrantResponse = await axios.get(`${QDRANT_URL}/health`, {
            headers: { 'api-key': QDRANT_API_KEY },
            timeout: 3000
        });
        health.qdrant = qdrantResponse.status === 200;
    } catch (error) {
        console.error('Qdrant health check failed:', error.message);
    }
    
    return health;
}

// Enhanced session cleanup with comprehensive health monitoring
async function advancedSessionCleanup(options = {}) {
    const {
        thresholdMinutes = null,
        enableHealthCheck = true,
        enableOrphanCleanup = true,
        enableMemoryCleanup = true
    } = options;

    const client = await db.pool.connect();
    const cleanupResults = {
        timestamp: new Date().toISOString(),
        sessions_cleaned: 0,
        chunks_recovered: 0,
        memory_freed: false,
        health_issues: [],
        performance_metrics: {}
    };

    try {
        const startTime = Date.now();
        await client.query('BEGIN');

        // 1. HEARTBEAT-BASED HEALTH MONITORING
        if (enableHealthCheck) {
            console.log('ü©∫ Running health check and heartbeat monitoring...');
            
            // Mark sessions without heartbeat as failed (90 second threshold)
            const heartbeatResult = await client.query(`
                UPDATE upload_sessions 
                SET status = 'failed', 
                    error_message = 'Health check timeout - no heartbeat detected',
                    completed_at = NOW()
                WHERE status = 'processing' 
                  AND last_activity < NOW() - INTERVAL '90 seconds'
                RETURNING session_id, filename, last_activity
            `);

            cleanupResults.sessions_cleaned += heartbeatResult.rowCount;
            
            if (heartbeatResult.rowCount > 0) {
                console.log(`üíó Heartbeat cleanup: ${heartbeatResult.rowCount} sessions without heartbeat`);
                heartbeatResult.rows.forEach(row => {
                    const stuckDuration = Math.round((Date.now() - new Date(row.last_activity).getTime()) / 1000);
                    console.log(`   ‚îî‚îÄ‚îÄ Session ${row.session_id} (${row.filename}) stuck for ${stuckDuration}s`);
                });
            }
        }

        // 2. PROCESS TIMEOUT RECOVERY
        const threshold = thresholdMinutes || parseInt(process.env.SESSION_CLEANUP_THRESHOLD || '8');
        const timeoutResult = await client.query(`
            UPDATE upload_sessions 
            SET status = 'failed',
                error_message = $2,
                completed_at = NOW()
            WHERE status = 'processing' 
              AND created_at < NOW() - INTERVAL '1 minute' * $1
            RETURNING session_id, filename, created_at
        `, [threshold, `Process timeout - stuck for ${threshold}+ minutes`]);

        cleanupResults.sessions_cleaned += timeoutResult.rowCount;

        if (timeoutResult.rowCount > 0) {
            console.log(`‚è∞ Timeout cleanup: ${timeoutResult.rowCount} sessions exceeded ${threshold}m limit`);
            timeoutResult.rows.forEach(row => {
                const totalDuration = Math.round((Date.now() - new Date(row.created_at).getTime()) / 1000 / 60);
                console.log(`   ‚îî‚îÄ‚îÄ Session ${row.session_id} (${row.filename}) ran for ${totalDuration}m`);
            });
        }

        // 3. ORPHANED CHUNK RECOVERY
        if (enableOrphanCleanup) {
            const orphanResult = await client.query(`
                UPDATE processed_content 
                SET processing_status = 'failed',
                    processed_date = NOW()
                WHERE processing_status = 'processing' 
                  AND processed_date < NOW() - INTERVAL '10 minutes'
                RETURNING url
            `);

            cleanupResults.chunks_recovered = orphanResult.rowCount;
            
            if (orphanResult.rowCount > 0) {
                console.log(`üîÑ Orphan recovery: ${orphanResult.rowCount} stuck chunks recovered`);
                console.log(`   ‚îî‚îÄ‚îÄ URLs affected: ${[...new Set(orphanResult.rows.map(r => r.url))].length} unique URLs`);
            }
        }

        // 4. IN-MEMORY SESSION CLEANUP
        if (enableMemoryCleanup) {
            let memoryCleanedCount = 0;
            for (const [sessionId, session] of activeProcessingSessions.entries()) {
                const timeSinceLastUpdate = Date.now() - (session.lastUpdate?.getTime() || 0);
                
                // Remove sessions stuck for more than 5 minutes from memory
                if (timeSinceLastUpdate > 300000) {
                    activeProcessingSessions.delete(sessionId);
                    memoryCleanedCount++;
                }
            }
            
            if (memoryCleanedCount > 0) {
                console.log(`üß† Memory cleanup: ${memoryCleanedCount} sessions removed from active tracking`);
                cleanupResults.memory_freed = true;
            }
        }

        // 5. SYSTEM HEALTH DIAGNOSTICS (separate from transaction)
        // Commit transaction first to avoid issues with complex queries
        await client.query('COMMIT');
        
        try {
            const healthCheck = await Promise.allSettled([
                // Database connection health  
                db.pool.query('SELECT NOW() as db_time'),
                
                // Active sessions analysis
                db.pool.query(`
                    SELECT 
                        status,
                        COUNT(*) as count,
                        AVG(EXTRACT(EPOCH FROM (NOW() - created_at))/60)::int as avg_duration_minutes,
                        MAX(EXTRACT(EPOCH FROM (NOW() - created_at))/60)::int as max_duration_minutes
                    FROM upload_sessions 
                    WHERE created_at > NOW() - INTERVAL '24 hours'
                    GROUP BY status
                `),
                
                // Recent failure analysis
                db.pool.query(`
                    SELECT 
                        LEFT(error_message, 50) as error_type,
                        COUNT(*) as error_count
                    FROM upload_sessions 
                    WHERE status = 'failed' 
                      AND updated_at > NOW() - INTERVAL '1 hour'
                      AND error_message IS NOT NULL
                    GROUP BY LEFT(error_message, 50)
                    ORDER BY error_count DESC
                    LIMIT 5
                `)
            ]);

            // Process health check results
            if (healthCheck[0].status === 'fulfilled') {
                cleanupResults.performance_metrics.database = {
                    response_time_ms: Date.now() - startTime,
                    status: 'healthy'
                };
            }

            if (healthCheck[1].status === 'fulfilled') {
                cleanupResults.performance_metrics.sessions = healthCheck[1].value.rows.reduce((acc, row) => {
                    acc[row.status] = {
                        count: parseInt(row.count),
                        avg_duration_minutes: parseFloat(row.avg_duration_minutes || 0),
                        max_duration_minutes: parseFloat(row.max_duration_minutes || 0)
                    };
                    return acc;
                }, {});
            }

            if (healthCheck[2].status === 'fulfilled') {
                cleanupResults.health_issues = healthCheck[2].value.rows;
            }
        } catch (healthError) {
            console.warn('Health diagnostics failed:', healthError.message);
            cleanupResults.health_issues.push({
                error_type: 'health_diagnostics_failure',
                error_count: 1,
                message: healthError.message
            });
        }
        
        // Log comprehensive results
        if (cleanupResults.sessions_cleaned > 0 || cleanupResults.chunks_recovered > 0) {
            console.log(`üßπ Enhanced cleanup completed:`, {
                sessions_cleaned: cleanupResults.sessions_cleaned,
                chunks_recovered: cleanupResults.chunks_recovered,
                total_time_ms: Date.now() - startTime,
                threshold_minutes: threshold
            });
        }

        return cleanupResults;

    } catch (error) {
        await client.query('ROLLBACK');
        console.error('‚ùå Advanced cleanup failed:', error.message);
        cleanupResults.health_issues.push({
            error_type: 'cleanup_failure',
            error_count: 1,
            message: error.message
        });
        return cleanupResults;
    } finally {
        client.release();
    }
}

// Legacy wrapper for compatibility
async function cleanupStuckSessions(thresholdMinutes = null) {
    const results = await advancedSessionCleanup({ thresholdMinutes });
    return results.sessions_cleaned;
}

function startAutomaticSessionCleanup() {
    // Enhanced cleanup with health monitoring
    const CLEANUP_INTERVAL = parseInt(process.env.SESSION_CLEANUP_INTERVAL || '60') * 1000; // Default: 1 minute (more aggressive)
    const HEALTH_CHECK_INTERVAL = 30000; // Health checks every 30 seconds
    const EMERGENCY_CLEANUP_INTERVAL = 15000; // Emergency cleanup every 15 seconds
    
    console.log(`üßπ Starting enhanced session cleanup system:`);
    console.log(`   üìä Health monitoring: every ${HEALTH_CHECK_INTERVAL/1000}s`);
    console.log(`   üßπ Regular cleanup: every ${CLEANUP_INTERVAL/60000}m`);
    console.log(`   üö® Emergency cleanup: every ${EMERGENCY_CLEANUP_INTERVAL/1000}s`);
    
    // Initial comprehensive cleanup after startup
    setTimeout(async () => {
        console.log('üöÄ Running initial comprehensive cleanup...');
        const results = await advancedSessionCleanup({ 
            thresholdMinutes: 5,  // Aggressive startup cleanup
            enableHealthCheck: true,
            enableOrphanCleanup: true,
            enableMemoryCleanup: true 
        });
        
        console.log(`‚úÖ Startup cleanup complete:`, {
            sessions_cleaned: results.sessions_cleaned,
            chunks_recovered: results.chunks_recovered,
            memory_freed: results.memory_freed
        });
    }, 10000); // Wait 10s after startup
    
    // Main enhanced cleanup interval 
    setInterval(async () => {
        try {
            const results = await advancedSessionCleanup();
            
            // Log detailed results for monitoring
            if (results.sessions_cleaned > 0 || results.chunks_recovered > 0 || results.health_issues.length > 0) {
                console.log(`üîç Health monitoring results:`, {
                    timestamp: results.timestamp,
                    sessions_cleaned: results.sessions_cleaned,
                    chunks_recovered: results.chunks_recovered,
                    health_issues: results.health_issues.length,
                    database_response_ms: results.performance_metrics.database?.response_time_ms
                });
                
                // Alert on significant issues
                if (results.health_issues.length > 0) {
                    console.warn(`‚ö†Ô∏è Health issues detected:`, results.health_issues);
                }
            }
        } catch (error) {
            console.error('‚ùå Enhanced cleanup failed:', error.message);
        }
    }, CLEANUP_INTERVAL);
    
    // Emergency cleanup for critical stuck sessions (heartbeat-only)
    setInterval(async () => {
        try {
            const results = await advancedSessionCleanup({ 
                enableHealthCheck: true,  // Only heartbeat monitoring
                enableOrphanCleanup: false,
                enableMemoryCleanup: false,
                thresholdMinutes: 2  // Very short threshold for emergency
            });
            
            if (results.sessions_cleaned > 0) {
                console.log(`üö® Emergency cleanup: ${results.sessions_cleaned} critical sessions recovered`);
            }
        } catch (error) {
            console.error('‚ùå Emergency cleanup failed:', error.message);
        }
    }, EMERGENCY_CLEANUP_INTERVAL);
}

// Initialize database and start server
async function startServer() {
    console.log('üöÄ Starting AutoLlama API server with real-time PostgreSQL...');
    
    // Initialize database connection
    const dbReady = await db.initializeDatabase();
    if (!dbReady) {
        console.error('‚ùå Failed to initialize database. Exiting...');
        process.exit(1);
    }
    
    console.log('üîß Database initialization completed, proceeding to API configuration...');
    
    // Always setup documentation routes (they don't depend on external services)
    try {
        console.log('üìö Setting up API documentation routes...');
        const docsRoutes = require('./src/routes/docs.routes');
        app.use('/', docsRoutes);
        console.log('‚úÖ API documentation routes enabled successfully');
    } catch (error) {
        console.warn('‚ö†Ô∏è Documentation routes setup failed:', error.message);
    }

    // Initialize new route system (Phase 2 - Day 6) - TEMPORARILY DISABLED to fix upload issues
    try {
        console.log('üîß New route system temporarily disabled - using original endpoints for file uploads');
        // const services = { 
        //     aiServices, 
        //     storageServices, 
        //     database: db 
        // };
        const services = {
            aiServices, 
            storageServices, 
            database: db 
        };
        setupRoutes(app, services);
        routesEnabled = true;
        console.log('‚ö†Ô∏è Using original file upload endpoints until route conflict is resolved');
    } catch (error) {
        console.warn('‚ö†Ô∏è New route system failed, using original endpoints:', error.message);
    }
    
    // Initialize API configuration from database
    console.log('üîß About to initialize API configuration...');
    const configReady = await initializeApiConfiguration();
    console.log(`üîß API configuration result: ${configReady}`);
    
    console.log('üîß Configuration completed, proceeding to WebSocket...');
    
    // Initialize WebSocket server
    console.log('üîß About to initialize WebSocket server...');
    const wsReady = initializeWebSocket();
    console.log(`üîß WebSocket initialization result: ${wsReady}`);
    if (!wsReady) {
        console.warn('‚ö†Ô∏è WebSocket server failed to start, continuing without real-time updates');
    }
    
    // Start the Express server
    app.listen(PORT, () => {
        console.log(`AutoLlama API server running on port ${PORT}`);
        console.log(`üîå Database: PostgreSQL with hybrid caching`);
        console.log(`üìã Endpoints:`);
        console.log(`   GET /api/recent-records - Smart content mix (real-time + cached)`);
        console.log(`   GET /api/in-progress - Real-time active sessions`);
        console.log(`   GET /health - Health check with database status`);
        console.log('‚úÖ Server ready for real-time performance!');
        
        // Start automatic session cleanup
        startAutomaticSessionCleanup();
    });
}

startServer().catch(error => {
    console.error('üí• Failed to start server:', error);
    process.exit(1);
});