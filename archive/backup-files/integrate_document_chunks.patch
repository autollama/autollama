// Integration patch for server.js to support Document/Chunk distinction
// Apply these changes to your existing server.js file

// 1. Import the new database functions at the top
const {
    // ... existing imports ...
    createDocumentRecord,
    getDocumentsOnly,
    getChunksByDocumentId,
    linkChunksToDocument
} = require('./database');

// 2. Modify processContentChunks function to create document first
// Find the processContentChunks function and add document creation after chunking:

// Around line 2013, after chunks are created:
console.log(`Created ${chunks.length} chunks`);

// ADD THIS BLOCK:
// Create a document record first
let documentRecord = null;
try {
    const title = url.split('/').pop()?.replace(/[-_]/g, ' ').replace(/\.\w+$/, '') || 'Untitled';
    
    documentRecord = await createDocumentRecord({
        url: url,
        title: title,
        summary: `Document with ${chunks.length} chunks`,
        full_content: content.substring(0, 5000),
        upload_source: 'user'
    });
    
    console.log('ðŸ“„ Created document record:', documentRecord.id);
    
    if (sseCallback) {
        sseCallback({
            type: 'document_created',
            documentId: documentRecord.id,
            title: title,
            totalChunks: chunks.length
        });
    }
} catch (error) {
    console.error('Error creating document record:', error);
}

// 3. Modify storeInAirtableAndPostgres to include parent document
// Around line 465, update the contentData object:

const contentData = {
    // ... existing fields ...
    upload_source: uploadSource,
    record_type: 'chunk', // ADD THIS
    parent_document_id: parentDocumentId // ADD THIS (pass as parameter)
};

// 4. Update the chunk processing loop to pass document ID
// Around line 2090, in the processing loop:

const { airtableId, postgresId } = await storeInAirtableAndPostgres(
    processedChunk,
    'user', // upload source
    documentRecord ? documentRecord.id : null // ADD THIS - parent document ID
);

// 5. Add new API endpoints for documents
// Add these endpoints after existing routes:

// Get all documents (not chunks)
app.get('/api/documents', async (req, res) => {
    try {
        const limit = parseInt(req.query.limit) || 50;
        const offset = parseInt(req.query.offset) || 0;
        
        const documents = await getDocumentsOnly(limit, offset);
        
        res.json({
            success: true,
            documents: documents,
            count: documents.length
        });
    } catch (error) {
        console.error('Error fetching documents:', error);
        res.status(500).json({
            success: false,
            error: 'Failed to fetch documents'
        });
    }
});

// Get chunks for a specific document
app.get('/api/document/:documentId/chunks', async (req, res) => {
    try {
        const { documentId } = req.params;
        const chunks = await getChunksByDocumentId(documentId);
        
        res.json({
            success: true,
            documentId: documentId,
            chunks: chunks,
            count: chunks.length
        });
    } catch (error) {
        console.error('Error fetching document chunks:', error);
        res.status(500).json({
            success: false,
            error: 'Failed to fetch chunks'
        });
    }
});

// 6. Update the storeInAirtableAndPostgres function signature
// Change from:
async function storeInAirtableAndPostgres(chunkData, uploadSource = 'user') {

// To:
async function storeInAirtableAndPostgres(chunkData, uploadSource = 'user', parentDocumentId = null) {

// 7. For file uploads, also create document records
// In the processFile function, add similar document creation logic

// After chunks are created:
const documentRecord = await createDocumentRecord({
    url: filePath,
    title: file.originalname,
    summary: `Uploaded file with ${chunks.length} chunks`,
    full_content: content.substring(0, 5000),
    upload_source: uploadSource || 'user'
});

// Then pass documentRecord.id when storing chunks