const express = require('express');
const cors = require('cors');
const fetch = require('node-fetch');
const axios = require('axios');
const https = require('https');
const TurndownService = require('turndown');
const OpenAI = require('openai');
const { v4: uuidv4 } = require('uuid');
const cheerio = require('cheerio');
const pdfParse = require('pdf-parse');
const Busboy = require('busboy');
const mammoth = require('mammoth');
const epub = require('epub');
const { parse: csvParse } = require('csv-parse');
const tmp = require('tmp');

// Import the new PostgreSQL database layer
const db = require('./database');

const app = express();
const PORT = process.env.PORT || 3001;

// Enable CORS for all origins
app.use(cors());
app.use(express.json());

// Initialize services
const turndownService = new TurndownService();
const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY
});

// Configuration
const AIRTABLE_BASE_ID = process.env.AIRTABLE_BASE_ID || 'appi5lnDWjvstGsqr';
const AIRTABLE_TABLE_ID = process.env.AIRTABLE_TABLE_ID || 'tblrO3XjykQIqURb4';
const AIRTABLE_UPLOAD_SESSIONS_TABLE_ID = process.env.AIRTABLE_UPLOAD_SESSIONS_TABLE_ID || 'tblUploadSessions'; // You'll need to update this with actual table ID
const AIRTABLE_API_KEY = process.env.AIRTABLE_API_KEY;
const QDRANT_URL = process.env.QDRANT_URL || 'https://c4c8ee46-d9dd-4c0f-a00e-9215675351da.us-west-1-0.aws.cloud.qdrant.io';
const QDRANT_API_KEY = process.env.QDRANT_API_KEY || '[QDRANT_API_KEY_REMOVED]';

// In-memory tracking for active processing sessions
const activeProcessingSessions = new Map();

// Simple cache for Airtable records
const recordsCache = {
    data: null,
    timestamp: null,
    ttl: 3600000 // 1 hour cache
};

// User agents for rotation
const USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15'
];

// Processing functions
async function fetchWebContent(url, retryCount = 0) {
    const maxRetries = 2;
    const userAgent = USER_AGENTS[retryCount % USER_AGENTS.length];
    
    try {
        // Add random delay to appear more human-like
        if (retryCount > 0) {
            await new Promise(resolve => setTimeout(resolve, Math.random() * 2000 + 1000));
        }
        
        const response = await axios.get(url, {
            headers: {
                'User-Agent': userAgent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'Connection': 'keep-alive'
            },
            timeout: 30000,
            responseType: 'arraybuffer',  // Get response as buffer to handle both HTML and PDF
            httpsAgent: new https.Agent({
                rejectUnauthorized: false  // Allow self-signed certificates for academic/research sites
            })
        });
        
        const contentType = response.headers['content-type'] || '';
        
        // Check if it's a PDF
        if (contentType.includes('application/pdf') || url.toLowerCase().endsWith('.pdf')) {
            console.log('Detected PDF content, extracting text...');
            const pdfData = await pdfParse(response.data);
            return { 
                content: pdfData.text, 
                type: 'pdf',
                metadata: {
                    pages: pdfData.numpages,
                    info: pdfData.info
                }
            };
        } else {
            // Convert buffer back to string for HTML content
            const htmlContent = response.data.toString('utf-8');
            return { 
                content: htmlContent, 
                type: 'html' 
            };
        }
    } catch (error) {
        // Check if it's a 403 and we haven't exhausted retries
        if (error.response?.status === 403 && retryCount < maxRetries) {
            console.log(`Attempt ${retryCount + 1} failed with 403, retrying with different user agent...`);
            return fetchWebContent(url, retryCount + 1);
        }
        
        // Provide more helpful error messages
        if (error.response?.status === 403) {
            throw new Error(`Access denied (403): This website blocks automated access. The site may require JavaScript, have CAPTCHA protection, or block all automated requests. Try using a different URL or accessing the content manually.`);
        } else if (error.response?.status === 404) {
            throw new Error(`Page not found (404): The URL doesn't exist or has been moved.`);
        } else if (error.response?.status >= 500) {
            throw new Error(`Server error (${error.response.status}): The website is experiencing issues.`);
        } else {
            throw new Error(`Failed to fetch URL: ${error.message}`);
        }
    }
}

function htmlToMarkdown(html) {
    const $ = cheerio.load(html);
    // Remove script and style elements
    $('script, style, nav, footer, aside').remove();
    // Get main content (try common content selectors)
    const contentSelectors = ['main', 'article', '.content', '.post', '#content', '.entry-content'];
    let content = '';
    
    for (const selector of contentSelectors) {
        const element = $(selector);
        if (element.length > 0 && element.text().trim().length > 100) {
            content = element.html();
            break;
        }
    }
    
    // If no main content found, use body but clean it up
    if (!content) {
        $('header, nav, footer, aside, .sidebar, .menu, .navigation').remove();
        content = $('body').html() || html;
    }
    
    return turndownService.turndown(content);
}

function chunkText(content, url, chunkSize = 1200, overlap = 200) {
    if (!content || content.length === 0) {
        throw new Error('No content to chunk');
    }
    
    const cleanContent = content.replace(/\s+/g, ' ').trim();
    const chunks = [];
    
    for (let i = 0; i < cleanContent.length; i += chunkSize - overlap) {
        const chunk = cleanContent.slice(i, i + chunkSize);
        const chunkId = uuidv4();
        
        chunks.push({
            chunk_text: chunk.trim(),
            chunk_id: chunkId,
            chunk_index: Math.floor(i / (chunkSize - overlap)),
            original_url: url,
            total_chunks: Math.ceil(cleanContent.length / (chunkSize - overlap))
        });
    }
    
    return chunks;
}

async function analyzeChunk(chunkText) {
    const systemPrompt = `You are a RAG content analyzer. Extract structured metadata from text chunks.

Return JSON with these exact fields:
{
  "title": "Main title/topic (string)",
  "summary": "2-3 sentence summary (string)", 
  "category": "Primary category (string)",
  "tags": ["tag1", "tag2", "tag3"] (array of strings),
  "key_concepts": ["concept1", "concept2"] (array of strings),
  "content_type": "article|blog|academic|news|reference|other (string)",
  "technical_level": "beginner|intermediate|advanced (string)",
  "sentiment": "positive|negative|neutral|mixed (string)",
  "emotions": ["emotion1", "emotion2"] (array from: joy, sadness, anger, fear, surprise, disgust, trust, anticipation),
  "key_entities": {
    "people": ["name1", "name2"] (array of person names mentioned),
    "organizations": ["org1", "org2"] (array of organization names),
    "locations": ["location1", "location2"] (array of place names)
  },
  "main_topics": ["topic1", "topic2", "topic3"] (array of broader topics beyond the title)
}

Analyze thoroughly but keep responses concise and focused.`;

    try {
        const response = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                { role: 'system', content: systemPrompt },
                { role: 'user', content: `Process this text chunk: ${chunkText}` }
            ],
            response_format: { type: 'json_object' }
        });
        
        return JSON.parse(response.choices[0].message.content);
    } catch (error) {
        throw new Error(`Failed to analyze chunk: ${error.message}`);
    }
}

async function generateEmbedding(text) {
    try {
        const response = await openai.embeddings.create({
            model: 'text-embedding-3-small',
            input: text
        });
        
        return response.data[0].embedding;
    } catch (error) {
        throw new Error(`Failed to generate embedding: ${error.message}`);
    }
}

async function storeInQdrant(chunkData, embedding, analysis) {
    try {
        const payload = {
            url: chunkData.original_url,
            title: analysis.title,
            chunk_text: chunkData.chunk_text,
            chunk_index: chunkData.chunk_index,
            summary: analysis.summary,
            category: analysis.category,
            tags: analysis.tags,
            key_concepts: analysis.key_concepts,
            content_type: analysis.content_type,
            technical_level: analysis.technical_level,
            processed_date: new Date().toISOString()
        };
        
        const response = await axios.put(
            `${QDRANT_URL}/collections/autollama-content/points`,
            {
                points: [{
                    id: chunkData.chunk_id,
                    vector: embedding,
                    payload: payload
                }]
            },
            {
                headers: {
                    'api-key': QDRANT_API_KEY,
                    'Content-Type': 'application/json'
                }
            }
        );
        
        return response.data;
    } catch (error) {
        throw new Error(`Failed to store in Qdrant: ${error.message}`);
    }
}

async function storeInPostgreSQL(chunkData, analysis, embeddingStatus = 'unknown', sessionId = null) {
    if (!AIRTABLE_API_KEY || AIRTABLE_API_KEY === 'your_airtable_api_key_here') {
        console.log('Skipping Airtable storage - no API key configured');
        return { status: 'skipped' };
    }
    
    try {
        console.log('Attempting Airtable storage with analysis:', analysis);
        
        // Ensure analysis has required fields
        if (!analysis || !analysis.title || !analysis.summary) {
            throw new Error('Analysis object missing required fields');
        }
        
        // Helper function to map values to Airtable format
        const mapSentiment = (sentiment) => {
            if (!sentiment) return null;
            const sentimentMap = {
                'positive': 'Positive',
                'negative': 'Negative', 
                'neutral': 'Neutral',
                'mixed': 'Neutral' // fallback for mixed sentiment
            };
            return sentimentMap[sentiment.toLowerCase()] || 'Neutral';
        };

        const mapEmbeddingStatus = (status) => {
            const statusMap = {
                'success': 'complete',
                'failed': 'error',
                'pending': 'pending'
            };
            return statusMap[status] || 'pending';
        };

        const mapCategory = (category) => {
            if (!category) return null;
            // Map common categories to Airtable options
            const categoryMap = {
                'technology': 'Technology',
                'politics': 'US Politics',
                'us politics': 'US Politics',
                'world news': 'World News',
                'business': 'Business',
                'science': 'Science',
                'history': 'History',
                'web development': 'Technology',
                'internet': 'Technology'
            };
            return categoryMap[category.toLowerCase()] || 'General Interest';
        };

        const mapContentType = (contentType) => {
            if (!contentType) return null;
            const typeMap = {
                'article': 'article',
                'blog': 'article',
                'academic': 'documentation',
                'news': 'news',
                'reference': 'documentation',
                'tutorial': 'tutorial',
                'other': 'other'
            };
            return typeMap[contentType.toLowerCase()] || 'other';
        };

        // Build Airtable data with proper field formatting
        const airtableData = {
            "Title": analysis.title || 'Untitled',
            "Summary": analysis.summary || 'No summary available',
            "URL": chunkData.original_url,
            "Select": "Complete",
            "Source": "autollama.io",
            "Date Processed": new Date().toISOString().split('T')[0] // YYYY-MM-DD format
        };

        // Add optional fields with proper formatting
        const mappedCategory = mapCategory(analysis.category);
        if (mappedCategory) airtableData["Category"] = mappedCategory;

        const mappedSentiment = mapSentiment(analysis.sentiment);
        if (mappedSentiment) airtableData["Sentiment"] = mappedSentiment;

        // Emotions as array for multi-select field
        if (analysis.emotions && analysis.emotions.length > 0) {
            // Map emotion names to match Airtable options (capitalize first letter)
            const mappedEmotions = analysis.emotions.map(emotion => 
                emotion.charAt(0).toUpperCase() + emotion.slice(1).toLowerCase()
            ).filter(emotion => 
                ['Joy', 'Anger', 'Fear', 'Surprise', 'Sadness', 'Anticipation', 'Trust', 'Disgust'].includes(emotion)
            );
            if (mappedEmotions.length > 0) {
                airtableData["Emotions"] = mappedEmotions;
            }
        }

        // Text fields
        if (analysis.tags && analysis.tags.length > 0) {
            airtableData["Tags"] = analysis.tags.join(', ');
        }
        
        if (analysis.key_concepts && analysis.key_concepts.length > 0) {
            airtableData["Key Concepts"] = analysis.key_concepts.join(', ');
        }

        const mappedContentType = mapContentType(analysis.content_type);
        if (mappedContentType) airtableData["Content Type"] = mappedContentType;

        if (analysis.technical_level) {
            airtableData["Technical Level"] = analysis.technical_level.toLowerCase();
        }

        // JSON fields for complex data
        if (analysis.key_entities) {
            airtableData["Key Entities"] = JSON.stringify(analysis.key_entities);
        }

        if (analysis.main_topics && analysis.main_topics.length > 0) {
            airtableData["Main Topics"] = JSON.stringify(analysis.main_topics);
        }

        // Chunk-specific fields
        if (chunkData.chunk_id) {
            airtableData["Chunk ID"] = chunkData.chunk_id;
        }
        
        if (typeof chunkData.chunk_index === 'number') {
            airtableData["Chunk Index"] = chunkData.chunk_index;
        }

        if (chunkData.chunk_text) {
            airtableData["Chunk Text"] = chunkData.chunk_text;
        }

        // Embedding status
        airtableData["Embedding Status"] = mapEmbeddingStatus(embeddingStatus);
        
        // Session ID for resumable uploads (only add if the field exists in the table)
        // Note: This field may not exist in all Airtable configurations
        // if (sessionId) {
        //     airtableData["Session ID"] = sessionId;
        // }
        
        console.log('Airtable payload:', airtableData);
        
        const response = await axios.post(
            `https://api.airtable.com/v0/${AIRTABLE_BASE_ID}/${AIRTABLE_TABLE_ID}`,
            { fields: airtableData },
            {
                headers: {
                    'Authorization': `Bearer ${AIRTABLE_API_KEY}`,
                    'Content-Type': 'application/json'
                }
            }
        );
        
        console.log('Airtable storage successful!', response.data);
        return response.data;
    } catch (error) {
        console.error('AIRTABLE STORAGE ERROR:');
        console.error('Status:', error.response?.status);
        console.error('Response:', error.response?.data);
        console.error('Payload that failed:', JSON.stringify(airtableData, null, 2));
        
        if (error.response?.data?.error?.type === 'UNKNOWN_FIELD_NAME') {
            console.error('FIELD NAME ERROR: The field "' + error.response.data.error.message.replace('Unknown field name: "', '').replace('"', '') + '" does not exist in your Airtable table.');
            console.error('Please either:');
            console.error('1. Add this field to your Airtable table, or');
            console.error('2. Remove/comment this field from the code');
        }
        
        throw new Error(`Failed to store in Airtable: ${error.message}`);
    }
}

// Upload session management functions
async function createUploadSession(filename, totalChunks, filePath) {
    if (!AIRTABLE_API_KEY || AIRTABLE_API_KEY === 'your_airtable_api_key_here') {
        console.log('Skipping upload session creation - no API key configured');
        return { id: uuidv4(), status: 'skipped' };
    }
    
    try {
        const sessionId = uuidv4();
        const sessionData = {
            "Session ID": sessionId,
            "Filename": filename,
            "Total Chunks": totalChunks,
            "Completed Chunks": 0,
            "Status": "Processing",
            "File Path": filePath,
            "Created At": new Date().toISOString(),
            "Updated At": new Date().toISOString()
        };
        
        const response = await axios.post(
            `https://api.airtable.com/v0/${AIRTABLE_BASE_ID}/${AIRTABLE_UPLOAD_SESSIONS_TABLE_ID}`,
            { fields: sessionData },
            {
                headers: {
                    'Authorization': `Bearer ${AIRTABLE_API_KEY}`,
                    'Content-Type': 'application/json'
                }
            }
        );
        
        console.log('Upload session created:', response.data.id);
        return { id: response.data.id, sessionId: sessionId };
    } catch (error) {
        console.error('Failed to create upload session:', error.message);
        throw new Error(`Failed to create upload session: ${error.message}`);
    }
}

async function updateUploadSession(recordId, completedChunks, status = null) {
    if (!AIRTABLE_API_KEY || AIRTABLE_API_KEY === 'your_airtable_api_key_here') {
        console.log('Skipping upload session update - no API key configured');
        return { status: 'skipped' };
    }
    
    try {
        const updateData = {
            "Completed Chunks": completedChunks,
            "Updated At": new Date().toISOString()
        };
        
        if (status) {
            updateData["Status"] = status;
        }
        
        const response = await axios.patch(
            `https://api.airtable.com/v0/${AIRTABLE_BASE_ID}/${AIRTABLE_UPLOAD_SESSIONS_TABLE_ID}/${recordId}`,
            { fields: updateData },
            {
                headers: {
                    'Authorization': `Bearer ${AIRTABLE_API_KEY}`,
                    'Content-Type': 'application/json'
                }
            }
        );
        
        return response.data;
    } catch (error) {
        console.error('Failed to update upload session:', error.message);
        throw new Error(`Failed to update upload session: ${error.message}`);
    }
}

async function getUploadSession(sessionId) {
    if (!AIRTABLE_API_KEY || AIRTABLE_API_KEY === 'your_airtable_api_key_here') {
        console.log('Skipping upload session retrieval - no API key configured');
        return null;
    }
    
    try {
        const response = await axios.get(
            `https://api.airtable.com/v0/${AIRTABLE_BASE_ID}/${AIRTABLE_UPLOAD_SESSIONS_TABLE_ID}`,
            {
                params: {
                    filterByFormula: `{Session ID} = "${sessionId}"`
                },
                headers: {
                    'Authorization': `Bearer ${AIRTABLE_API_KEY}`,
                    'Content-Type': 'application/json'
                }
            }
        );
        
        if (response.data.records.length > 0) {
            return response.data.records[0];
        }
        return null;
    } catch (error) {
        console.error('Failed to get upload session:', error.message);
        throw new Error(`Failed to get upload session: ${error.message}`);
    }
}

async function getProcessedChunksForSession(sessionId) {
    if (!AIRTABLE_API_KEY || AIRTABLE_API_KEY === 'your_airtable_api_key_here') {
        console.log('Skipping processed chunks retrieval - no API key configured');
        return [];
    }
    
    try {
        const response = await axios.get(
            `https://api.airtable.com/v0/${AIRTABLE_BASE_ID}/${AIRTABLE_TABLE_ID}`,
            {
                params: {
                    filterByFormula: `{Session ID} = "${sessionId}"`
                },
                headers: {
                    'Authorization': `Bearer ${AIRTABLE_API_KEY}`,
                    'Content-Type': 'application/json'
                }
            }
        );
        
        return response.data.records.map(record => ({
            chunkIndex: record.fields['Chunk Index'],
            chunkId: record.fields['Chunk ID'],
            status: record.fields['Processing Status'] || 'completed'
        }));
    } catch (error) {
        console.error('Failed to get processed chunks:', error.message);
        return [];
    }
}

// Helper function to send SSE updates
function sendSSEUpdate(res, step, message, progress = null) {
    const data = {
        step,
        message,
        progress,
        timestamp: new Date().toISOString()
    };
    res.write(`data: ${JSON.stringify(data)}\n\n`);
}

// Server-Sent Events endpoint for real-time processing updates
app.post('/api/process-url-stream', async (req, res) => {
    // Set up SSE headers
    res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Cache-Control'
    });

    const startTime = Date.now();
    let processedChunks = 0;
    let qdrantStored = 0;
    let airtableStored = 0;

    try {
        const { url } = req.body;
        
        if (!url) {
            sendSSEUpdate(res, 'error', 'URL is required');
            res.end();
            return;
        }

        sendSSEUpdate(res, 'start', `ðŸš€ Starting processing for: ${url}`);

        // Step 1: Fetch web content
        sendSSEUpdate(res, 'fetch', 'ðŸ“¥ Fetching content from URL...');
        const contentData = await fetchWebContent(url);
        
        // Step 2: Content detection and conversion
        let markdown;
        if (contentData.type === 'pdf') {
            sendSSEUpdate(res, 'extract', `ðŸ“„ PDF detected - extracting text from ${contentData.metadata.pages} pages...`);
            markdown = contentData.content;
        } else {
            sendSSEUpdate(res, 'convert', 'ðŸ”„ HTML detected - converting to markdown...');
            markdown = htmlToMarkdown(contentData.content);
        }
        
        if (!markdown || markdown.trim().length < 100) {
            sendSSEUpdate(res, 'error', 'âŒ No substantial content found at URL');
            res.end();
            return;
        }

        // Step 3: Chunk the text
        sendSSEUpdate(res, 'chunk', 'âœ‚ï¸ Splitting content into chunks...');
        const chunks = chunkText(markdown, url);
        sendSSEUpdate(res, 'chunk', `âœ… Content split into ${chunks.length} chunks`);

        // Step 4: Process each chunk
        for (const chunk of chunks) {
            try {
                const chunkNum = chunk.chunk_index + 1;
                sendSSEUpdate(res, 'analyze', 
                    `ðŸ¤– Processing chunk ${chunkNum}/${chunks.length} with AI analysis...`, 
                    { current: chunkNum, total: chunks.length }
                );
                
                // Analyze chunk with GPT-4o-mini
                const analysis = await analyzeChunk(chunk.chunk_text);
                
                // Generate embedding
                sendSSEUpdate(res, 'embed', `ðŸ§  Generating embeddings for chunk ${chunkNum}/${chunks.length}...`);
                const embedding = await generateEmbedding(chunk.chunk_text);
                
                // Store in Qdrant first to determine embedding status
                let embeddingStatus = 'pending';
                try {
                    await storeInQdrant(chunk, embedding, analysis);
                    qdrantStored++;
                    embeddingStatus = 'success';
                    sendSSEUpdate(res, 'store', `ðŸ’¾ Stored chunk ${chunkNum} in vector database`);
                } catch (error) {
                    embeddingStatus = 'failed';
                    sendSSEUpdate(res, 'warning', `âš ï¸ Vector storage failed for chunk ${chunkNum}: ${error.message}`);
                }
                
                // Store in Airtable with embedding status
                try {
                    const result = await storeInAirtable(chunk, analysis, embeddingStatus);
                    if (result.status !== 'skipped') {
                        airtableStored++;
                        sendSSEUpdate(res, 'store', `ðŸ“ Stored chunk ${chunkNum} metadata with embedding status: ${embeddingStatus}`);
                    }
                } catch (error) {
                    sendSSEUpdate(res, 'warning', `âš ï¸ Metadata storage failed for chunk ${chunkNum}: ${error.message}`);
                }
                processedChunks++;
                
            } catch (error) {
                sendSSEUpdate(res, 'warning', `âš ï¸ Failed to process chunk ${chunk.chunk_index + 1}: ${error.message}`);
            }
        }
        
        const processingTime = Math.round((Date.now() - startTime) / 1000);
        
        sendSSEUpdate(res, 'complete', 
            `âœ… Processing complete! ${processedChunks}/${chunks.length} chunks processed in ${processingTime}s`
        );
        
        sendSSEUpdate(res, 'summary', JSON.stringify({
            status: 'success',
            url: url,
            chunks_total: chunks.length,
            chunks_processed: processedChunks,
            qdrant_stored: qdrantStored,
            airtable_stored: airtableStored,
            processing_time: `${processingTime}s`
        }));
        
    } catch (error) {
        const processingTime = Math.round((Date.now() - startTime) / 1000);
        sendSSEUpdate(res, 'error', `âŒ Processing failed: ${error.message}`);
        sendSSEUpdate(res, 'summary', JSON.stringify({
            status: 'error',
            error: error.message,
            chunks_processed: processedChunks,
            qdrant_stored: qdrantStored,
            airtable_stored: airtableStored,
            processing_time: `${processingTime}s`
        }));
    }
    
    res.end();
});

// Main processing endpoint - replaces n8n webhook
app.post('/api/process-url', async (req, res) => {
    const startTime = Date.now();
    let processedChunks = 0;
    let qdrantStored = 0;
    let airtableStored = 0;
    
    try {
        const { url } = req.body;
        
        if (!url) {
            return res.status(400).json({ error: 'URL is required' });
        }
        
        console.log(`Starting processing for URL: ${url}`);
        
        // Step 1: Fetch web content
        console.log('Fetching web content...');
        const contentData = await fetchWebContent(url);
        
        // Step 2: Convert to markdown (or use PDF text directly)
        let markdown;
        if (contentData.type === 'pdf') {
            console.log(`Processing PDF with ${contentData.metadata.pages} pages...`);
            markdown = contentData.content; // PDF text is already plain text
        } else {
            console.log('Converting HTML to markdown...');
            markdown = htmlToMarkdown(contentData.content);
        }
        
        if (!markdown || markdown.trim().length < 100) {
            return res.status(400).json({ error: 'No substantial content found at URL' });
        }
        
        // Step 3: Chunk the text
        console.log('Chunking text...');
        const chunks = chunkText(markdown, url);
        console.log(`Created ${chunks.length} chunks`);
        
        // Step 4: Process each chunk
        for (const chunk of chunks) {
            try {
                console.log(`Processing chunk ${chunk.chunk_index + 1}/${chunks.length}`);
                
                // Analyze chunk with GPT-4o-mini
                const analysis = await analyzeChunk(chunk.chunk_text);
                console.log('Analysis result for chunk:', analysis);
                
                // Generate embedding
                const embedding = await generateEmbedding(chunk.chunk_text);
                
                // Store in Qdrant first to determine embedding status
                let embeddingStatus = 'pending';
                try {
                    await storeInQdrant(chunk, embedding, analysis);
                    qdrantStored++;
                    embeddingStatus = 'success';
                } catch (error) {
                    embeddingStatus = 'failed';
                    console.error('Qdrant storage failed:', error);
                }
                
                // Store in Airtable with embedding status
                try {
                    const result = await storeInAirtable(chunk, analysis, embeddingStatus);
                    console.log('Airtable result:', result);
                    if (result.status !== 'skipped') {
                        airtableStored++;
                        console.log('Airtable stored count incremented to:', airtableStored);
                    }
                } catch (error) {
                    console.error('Airtable storage failed:', error.message);
                    // Don't increment airtableStored on error
                }
                
                processedChunks++;
                
            } catch (error) {
                console.error(`Failed to process chunk ${chunk.chunk_index}:`, error);
                // Continue with other chunks even if one fails
            }
        }
        
        const processingTime = Math.round((Date.now() - startTime) / 1000);
        
        console.log(`Processing complete: ${processedChunks}/${chunks.length} chunks processed in ${processingTime}s`);
        
        res.json({
            status: 'success',
            url: url,
            chunks_total: chunks.length,
            chunks_processed: processedChunks,
            qdrant_stored: qdrantStored,
            airtable_stored: airtableStored,
            processing_time: `${processingTime}s`,
            timestamp: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('Processing failed:', error);
        
        const processingTime = Math.round((Date.now() - startTime) / 1000);
        
        res.status(500).json({
            status: 'error',
            error: error.message,
            chunks_processed: processedChunks,
            qdrant_stored: qdrantStored,
            airtable_stored: airtableStored,
            processing_time: `${processingTime}s`,
            timestamp: new Date().toISOString()
        });
    }
});

// Route to get individual record from Airtable
app.get('/api/record', async (req, res) => {
    try {
        const recordId = req.query.id;
        
        if (!recordId) {
            return res.status(400).json({ error: 'Record ID is required' });
        }

        if (!AIRTABLE_API_KEY || AIRTABLE_API_KEY === 'your_airtable_api_key_here') {
            console.log('Warning: AIRTABLE_API_KEY not set, returning sample data');
            // Return sample data for the specific record ID
            const sampleRecord = {
                id: recordId,
                url: "https://www.americanyawp.com/text/01-the-new-world/",
                title: "Sample Record",
                summary: "This is sample data because Airtable API key is not configured.",
                processedDate: "2025-07-24T00:00:00.000Z",
                category: "Sample",
                status: "Complete",
                content: "Sample content for testing purposes."
            };
            return res.json(sampleRecord);
        }

        // Real Airtable API call for individual record
        const airtableUrl = `https://api.airtable.com/v0/${AIRTABLE_BASE_ID}/${AIRTABLE_TABLE_ID}/${recordId}`;
        
        const response = await fetch(airtableUrl, {
            headers: {
                'Authorization': `Bearer ${AIRTABLE_API_KEY}`,
                'Content-Type': 'application/json'
            }
        });

        if (!response.ok) {
            if (response.status === 404) {
                return res.status(404).json({ error: 'Record not found' });
            }
            throw new Error(`Airtable API error: ${response.status} ${response.statusText}`);
        }

        const data = await response.json();
        
        // Transform Airtable record to include all rich metadata
        const record = {
            id: data.id,
            url: data.fields.URL || '',
            title: data.fields.Title || 'Untitled',
            summary: data.fields.Summary || '',
            content: data.fields.Content || '',
            processedDate: data.fields['Date Processed'] || data.createdTime,
            category: data.fields.Category || 'General',
            status: data.fields.Select || 'Unknown',
            createdTime: data.createdTime,
            
            // Rich metadata fields
            sentiment: data.fields.Sentiment || 'Neutral',
            emotions: data.fields.Emotions || [],
            tags: data.fields.Tags || '',
            keyEntities: data.fields['Key Entities'] ? JSON.parse(data.fields['Key Entities'] || '{}') : {},
            mainTopics: data.fields['Main Topics'] ? JSON.parse(data.fields['Main Topics'] || '[]') : [],
            keyConcepts: data.fields['Key Concepts'] || '',
            contentType: data.fields['Content Type'] || 'other',
            technicalLevel: data.fields['Technical Level'] || 'intermediate',
            chunkId: data.fields['Chunk ID'] || '',
            chunkIndex: data.fields['Chunk Index'] || 0,
            chunkText: data.fields['Chunk Text'] || '',
            embeddingStatus: data.fields['Embedding Status'] || 'pending',
            source: data.fields.Source || 'autollama.io',
            links: data.fields.Links || ''
        };

        res.json(record);
    } catch (error) {
        console.error('Error fetching record from Airtable:', error);
        res.status(500).json({ error: 'Failed to fetch record' });
    }
});

// NEW: Smart recent records endpoint with real-time + cached performance
app.get('/api/recent-records', async (req, res) => {
    try {
        console.log('âš¡ SMART RECENT RECORDS API CALLED');
        const startTime = Date.now();
        
        // Get smart content mix (real-time recent + cached historical)
        const result = await db.getSmartContentMix();
        
        const responseTime = Date.now() - startTime;
        console.log(`ðŸ“Š Smart content delivered in ${responseTime}ms`);
        console.log(`   ðŸ“ˆ Recent: ${result.metadata.recent_count}, Historical: ${result.metadata.historical_count}`);
        
        // Add performance headers
        res.set({
            'X-Response-Time': `${responseTime}ms`,
            'X-Data-Source': 'postgresql-hybrid',
            'X-Recent-Count': result.metadata.recent_count,
            'X-Historical-Count': result.metadata.historical_count,
            'X-Cache-Status': result.metadata.cache_status
        });
        
        res.json(result.records);
        
    } catch (error) {
        console.error('âŒ Error in smart recent records:', error.message);
        console.error(error.stack);
        
        // Fallback: return sample data to keep UI functional
        const fallbackData = [
            {
                id: 'fallback-1',
                url: 'https://fallback.example.com',
                title: 'Service Temporarily Unavailable',
                summary: 'The content service is temporarily unavailable. Please try again in a moment.',
                sentiment: 'Neutral',
                category: 'System',
                content_type: 'notice',
                embedding_status: 'complete',
                created_time: new Date().toISOString(),
                data_source: 'fallback'
            }
        ];
        
        res.status(200).json(fallbackData); // Return 200 to keep UI working
    }
});
app.get('/api/pipeline/download', async (req, res) => {
    try {
        const fs = require('fs');
        const path = require('path');
        
        // Read the pipeline template
        const pipelinePath = path.join(__dirname, 'autollama_rag_pipeline.py');
        let pipelineContent = fs.readFileSync(pipelinePath, 'utf8');
        
        // Replace placeholders with actual configuration
        pipelineContent = pipelineContent.replace(
            'QDRANT_URL: str = "https://c4c8ee46-d9dd-4c0f-a00e-9215675351da.us-west-1-0.aws.cloud.qdrant.io"',
            `QDRANT_URL: str = "${QDRANT_URL}"`
        );
        
        pipelineContent = pipelineContent.replace(
            'QDRANT_API_KEY: str = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiO2JtIn0.ghMgF9xxBObWVQnMQab9wCk7JP4jkHI7k4Z1TYo8zqg"',
            `QDRANT_API_KEY: str = "${QDRANT_API_KEY}"`
        );
        
        // Add current date
        const currentDate = new Date().toISOString().split('T')[0];
        pipelineContent = pipelineContent.replace(
            'date: 2025-07-25',
            `date: ${currentDate}`
        );
        
        // Set response headers for file download
        res.setHeader('Content-Type', 'text/x-python');
        res.setHeader('Content-Disposition', 'attachment; filename="autollama_rag_pipeline.py"');
        res.setHeader('Access-Control-Allow-Origin', '*');
        
        res.send(pipelineContent);
        
    } catch (error) {
        console.error('Error generating pipeline:', error);
        res.status(500).json({ error: 'Failed to generate pipeline file' });
    }
});

// Knowledge base stats endpoint
app.get('/api/knowledge-base/stats', async (req, res) => {
    try {
        // Get collection info from Qdrant
        const collectionResponse = await axios.get(
            `${QDRANT_URL}/collections/autollama-content`,
            {
                headers: {
                    'api-key': QDRANT_API_KEY
                }
            }
        );
        
        const pointsCount = collectionResponse.data.result.points_count || 0;
        
        // Get recent records from Airtable to estimate unique URLs
        const airtableResponse = await fetch(`https://api.airtable.com/v0/${AIRTABLE_BASE_ID}/${AIRTABLE_TABLE_ID}`, {
            headers: {
                'Authorization': `Bearer ${AIRTABLE_API_KEY}`,
                'Content-Type': 'application/json'
            }
        });
        
        let uniqueUrls = 0;
        if (airtableResponse.ok) {
            const airtableData = await airtableResponse.json();
            const urls = new Set();
            airtableData.records.forEach(record => {
                if (record.fields.URL) {
                    urls.add(record.fields.URL);
                }
            });
            uniqueUrls = urls.size;
        }
        
        res.json({
            success: true,
            totalPoints: pointsCount,
            uniqueUrls: uniqueUrls,
            collectionStatus: 'active',
            lastUpdated: new Date().toISOString()
        });
        
    } catch (error) {
        console.error('Error getting knowledge base stats:', error);
        res.status(500).json({
            success: false,
            error: 'Failed to retrieve knowledge base statistics',
            totalPoints: 0,
            uniqueUrls: 0,
            collectionStatus: 'error'
        });
    }
});

// Health check endpoint
app.get('/health', (req, res) => {
    console.log('Health endpoint called');
    res.json({ 
        status: 'OK', 
        timestamp: new Date().toISOString(),
        airtableKeyLength: AIRTABLE_API_KEY ? AIRTABLE_API_KEY.length : 0,
        airtableKeyValue: AIRTABLE_API_KEY
    });
});

// NEW: Real-time in-progress endpoint with in-memory tracking
app.get('/api/in-progress', async (req, res) => {
    try {
        console.log('âš¡ REAL-TIME IN-PROGRESS API CALLED');
        const startTime = Date.now();
        
        // Get active sessions from in-memory tracking
        const sessions = Array.from(activeProcessingSessions.values());
        
        const responseTime = Date.now() - startTime;
        console.log(`ðŸ“Š In-progress sessions delivered in ${responseTime}ms`);
        console.log(`   ðŸ”„ Active sessions: ${sessions.length}`);
        
        // Transform to match frontend expectations
        const transformedSessions = sessions.map(session => ({
            id: session.id,
            url: session.url,
            filename: session.filename || 'Unknown File',
            title: session.title || session.filename || 'Processing...',
            totalChunks: session.totalChunks || 0,
            completedChunks: session.processedChunks || 0,
            processedChunks: session.processedChunks || 0,
            status: session.status,
            lastActivity: session.lastUpdate ? session.lastUpdate.toISOString() : new Date().toISOString(),
            createdAt: session.startTime ? session.startTime.toISOString() : new Date().toISOString(),
            progress: session.totalChunks > 0 ? 
                Math.round((session.processedChunks / session.totalChunks) * 100) : 0
        }));
        
        // Add performance headers
        res.set({
            'X-Response-Time': `${responseTime}ms`,
            'X-Data-Source': 'memory-realtime',
            'X-Active-Sessions': sessions.length
        });
        
        res.json(transformedSessions);
        
    } catch (error) {
        console.error('âŒ Error in real-time in-progress:', error.message);
        
        // Return empty array to keep UI functional
        res.status(200).json([]);
    }
});

// File processing utilities
async function streamToBuffer(stream) {
    return new Promise((resolve, reject) => {
        const chunks = [];
        stream.on('data', chunk => chunks.push(chunk));
        stream.on('end', () => resolve(Buffer.concat(chunks)));
        stream.on('error', reject);
    });
}

class FileProcessor {
    constructor() {
        this.parsers = {
            'application/pdf': this.parsePDF.bind(this),
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document': this.parseDOCX.bind(this),
            'application/epub+zip': this.parseEPUB.bind(this),
            'text/plain': this.parseText.bind(this),
            'text/csv': this.parseCSV.bind(this),
            'text/html': this.parseHTML.bind(this),
            'application/msword': this.parseDOCX.bind(this) // fallback for older Word docs
        };
    }

    async selectParser(mimeType, filename) {
        // Try mime type first
        if (this.parsers[mimeType]) {
            return this.parsers[mimeType];
        }
        
        // Fallback to file extension
        const ext = filename.toLowerCase().split('.').pop();
        const extToMime = {
            'pdf': 'application/pdf',
            'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'doc': 'application/msword',
            'epub': 'application/epub+zip',
            'txt': 'text/plain',
            'csv': 'text/csv',
            'html': 'text/html',
            'htm': 'text/html'
        };
        
        const detectedMime = extToMime[ext];
        if (detectedMime && this.parsers[detectedMime]) {
            return this.parsers[detectedMime];
        }
        
        throw new Error(`Unsupported file type: ${mimeType} (${filename})`);
    }

    async parsePDF(buffer) {
        const data = await pdfParse(buffer);
        return {
            content: data.text,
            type: 'pdf',
            metadata: {
                pages: data.numpages,
                info: data.info
            }
        };
    }

    async parseDOCX(buffer) {
        const result = await mammoth.extractRawText({ buffer });
        return {
            content: result.value,
            type: 'docx',
            metadata: {
                messages: result.messages
            }
        };
    }

    async parseEPUB(buffer) {
        return new Promise((resolve, reject) => {
            // Write buffer to temporary file since epub library requires file path
            tmp.file({ postfix: '.epub' }, (err, path, fd, cleanupCallback) => {
                if (err) {
                    reject(err);
                    return;
                }
                
                require('fs').writeFile(path, buffer, (writeErr) => {
                    if (writeErr) {
                        cleanupCallback();
                        reject(writeErr);
                        return;
                    }
                    
                    // Parse EPUB
                    const epubParser = new epub(path);
                    
                    epubParser.on('error', (error) => {
                        cleanupCallback();
                        reject(error);
                    });
                    
                    epubParser.on('end', () => {
                        let content = '';
                        const chapters = epubParser.flow;
                        
                        let processed = 0;
                        const totalChapters = chapters.length;
                        
                        if (totalChapters === 0) {
                            cleanupCallback();
                            resolve({
                                content: '',
                                type: 'epub',
                                metadata: {
                                    title: epubParser.metadata.title,
                                    author: epubParser.metadata.creator,
                                    language: epubParser.metadata.language
                                }
                            });
                            return;
                        }
                        
                        chapters.forEach((chapter, index) => {
                            epubParser.getChapter(chapter.id, (error, text) => {
                                if (!error && text) {
                                    // Remove HTML tags
                                    const $ = cheerio.load(text);
                                    content += $.text() + '\n\n';
                                }
                                
                                processed++;
                                
                                if (processed === totalChapters) {
                                    cleanupCallback();
                                    resolve({
                                        content: content.trim(),
                                        type: 'epub',
                                        metadata: {
                                            title: epubParser.metadata.title,
                                            author: epubParser.metadata.creator,
                                            language: epubParser.metadata.language
                                        }
                                    });
                                }
                            });
                        });
                    });
                    
                    epubParser.parse();
                });
            });
        });
    }

    async parseText(buffer) {
        return {
            content: buffer.toString('utf-8'),
            type: 'text',
            metadata: {}
        };
    }

    async parseCSV(buffer) {
        return new Promise((resolve, reject) => {
            const content = buffer.toString('utf-8');
            const rows = [];
            
            csvParse(content, {
                columns: true,
                skip_empty_lines: true
            }, (err, records) => {
                if (err) {
                    reject(err);
                    return;
                }
                
                // Convert CSV to readable text
                let textContent = '';
                if (records.length > 0) {
                    const headers = Object.keys(records[0]);
                    textContent = `CSV Data with columns: ${headers.join(', ')}\n\n`;
                    
                    records.forEach((row, index) => {
                        textContent += `Row ${index + 1}:\n`;
                        headers.forEach(header => {
                            textContent += `${header}: ${row[header]}\n`;
                        });
                        textContent += '\n';
                    });
                }
                
                resolve({
                    content: textContent,
                    type: 'csv',
                    metadata: {
                        rowCount: records.length,
                        columns: records.length > 0 ? Object.keys(records[0]) : []
                    }
                });
            });
        });
    }

    async parseHTML(buffer) {
        const html = buffer.toString('utf-8');
        const $ = cheerio.load(html);
        
        // Remove script and style elements
        $('script, style').remove();
        
        // Extract text content
        const content = $.text().replace(/\s+/g, ' ').trim();
        
        return {
            content,
            type: 'html',
            metadata: {
                title: $('title').text(),
                description: $('meta[name="description"]').attr('content')
            }
        };
    }
}

// Initialize file processor
const fileProcessor = new FileProcessor();

// File upload endpoint with streaming and resumable support
app.post('/api/process-file-stream', (req, res) => {
    // Set headers for SSE
    res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Cache-Control'
    });

    const busboy = Busboy({ 
        headers: req.headers,
        limits: {
            fileSize: 500 * 1024 * 1024 // 500MB limit
        }
    });

    let processingId = uuidv4();
    
    function sendSSEUpdate(event, data) {
        res.write(`data: ${JSON.stringify({ event, data })}\n\n`);
    }

    sendSSEUpdate('start', { processingId, message: 'Starting file upload...' });

    busboy.on('file', async (name, file, info) => {
        let uploadSession = null;
        let tempFilePath = null;
        
        try {
            const { filename, mimeType } = info;
            sendSSEUpdate('upload', { progress: 10, message: `Uploading ${filename}...` });
            
            // Stream file to buffer
            const fileBuffer = await streamToBuffer(file);
            sendSSEUpdate('upload', { progress: 25, message: 'File uploaded, starting processing...' });
            
            // Save temporary file using tmp library
            const fs = require('fs');
            const tmpFile = tmp.fileSync({ postfix: `-${filename}`, keep: true });
            tempFilePath = tmpFile.name;
            fs.writeFileSync(tempFilePath, fileBuffer);
            sendSSEUpdate('upload', { progress: 30, message: 'File saved, analyzing content...' });
            
            // Select and execute parser
            const parser = await fileProcessor.selectParser(mimeType, filename);
            sendSSEUpdate('parse', { progress: 40, message: 'Parsing file content...' });
            
            const parseResult = await parser(fileBuffer);
            sendSSEUpdate('parse', { progress: 50, message: 'Content parsed, creating chunks...' });
            
            // Create chunks to determine total count
            const chunks = chunkText(parseResult.content, `file://${filename}`);
            sendSSEUpdate('parse', { progress: 55, message: `Content split into ${chunks.length} chunks` });
            
            // Create upload session
            try {
                uploadSession = await createUploadSession(filename, chunks.length, tempFilePath);
                sendSSEUpdate('session', { 
                    progress: 60, 
                    message: 'Upload session created',
                    sessionId: uploadSession.sessionId 
                });
            } catch (sessionError) {
                console.error('Failed to create upload session:', sessionError);
                // Continue without session tracking if Airtable is not available
                uploadSession = { id: null, sessionId: processingId };
            }
            
            // Process with existing pipeline including session tracking
            const processedData = await processFileContentWithSession(
                parseResult, 
                filename, 
                chunks,
                uploadSession,
                sendSSEUpdate
            );
            
            sendSSEUpdate('complete', { 
                progress: 100, 
                message: 'File processing completed!',
                data: processedData,
                sessionId: uploadSession.sessionId
            });
            
            // Clean up temp file on successful completion
            if (tempFilePath && fs.existsSync(tempFilePath)) {
                fs.unlinkSync(tempFilePath);
            }
            
        } catch (error) {
            console.error('File processing error:', error);
            
            // Update session status to failed if session was created
            if (uploadSession && uploadSession.id) {
                try {
                    await updateUploadSession(uploadSession.id, 0, 'Failed');
                } catch (updateError) {
                    console.error('Failed to update session status:', updateError);
                }
            }
            
            sendSSEUpdate('error', { 
                message: `Error processing file: ${error.message}`,
                error: error.toString(),
                sessionId: uploadSession?.sessionId,
                resumable: !!tempFilePath
            });
        } finally {
            res.end();
        }
    });

    busboy.on('error', (error) => {
        console.error('Busboy error:', error);
        sendSSEUpdate('error', { 
            message: `Upload error: ${error.message}`,
            error: error.toString()
        });
        res.end();
    });

    req.pipe(busboy);
});

// Standard file upload endpoint (non-streaming)
app.post('/api/process-file', (req, res) => {
    const busboy = Busboy({ 
        headers: req.headers,
        limits: {
            fileSize: 500 * 1024 * 1024 // 500MB limit
        }
    });

    busboy.on('file', async (name, file, info) => {
        try {
            const { filename, mimeType } = info;
            
            // Stream file to buffer
            const fileBuffer = await streamToBuffer(file);
            
            // Select and execute parser
            const parser = await fileProcessor.selectParser(mimeType, filename);
            const parseResult = await parser(fileBuffer);
            
            // Process with existing pipeline
            const processedData = await processFileContent(parseResult, filename);
            
            res.json({
                success: true,
                message: 'File processed successfully',
                data: processedData
            });
            
        } catch (error) {
            console.error('File processing error:', error);
            res.status(500).json({
                success: false,
                error: error.message
            });
        }
    });

    busboy.on('error', (error) => {
        console.error('Busboy error:', error);
        res.status(500).json({
            success: false,
            error: error.message
        });
    });

    req.pipe(busboy);
});

// Process file content using existing pipeline
async function processFileContent(parseResult, filename, sseCallback = null) {
    try {
        const url = `file://${filename}`;
        
        if (sseCallback) {
            sseCallback('analyze', { progress: 65, message: 'Analyzing content...' });
        }
        
        // Use existing content processing pipeline
        const processedChunks = await processContentChunks(parseResult.content, url, sseCallback);
        
        if (sseCallback) {
            sseCallback('store', { progress: 90, message: 'Storing processed data...' });
        }
        
        return {
            url,
            title: parseResult.metadata?.title || filename,
            type: parseResult.type,
            chunks: processedChunks.length,
            metadata: parseResult.metadata
        };
        
    } catch (error) {
        console.error('Error processing file content:', error);
        throw error;
    }
}

// Process file content with session tracking and concurrent processing
async function processFileContentWithSession(parseResult, filename, chunks, uploadSession, sseCallback = null) {
    let processedChunks = 0;
    let qdrantStored = 0;
    let airtableStored = 0;
    const url = `file://${filename}`;
    
    // Track this processing session in memory
    activeProcessingSessions.set(uploadSession.sessionId, {
        id: uploadSession.sessionId,
        url: url,
        filename: filename,
        totalChunks: chunks.length,
        processedChunks: 0,
        startTime: new Date(),
        lastUpdate: new Date(),
        status: 'processing'
    });
    
    try {
        if (sseCallback) {
            sseCallback('analyze', { 
                progress: 65, 
                message: `Processing ${chunks.length} chunks with session tracking...`,
                sessionId: uploadSession.sessionId
            });
        }
        
        // Process chunks with concurrency limit (3 at a time)
        const concurrencyLimit = 3;
        const processingPromises = [];
        
        for (let i = 0; i < chunks.length; i += concurrencyLimit) {
            const batch = chunks.slice(i, i + concurrencyLimit);
            
            const batchPromises = batch.map(async (chunk) => {
                try {
                    console.log(`Processing chunk ${chunk.chunk_index + 1}/${chunks.length} in session ${uploadSession.sessionId}`);
                    
                    // Analyze chunk with GPT-4o-mini
                    const analysis = await analyzeChunk(chunk.chunk_text);
                    
                    // Generate embedding
                    const embedding = await generateEmbedding(chunk.chunk_text);
                    
                    // Store in Qdrant
                    let embeddingStatus = 'pending';
                    try {
                        await storeInQdrant(chunk, embedding, analysis);
                        qdrantStored++;
                        embeddingStatus = 'success';
                    } catch (qdrantError) {
                        console.error('Qdrant storage failed:', qdrantError);
                        embeddingStatus = 'failed';
                    }
                    
                    // Store in Airtable with session ID
                    try {
                        await storeInAirtable(chunk, analysis, embeddingStatus, uploadSession.sessionId);
                        airtableStored++;
                    } catch (airtableError) {
                        console.error('Airtable storage failed:', airtableError);
                    }
                    
                    processedChunks++;
                    
                    // Update in-memory session progress
                    const session = activeProcessingSessions.get(uploadSession.sessionId);
                    if (session) {
                        session.processedChunks = processedChunks;
                        session.lastUpdate = new Date();
                        activeProcessingSessions.set(uploadSession.sessionId, session);
                    }
                    
                    // Update session progress
                    if (uploadSession.id) {
                        try {
                            await updateUploadSession(uploadSession.id, processedChunks);
                        } catch (updateError) {
                            console.error('Failed to update session progress:', updateError);
                        }
                    }
                    
                    // Update progress
                    if (sseCallback) {
                        const progress = 65 + Math.round((processedChunks / chunks.length) * 25);
                        sseCallback('analyze', { 
                            progress, 
                            message: `Processed ${processedChunks}/${chunks.length} chunks`,
                            current: processedChunks,
                            total: chunks.length,
                            sessionId: uploadSession.sessionId
                        });
                    }
                    
                    return { success: true, chunkIndex: chunk.chunk_index };
                    
                } catch (chunkError) {
                    console.error(`Error processing chunk ${chunk.chunk_index + 1}:`, chunkError);
                    return { success: false, chunkIndex: chunk.chunk_index, error: chunkError.message };
                }
            });
            
            // Wait for this batch to complete before starting the next
            const batchResults = await Promise.all(batchPromises);
            processingPromises.push(...batchResults);
            
            // Small delay between batches to prevent overwhelming APIs
            if (i + concurrencyLimit < chunks.length) {
                await new Promise(resolve => setTimeout(resolve, 500));
            }
        }
        
        // Mark session as completed
        if (uploadSession.id) {
            try {
                await updateUploadSession(uploadSession.id, processedChunks, 'Completed');
            } catch (updateError) {
                console.error('Failed to mark session as completed:', updateError);
            }
        }
        
        console.log(`Session ${uploadSession.sessionId} completed: ${processedChunks} chunks processed, ${qdrantStored} stored in Qdrant, ${airtableStored} stored in Airtable`);
        
        // Mark session as completed and remove from active sessions
        const session = activeProcessingSessions.get(uploadSession.sessionId);
        if (session) {
            session.status = 'completed';
            session.completedAt = new Date();
            activeProcessingSessions.delete(uploadSession.sessionId);
            console.log(`Session ${uploadSession.sessionId} completed and removed from active tracking`);
        }
        
        return {
            url,
            title: parseResult.metadata?.title || filename,
            type: parseResult.type,
            chunks: chunks.length,
            processedChunks,
            qdrantStored,
            airtableStored,
            sessionId: uploadSession.sessionId,
            metadata: parseResult.metadata
        };
        
    } catch (error) {
        console.error('Error in processFileContentWithSession:', error);
        
        // Mark session as failed and remove from active sessions
        const session = activeProcessingSessions.get(uploadSession.sessionId);
        if (session) {
            session.status = 'failed';
            session.error = error.message;
            session.completedAt = new Date();
            activeProcessingSessions.delete(uploadSession.sessionId);
            console.log(`Session ${uploadSession.sessionId} failed and removed from active tracking`);
        }
        
        // Mark session as failed
        if (uploadSession.id) {
            try {
                await updateUploadSession(uploadSession.id, processedChunks, 'Failed');
            } catch (updateError) {
                console.error('Failed to mark session as failed:', updateError);
            }
        }
        
        throw error;
    }
}

// Process content chunks using the same logic as URL processing
async function processContentChunks(content, url, sseCallback = null) {
    let processedChunks = 0;
    let qdrantStored = 0;
    let airtableStored = 0;
    
    // Generate session ID for tracking
    const sessionId = uuidv4();
    
    try {
        // Step 1: Chunk the text
        console.log('Chunking text...');
        const chunks = chunkText(content, url);
        console.log(`Created ${chunks.length} chunks`);
        
        // Track this processing session
        activeProcessingSessions.set(sessionId, {
            id: sessionId,
            url: url,
            filename: url.split('/').pop() || 'Unknown File',
            totalChunks: chunks.length,
            processedChunks: 0,
            startTime: new Date(),
            lastUpdate: new Date(),
            status: 'processing'
        });
        
        if (sseCallback) {
            sseCallback('analyze', { 
                progress: 70, 
                message: `Processing ${chunks.length} chunks...`,
                current: 0,
                total: chunks.length
            });
        }
        
        // Step 2: Process each chunk
        for (const chunk of chunks) {
            try {
                console.log(`Processing chunk ${chunk.chunk_index + 1}/${chunks.length}`);
                
                // Analyze chunk with GPT-4o-mini
                const analysis = await analyzeChunk(chunk.chunk_text);
                console.log('Analysis result for chunk:', analysis);
                
                // Generate embedding
                const embedding = await generateEmbedding(chunk.chunk_text);
                
                // Store in Qdrant
                try {
                    await storeInQdrant(chunk, embedding, analysis);
                    qdrantStored++;
                    console.log(`Stored chunk ${chunk.chunk_index + 1} in Qdrant`);
                } catch (qdrantError) {
                    console.error('Qdrant storage failed:', qdrantError);
                }
                
                // Store in Airtable
                try {
                    await storeInAirtable(chunk, analysis, 'success');
                    airtableStored++;
                    console.log(`Stored chunk ${chunk.chunk_index + 1} in Airtable`);
                } catch (airtableError) {
                    console.error('Airtable storage failed:', airtableError);
                }
                
                processedChunks++;
                
                // Update session progress
                const session = activeProcessingSessions.get(sessionId);
                if (session) {
                    session.processedChunks = processedChunks;
                    session.lastUpdate = new Date();
                    activeProcessingSessions.set(sessionId, session);
                }
                
                // Update progress
                if (sseCallback) {
                    const progress = 70 + Math.round((processedChunks / chunks.length) * 15);
                    sseCallback('analyze', { 
                        progress, 
                        message: `Processed ${processedChunks}/${chunks.length} chunks`,
                        current: processedChunks,
                        total: chunks.length
                    });
                }
                
            } catch (chunkError) {
                console.error(`Error processing chunk ${chunk.chunk_index + 1}:`, chunkError);
                // Continue with next chunk
            }
        }
        
        console.log(`Processing completed: ${processedChunks} chunks processed, ${qdrantStored} stored in Qdrant, ${airtableStored} stored in Airtable`);
        
        // Mark session as completed and remove from active sessions
        const session = activeProcessingSessions.get(sessionId);
        if (session) {
            session.status = 'completed';
            session.completedAt = new Date();
            activeProcessingSessions.delete(sessionId);
            console.log(`Session ${sessionId} completed and removed from active tracking`);
        }
        
        return chunks;
        
    } catch (error) {
        console.error('Error in processContentChunks:', error);
        
        // Mark session as failed and remove from active sessions
        const session = activeProcessingSessions.get(sessionId);
        if (session) {
            session.status = 'failed';
            session.error = error.message;
            session.completedAt = new Date();
            activeProcessingSessions.delete(sessionId);
            console.log(`Session ${sessionId} failed and removed from active tracking`);
        }
        
        throw error;
    }
}

// Resume upload endpoint
app.post('/api/resume-upload/:sessionId', async (req, res) => {
    // Set headers for SSE
    res.writeHead(200, {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Cache-Control'
    });

    const { sessionId } = req.params;
    
    function sendSSEUpdate(event, data) {
        res.write(`data: ${JSON.stringify({ event, data })}\n\n`);
    }

    try {
        sendSSEUpdate('start', { sessionId, message: 'Checking session status...' });
        
        // Get upload session details
        const uploadSession = await getUploadSession(sessionId);
        if (!uploadSession) {
            sendSSEUpdate('error', { message: 'Upload session not found' });
            res.end();
            return;
        }
        
        const sessionFields = uploadSession.fields;
        const sessionRecordId = uploadSession.id;
        
        // Check if session is already completed
        if (sessionFields.Status === 'Completed') {
            sendSSEUpdate('complete', { 
                message: 'Session already completed',
                sessionId: sessionId,
                progress: 100
            });
            res.end();
            return;
        }
        
        sendSSEUpdate('session', { 
            progress: 10, 
            message: `Found session: ${sessionFields.Filename}`,
            filename: sessionFields.Filename,
            totalChunks: sessionFields['Total Chunks'],
            completedChunks: sessionFields['Completed Chunks']
        });
        
        // Check if temp file still exists
        const fs = require('fs');
        const tempFilePath = sessionFields['File Path'];
        if (!tempFilePath || !fs.existsSync(tempFilePath)) {
            await updateUploadSession(sessionRecordId, sessionFields['Completed Chunks'], 'Failed');
            sendSSEUpdate('error', { 
                message: 'Temporary file no longer exists. Please re-upload the file.',
                sessionId: sessionId
            });
            res.end();
            return;
        }
        
        sendSSEUpdate('upload', { progress: 20, message: 'Reading file from temporary storage...' });
        
        // Read and parse the file
        const fileBuffer = fs.readFileSync(tempFilePath);
        const filename = sessionFields.Filename;
        const mimeType = getMimeTypeFromFilename(filename);
        
        const parser = await fileProcessor.selectParser(mimeType, filename);
        sendSSEUpdate('parse', { progress: 30, message: 'Parsing file content...' });
        
        const parseResult = await parser(fileBuffer);
        const chunks = chunkText(parseResult.content, `file://${filename}`);
        sendSSEUpdate('parse', { progress: 40, message: `Content split into ${chunks.length} chunks` });
        
        // Get already processed chunks
        const processedChunksData = await getProcessedChunksForSession(sessionId);
        const processedChunkIndices = new Set(processedChunksData.map(c => c.chunkIndex));
        
        // Filter out already processed chunks
        const remainingChunks = chunks.filter(chunk => !processedChunkIndices.has(chunk.chunk_index));
        
        sendSSEUpdate('resume', { 
            progress: 50, 
            message: `Resuming processing: ${remainingChunks.length} chunks remaining`,
            totalChunks: chunks.length,
            completedBefore: processedChunkIndices.size,
            remaining: remainingChunks.length
        });
        
        if (remainingChunks.length === 0) {
            // All chunks already processed, mark as completed
            await updateUploadSession(sessionRecordId, chunks.length, 'Completed');
            sendSSEUpdate('complete', { 
                progress: 100, 
                message: 'All chunks already processed!',
                sessionId: sessionId
            });
            
            // Clean up temp file
            if (fs.existsSync(tempFilePath)) {
                fs.unlinkSync(tempFilePath);
            }
            
            res.end();
            return;
        }
        
        // Update session status to processing
        await updateUploadSession(sessionRecordId, processedChunkIndices.size, 'Processing');
        
        // Resume processing with remaining chunks
        const resumeSession = {
            id: sessionRecordId,
            sessionId: sessionId
        };
        
        await processRemainingChunks(remainingChunks, resumeSession, processedChunkIndices.size, sendSSEUpdate);
        
        sendSSEUpdate('complete', { 
            progress: 100, 
            message: 'Upload resumed and completed successfully!',
            sessionId: sessionId
        });
        
        // Clean up temp file on successful completion
        if (fs.existsSync(tempFilePath)) {
            fs.unlinkSync(tempFilePath);
        }
        
    } catch (error) {
        console.error('Resume upload error:', error);
        sendSSEUpdate('error', { 
            message: `Resume failed: ${error.message}`,
            sessionId: sessionId
        });
    } finally {
        res.end();
    }
});

// Helper function to get MIME type from filename
function getMimeTypeFromFilename(filename) {
    const ext = filename.toLowerCase().split('.').pop();
    const extToMime = {
        'pdf': 'application/pdf',
        'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'doc': 'application/msword',
        'epub': 'application/epub+zip',
        'txt': 'text/plain',
        'csv': 'text/csv',
        'html': 'text/html',
        'htm': 'text/html'
    };
    return extToMime[ext] || 'application/octet-stream';
}

// Process remaining chunks for resume functionality
async function processRemainingChunks(remainingChunks, uploadSession, initialCompletedCount, sseCallback) {
    let processedChunks = initialCompletedCount;
    let qdrantStored = 0;
    let airtableStored = 0;
    
    try {
        // Process chunks with concurrency limit (3 at a time)
        const concurrencyLimit = 3;
        
        for (let i = 0; i < remainingChunks.length; i += concurrencyLimit) {
            const batch = remainingChunks.slice(i, i + concurrencyLimit);
            
            const batchPromises = batch.map(async (chunk) => {
                try {
                    console.log(`Resuming chunk ${chunk.chunk_index + 1} in session ${uploadSession.sessionId}`);
                    
                    // Analyze chunk with GPT-4o-mini
                    const analysis = await analyzeChunk(chunk.chunk_text);
                    
                    // Generate embedding
                    const embedding = await generateEmbedding(chunk.chunk_text);
                    
                    // Store in Qdrant
                    let embeddingStatus = 'pending';
                    try {
                        await storeInQdrant(chunk, embedding, analysis);
                        qdrantStored++;
                        embeddingStatus = 'success';
                    } catch (qdrantError) {
                        console.error('Qdrant storage failed:', qdrantError);
                        embeddingStatus = 'failed';
                    }
                    
                    // Store in Airtable with session ID
                    try {
                        await storeInAirtable(chunk, analysis, embeddingStatus, uploadSession.sessionId);
                        airtableStored++;
                    } catch (airtableError) {
                        console.error('Airtable storage failed:', airtableError);
                    }
                    
                    processedChunks++;
                    
                    // Update session progress
                    if (uploadSession.id) {
                        try {
                            await updateUploadSession(uploadSession.id, processedChunks);
                        } catch (updateError) {
                            console.error('Failed to update session progress:', updateError);
                        }
                    }
                    
                    // Update progress
                    if (sseCallback) {
                        const currentChunkInBatch = processedChunks - initialCompletedCount;
                        const progress = 50 + Math.round((currentChunkInBatch / remainingChunks.length) * 45);
                        sseCallback('analyze', { 
                            progress, 
                            message: `Resumed: processed ${processedChunks} total chunks`,
                            current: processedChunks,
                            total: initialCompletedCount + remainingChunks.length,
                            sessionId: uploadSession.sessionId
                        });
                    }
                    
                    return { success: true, chunkIndex: chunk.chunk_index };
                    
                } catch (chunkError) {
                    console.error(`Error processing chunk ${chunk.chunk_index + 1}:`, chunkError);
                    return { success: false, chunkIndex: chunk.chunk_index, error: chunkError.message };
                }
            });
            
            // Wait for this batch to complete before starting the next
            await Promise.all(batchPromises);
            
            // Small delay between batches to prevent overwhelming APIs
            if (i + concurrencyLimit < remainingChunks.length) {
                await new Promise(resolve => setTimeout(resolve, 500));
            }
        }
        
        // Mark session as completed
        if (uploadSession.id) {
            try {
                await updateUploadSession(uploadSession.id, processedChunks, 'Completed');
            } catch (updateError) {
                console.error('Failed to mark session as completed:', updateError);
            }
        }
        
        console.log(`Session ${uploadSession.sessionId} resumed and completed: ${processedChunks} total chunks processed`);
        
    } catch (error) {
        console.error('Error in processRemainingChunks:', error);
        
        // Mark session as failed
        if (uploadSession.id) {
            try {
                await updateUploadSession(uploadSession.id, processedChunks, 'Failed');
            } catch (updateError) {
                console.error('Failed to mark session as failed:', updateError);
            }
        }
        
        throw error;
    }
}


// Pipeline service health check endpoint
app.get('/api/pipeline/health', async (req, res) => {
    try {
        const pipelineUrl = 'http://127.0.0.1:9099';
        const response = await fetch(pipelineUrl, {
            method: 'GET',
            timeout: 5000
        });
        
        const isHealthy = response.ok;
        
        res.json({
            success: true,
            status: isHealthy ? 'healthy' : 'unhealthy',
            serviceUrl: pipelineUrl,
            port: '9099',
            lastChecked: new Date().toISOString(),
            responseStatus: response.status
        });
        
    } catch (error) {
        console.error('Pipeline health check failed:', error);
        res.json({
            success: false,
            status: 'error',
            serviceUrl: 'http://127.0.0.1:9099',
            port: '9099',
            lastChecked: new Date().toISOString(),
            error: error.message
        });
    }
});

// Initialize database and start server
async function startServer() {
    console.log('ðŸš€ Starting AutoLlama API server with real-time PostgreSQL...');
    
    // Initialize database connection
    const dbReady = await db.initDatabase();
    if (!dbReady) {
        console.error('âŒ Failed to initialize database. Exiting...');
        process.exit(1);
    }
    
    // Start the Express server
    app.listen(PORT, () => {
        console.log(`âš¡ AutoLlama API server running on port ${PORT} - ${new Date().toISOString()}`);
        console.log(`ðŸ“Š Performance mode: Real-time recent + cached historical`);
        console.log(`ðŸ”Œ Database: PostgreSQL with hybrid caching`);
        console.log(`ðŸ“‹ Endpoints:`);
        console.log(`   GET /api/recent-records - Smart content mix (real-time + cached)`);
        console.log(`   GET /api/in-progress - Real-time active sessions`);
        console.log(`   GET /health - Health check with database status`);
        console.log('âœ… Server ready for real-time performance!');
    });
}

startServer().catch(error => {
    console.error('ðŸ’¥ Failed to start server:', error);
    process.exit(1);
});